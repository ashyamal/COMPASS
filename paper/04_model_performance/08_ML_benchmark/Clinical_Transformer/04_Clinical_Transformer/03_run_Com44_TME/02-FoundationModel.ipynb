{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf5663a-f421-439d-a3e5-cb89329a3db6",
   "metadata": {},
   "source": [
    "# Self supervised Learning\n",
    "\n",
    "Pretrain a \"foundation model\" using data without outcomes, from a large unlabeled dataset. Note that the idea behind the pre-training is to guide the fine tunning task (survival prediction). **Note that given this is a toy example the pre-training effect may be marginal.** \n",
    "\n",
    "The input to the mode are the features from the dataset and **not the outcomes**\n",
    "\n",
    "**Note that this notebook may take a while to complete when number of epochs is large**: For self supervised learning it is recommended to use a large  number of epochs. For illustration, we ran it for 1,000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b49798-69ad-401a-ad69-4d071232e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:25:47.372835: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-14 15:25:47.436845: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-14 15:25:47.799757: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shenwanxiang/anaconda3/lib:\n",
      "2025-08-14 15:25:47.799805: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shenwanxiang/anaconda3/lib:\n",
      "2025-08-14 15:25:47.799808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2546052/2123114778.py:5: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:25:48.230371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:25:48.257439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.274147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.274246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.567814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.567936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.568003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:25:48.568072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 9998 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/shenwanxiang/anaconda3\"\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8d7a0eb-e2ff-4541-bb5a-54325c0cfa56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../codeocean/environment/clinical_transformer/')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from xai.models.SimplifiedClinicalTransformer.Trainer import Trainer\n",
    "from xai.losses.survival import cIndex_SigmoidApprox as cindex_loss\n",
    "from xai.metrics.survival import sigmoid_concordance as cindex\n",
    "\n",
    "from xai.models import Trainer\n",
    "from xai.models import SelfSupervisedTransformer\n",
    "from xai.models import OptimizedSelfSupervisedDataGenerator as SelfSupervisedDataGenerator\n",
    "\n",
    "from xai.losses.selfsupervision.classifier_regression import CompositeLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b96e3cfe-bac1-4be7-9949-282247270dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from samecode.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde68ad-6c9c-4297-80a5-3afb7d02548b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308bd8fc-9636-4b16-9e2c-a6ce858d5af0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "568c797d-141f-47eb-9648-29130f706f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features_percentile=100\n",
    "test_size=0.1\n",
    "mode='self-supervision'\n",
    "learning_rate=0.0001\n",
    "repetitions=1\n",
    "epochs=2000\n",
    "verbose=1\n",
    "seed=0\n",
    "embedding_size = 128\n",
    "num_heads = 2\n",
    "num_layers = 8\n",
    "\n",
    "loss = CompositeLoss(feature_w=1, value_w=0.1) # Contribution of individual losses (predicts keys, values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc549ab4-af3f-4652-85ec-bfe9d635441b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10184, 46)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/dataset-pretrain.data.csv')\n",
    "features = data.columns[-44:].tolist()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a05e947-03f0-4ce0-abaf-bd27fd4258bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /home/shenwanxiang/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/sh)\n",
      "rm: cannot remove './FoundationModel': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r ./FoundationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ea6168-00ff-4b1c-88d7-7aeed37802be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO\t2025-08-14 15:25:59,577\tSetting up working directory: ./FoundationModel/\n",
      "INFO\t2025-08-14 15:25:59,579\tNumber of continuous features: 44\n",
      "INFO\t2025-08-14 15:25:59,579\tNumber of discrete features: 0\n",
      "INFO\t2025-08-14 15:25:59,579\tNumber of samples: 10184\n",
      "INFO\t2025-08-14 15:25:59,593\tNumber of classes: 52\n",
      "INFO\t2025-08-14 15:25:59,593\tRUN ID: fold-0_id-0\n",
      "INFO\t2025-08-14 15:25:59,594\tRUN ID out directory: ./FoundationModel//fold-0_id-0/\n",
      "INFO\t2025-08-14 15:26:00,537\tTraining samples: 9165\n",
      "INFO\t2025-08-14 15:26:00,537\tTesting samples: 1019\n",
      "INFO\t2025-08-14 15:26:00,562\tNumber of features at 100th percentile: 44 that are non nans\n",
      "2025-08-14 15:26:00.563029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.563177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.563238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-14 15:26:00.564534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9998 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-08-14 15:26:01.217273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8220597320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,497\tAutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8220597320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f8220597320> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f833de79450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,503\tAutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f833de79450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f833de79450>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f833de79f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,512\tAutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f833de79f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f833de79f10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f833de799d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,518\tAutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f833de799d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f833de799d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f833de86e10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,527\tAutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f833de86e10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f833de86e10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f833de92f90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,536\tAutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f833de92f90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f833de92f90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f833de9fad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:01,541\tAutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f833de9fad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f833de9fad0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f834f90a290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:02,169\tAutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f834f90a290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f834f90a290>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f820438ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:02,820\tAutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f820438ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f820438ddd0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 15:26:07.516550: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x83dea220 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-14 15:26:07.516575: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2025-08-14 15:26:07.520510: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-14 15:26:07.559438: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-14 15:26:07.579687: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - ETA: 0s - loss: 0.9375 WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f8184111ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-14 15:26:13,225\tAutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f8184111ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f8184111ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "2/2 [==============================] - 13s 1s/step - loss: 0.9375 - val_loss: 0.8447\n",
      "Epoch 2/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.8864 - val_loss: 0.8035\n",
      "Epoch 3/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.8583 - val_loss: 0.7924\n",
      "Epoch 4/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.8467 - val_loss: 0.7875\n",
      "Epoch 5/2000\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.8377 - val_loss: 0.7781\n",
      "Epoch 6/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.8309 - val_loss: 0.7713\n",
      "Epoch 7/2000\n",
      "2/2 [==============================] - 1s 392ms/step - loss: 0.8239 - val_loss: 0.7681\n",
      "Epoch 8/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.8203 - val_loss: 0.7649\n",
      "Epoch 9/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.8177 - val_loss: 0.7637\n",
      "Epoch 10/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.8128 - val_loss: 0.7623\n",
      "Epoch 11/2000\n",
      "2/2 [==============================] - 1s 393ms/step - loss: 0.8093 - val_loss: 0.7609\n",
      "Epoch 12/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.8066 - val_loss: 0.7600\n",
      "Epoch 13/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.8044 - val_loss: 0.7566\n",
      "Epoch 14/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.8011 - val_loss: 0.7550\n",
      "Epoch 15/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.8006 - val_loss: 0.7530\n",
      "Epoch 16/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.7974 - val_loss: 0.7526\n",
      "Epoch 17/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.7956 - val_loss: 0.7514\n",
      "Epoch 18/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.7945 - val_loss: 0.7513\n",
      "Epoch 19/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7931 - val_loss: 0.7510\n",
      "Epoch 20/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7923 - val_loss: 0.7504\n",
      "Epoch 21/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.7901 - val_loss: 0.7499\n",
      "Epoch 22/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7892 - val_loss: 0.7498\n",
      "Epoch 23/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7876 - val_loss: 0.7490\n",
      "Epoch 24/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7869 - val_loss: 0.7491\n",
      "Epoch 25/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.7859 - val_loss: 0.7482\n",
      "Epoch 26/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7846 - val_loss: 0.7478\n",
      "Epoch 27/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7842 - val_loss: 0.7471\n",
      "Epoch 28/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7833 - val_loss: 0.7474\n",
      "Epoch 29/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7823 - val_loss: 0.7469\n",
      "Epoch 30/2000\n",
      "2/2 [==============================] - 1s 394ms/step - loss: 0.7811 - val_loss: 0.7466\n",
      "Epoch 31/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7800 - val_loss: 0.7466\n",
      "Epoch 32/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7798 - val_loss: 0.7462\n",
      "Epoch 33/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7787 - val_loss: 0.7460\n",
      "Epoch 34/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7787 - val_loss: 0.7462\n",
      "Epoch 35/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7780 - val_loss: 0.7457\n",
      "Epoch 36/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.7772 - val_loss: 0.7450\n",
      "Epoch 37/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7767 - val_loss: 0.7451\n",
      "Epoch 38/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7750 - val_loss: 0.7447\n",
      "Epoch 39/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.7753 - val_loss: 0.7443\n",
      "Epoch 40/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7744 - val_loss: 0.7438\n",
      "Epoch 41/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7732 - val_loss: 0.7436\n",
      "Epoch 42/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.7725 - val_loss: 0.7428\n",
      "Epoch 43/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7715 - val_loss: 0.7417\n",
      "Epoch 44/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7707 - val_loss: 0.7410\n",
      "Epoch 45/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7704 - val_loss: 0.7399\n",
      "Epoch 46/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.7704 - val_loss: 0.7386\n",
      "Epoch 47/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.7695 - val_loss: 0.7368\n",
      "Epoch 48/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.7669 - val_loss: 0.7340\n",
      "Epoch 49/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7646 - val_loss: 0.7311\n",
      "Epoch 50/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7629 - val_loss: 0.7268\n",
      "Epoch 51/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7600 - val_loss: 0.7208\n",
      "Epoch 52/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7564 - val_loss: 0.7122\n",
      "Epoch 53/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.7516 - val_loss: 0.7062\n",
      "Epoch 54/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.7457 - val_loss: 0.7019\n",
      "Epoch 55/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.7408 - val_loss: 0.6981\n",
      "Epoch 56/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.7357 - val_loss: 0.6944\n",
      "Epoch 57/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.7310 - val_loss: 0.6895\n",
      "Epoch 58/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.7254 - val_loss: 0.6847\n",
      "Epoch 59/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.7206 - val_loss: 0.6813\n",
      "Epoch 60/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7149 - val_loss: 0.6772\n",
      "Epoch 61/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.7120 - val_loss: 0.6726\n",
      "Epoch 62/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.7083 - val_loss: 0.6710\n",
      "Epoch 63/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.7040 - val_loss: 0.6687\n",
      "Epoch 64/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.7012 - val_loss: 0.6643\n",
      "Epoch 65/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6988 - val_loss: 0.6597\n",
      "Epoch 66/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.6956 - val_loss: 0.6582\n",
      "Epoch 67/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.6917 - val_loss: 0.6571\n",
      "Epoch 68/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6898 - val_loss: 0.6533\n",
      "Epoch 69/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6877 - val_loss: 0.6482\n",
      "Epoch 70/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6839 - val_loss: 0.6467\n",
      "Epoch 71/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.6816 - val_loss: 0.6436\n",
      "Epoch 72/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6790 - val_loss: 0.6429\n",
      "Epoch 73/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.6752 - val_loss: 0.6402\n",
      "Epoch 74/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6737 - val_loss: 0.6379\n",
      "Epoch 75/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.6710 - val_loss: 0.6368\n",
      "Epoch 76/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6689 - val_loss: 0.6362\n",
      "Epoch 77/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6671 - val_loss: 0.6319\n",
      "Epoch 78/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6646 - val_loss: 0.6314\n",
      "Epoch 79/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6635 - val_loss: 0.6300\n",
      "Epoch 80/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6604 - val_loss: 0.6294\n",
      "Epoch 81/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6588 - val_loss: 0.6263\n",
      "Epoch 82/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6578 - val_loss: 0.6263\n",
      "Epoch 83/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6555 - val_loss: 0.6228\n",
      "Epoch 84/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6538 - val_loss: 0.6215\n",
      "Epoch 85/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6521 - val_loss: 0.6195\n",
      "Epoch 86/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6506 - val_loss: 0.6157\n",
      "Epoch 87/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6498 - val_loss: 0.6141\n",
      "Epoch 88/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6483 - val_loss: 0.6147\n",
      "Epoch 89/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6458 - val_loss: 0.6144\n",
      "Epoch 90/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.6455 - val_loss: 0.6104\n",
      "Epoch 91/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.6433 - val_loss: 0.6075\n",
      "Epoch 92/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6425 - val_loss: 0.6068\n",
      "Epoch 93/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6406 - val_loss: 0.6068\n",
      "Epoch 94/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6386 - val_loss: 0.6047\n",
      "Epoch 95/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6385 - val_loss: 0.6048\n",
      "Epoch 96/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6371 - val_loss: 0.6043\n",
      "Epoch 97/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6358 - val_loss: 0.6022\n",
      "Epoch 98/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6352 - val_loss: 0.6027\n",
      "Epoch 99/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6340 - val_loss: 0.6003\n",
      "Epoch 100/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6334 - val_loss: 0.6014\n",
      "Epoch 101/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6321 - val_loss: 0.5992\n",
      "Epoch 102/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.6297 - val_loss: 0.5973\n",
      "Epoch 103/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6294 - val_loss: 0.5991\n",
      "Epoch 104/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6293 - val_loss: 0.5950\n",
      "Epoch 105/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6284 - val_loss: 0.5957\n",
      "Epoch 106/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6271 - val_loss: 0.5953\n",
      "Epoch 107/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6263 - val_loss: 0.5941\n",
      "Epoch 108/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6254 - val_loss: 0.5922\n",
      "Epoch 109/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.6244 - val_loss: 0.5899\n",
      "Epoch 110/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6241 - val_loss: 0.5912\n",
      "Epoch 111/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6235 - val_loss: 0.5910\n",
      "Epoch 112/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6223 - val_loss: 0.5910\n",
      "Epoch 113/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6216 - val_loss: 0.5861\n",
      "Epoch 114/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6201 - val_loss: 0.5858\n",
      "Epoch 115/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6183 - val_loss: 0.5880\n",
      "Epoch 116/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6181 - val_loss: 0.5866\n",
      "Epoch 117/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6182 - val_loss: 0.5871\n",
      "Epoch 118/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6166 - val_loss: 0.5855\n",
      "Epoch 119/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6171 - val_loss: 0.5830\n",
      "Epoch 120/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6156 - val_loss: 0.5821\n",
      "Epoch 121/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.6152 - val_loss: 0.5839\n",
      "Epoch 122/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6147 - val_loss: 0.5793\n",
      "Epoch 123/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6129 - val_loss: 0.5802\n",
      "Epoch 124/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6112 - val_loss: 0.5775\n",
      "Epoch 125/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6117 - val_loss: 0.5763\n",
      "Epoch 126/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6111 - val_loss: 0.5747\n",
      "Epoch 127/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.6100 - val_loss: 0.5760\n",
      "Epoch 128/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.6090 - val_loss: 0.5742\n",
      "Epoch 129/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.6079 - val_loss: 0.5752\n",
      "Epoch 130/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.6074 - val_loss: 0.5742\n",
      "Epoch 131/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.6064 - val_loss: 0.5746\n",
      "Epoch 132/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6053 - val_loss: 0.5711\n",
      "Epoch 133/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6050 - val_loss: 0.5712\n",
      "Epoch 134/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6030 - val_loss: 0.5691\n",
      "Epoch 135/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.6028 - val_loss: 0.5680\n",
      "Epoch 136/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.6025 - val_loss: 0.5682\n",
      "Epoch 137/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.6021 - val_loss: 0.5670\n",
      "Epoch 138/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.6015 - val_loss: 0.5648\n",
      "Epoch 139/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5992 - val_loss: 0.5626\n",
      "Epoch 140/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5987 - val_loss: 0.5657\n",
      "Epoch 141/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5987 - val_loss: 0.5643\n",
      "Epoch 142/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5976 - val_loss: 0.5655\n",
      "Epoch 143/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.5978 - val_loss: 0.5586\n",
      "Epoch 144/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5960 - val_loss: 0.5571\n",
      "Epoch 145/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5962 - val_loss: 0.5580\n",
      "Epoch 146/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5951 - val_loss: 0.5582\n",
      "Epoch 147/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.5929 - val_loss: 0.5543\n",
      "Epoch 148/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5933 - val_loss: 0.5575\n",
      "Epoch 149/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5917 - val_loss: 0.5569\n",
      "Epoch 150/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5919 - val_loss: 0.5521\n",
      "Epoch 151/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5899 - val_loss: 0.5521\n",
      "Epoch 152/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5894 - val_loss: 0.5506\n",
      "Epoch 153/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5903 - val_loss: 0.5521\n",
      "Epoch 154/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5895 - val_loss: 0.5490\n",
      "Epoch 155/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5869 - val_loss: 0.5509\n",
      "Epoch 156/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5881 - val_loss: 0.5484\n",
      "Epoch 157/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5877 - val_loss: 0.5496\n",
      "Epoch 158/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5852 - val_loss: 0.5479\n",
      "Epoch 159/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5851 - val_loss: 0.5469\n",
      "Epoch 160/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5841 - val_loss: 0.5456\n",
      "Epoch 161/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5840 - val_loss: 0.5423\n",
      "Epoch 162/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5831 - val_loss: 0.5415\n",
      "Epoch 163/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5824 - val_loss: 0.5448\n",
      "Epoch 164/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5818 - val_loss: 0.5411\n",
      "Epoch 165/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.5812 - val_loss: 0.5429\n",
      "Epoch 166/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5809 - val_loss: 0.5401\n",
      "Epoch 167/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5798 - val_loss: 0.5375\n",
      "Epoch 168/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5781 - val_loss: 0.5373\n",
      "Epoch 169/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5786 - val_loss: 0.5378\n",
      "Epoch 170/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5776 - val_loss: 0.5381\n",
      "Epoch 171/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5775 - val_loss: 0.5370\n",
      "Epoch 172/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5768 - val_loss: 0.5347\n",
      "Epoch 173/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5763 - val_loss: 0.5345\n",
      "Epoch 174/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5758 - val_loss: 0.5312\n",
      "Epoch 175/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5767 - val_loss: 0.5366\n",
      "Epoch 176/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5750 - val_loss: 0.5291\n",
      "Epoch 177/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5746 - val_loss: 0.5312\n",
      "Epoch 178/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5734 - val_loss: 0.5281\n",
      "Epoch 179/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5717 - val_loss: 0.5307\n",
      "Epoch 180/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5720 - val_loss: 0.5243\n",
      "Epoch 181/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5710 - val_loss: 0.5314\n",
      "Epoch 182/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5693 - val_loss: 0.5291\n",
      "Epoch 183/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5706 - val_loss: 0.5263\n",
      "Epoch 184/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5701 - val_loss: 0.5245\n",
      "Epoch 185/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5692 - val_loss: 0.5271\n",
      "Epoch 186/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5668 - val_loss: 0.5217\n",
      "Epoch 187/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5672 - val_loss: 0.5213\n",
      "Epoch 188/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5666 - val_loss: 0.5228\n",
      "Epoch 189/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5658 - val_loss: 0.5224\n",
      "Epoch 190/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5664 - val_loss: 0.5162\n",
      "Epoch 191/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5638 - val_loss: 0.5199\n",
      "Epoch 192/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5643 - val_loss: 0.5216\n",
      "Epoch 193/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5637 - val_loss: 0.5198\n",
      "Epoch 194/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.5641 - val_loss: 0.5168\n",
      "Epoch 195/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5618 - val_loss: 0.5189\n",
      "Epoch 196/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5615 - val_loss: 0.5130\n",
      "Epoch 197/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5613 - val_loss: 0.5129\n",
      "Epoch 198/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5614 - val_loss: 0.5156\n",
      "Epoch 199/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5601 - val_loss: 0.5153\n",
      "Epoch 200/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5610 - val_loss: 0.5121\n",
      "Epoch 201/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5592 - val_loss: 0.5150\n",
      "Epoch 202/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5591 - val_loss: 0.5152\n",
      "Epoch 203/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5578 - val_loss: 0.5115\n",
      "Epoch 204/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5579 - val_loss: 0.5116\n",
      "Epoch 205/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5571 - val_loss: 0.5078\n",
      "Epoch 206/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5559 - val_loss: 0.5093\n",
      "Epoch 207/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5567 - val_loss: 0.5127\n",
      "Epoch 208/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5552 - val_loss: 0.5063\n",
      "Epoch 209/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.5553 - val_loss: 0.5092\n",
      "Epoch 210/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5545 - val_loss: 0.5064\n",
      "Epoch 211/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5519 - val_loss: 0.5077\n",
      "Epoch 212/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5532 - val_loss: 0.5082\n",
      "Epoch 213/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5522 - val_loss: 0.5067\n",
      "Epoch 214/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5524 - val_loss: 0.5042\n",
      "Epoch 215/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5516 - val_loss: 0.5023\n",
      "Epoch 216/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5507 - val_loss: 0.5023\n",
      "Epoch 217/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5489 - val_loss: 0.5042\n",
      "Epoch 218/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5498 - val_loss: 0.5008\n",
      "Epoch 219/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5496 - val_loss: 0.5027\n",
      "Epoch 220/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5483 - val_loss: 0.5038\n",
      "Epoch 221/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5494 - val_loss: 0.5005\n",
      "Epoch 222/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5477 - val_loss: 0.4991\n",
      "Epoch 223/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5469 - val_loss: 0.4968\n",
      "Epoch 224/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5471 - val_loss: 0.4996\n",
      "Epoch 225/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5461 - val_loss: 0.4986\n",
      "Epoch 226/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5450 - val_loss: 0.4950\n",
      "Epoch 227/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5450 - val_loss: 0.4986\n",
      "Epoch 228/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5447 - val_loss: 0.4952\n",
      "Epoch 229/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5442 - val_loss: 0.4915\n",
      "Epoch 230/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5434 - val_loss: 0.4927\n",
      "Epoch 231/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5428 - val_loss: 0.4951\n",
      "Epoch 232/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5407 - val_loss: 0.4908\n",
      "Epoch 233/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.5423 - val_loss: 0.4862\n",
      "Epoch 234/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5426 - val_loss: 0.4907\n",
      "Epoch 235/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5408 - val_loss: 0.4906\n",
      "Epoch 236/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5401 - val_loss: 0.4879\n",
      "Epoch 237/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5389 - val_loss: 0.4891\n",
      "Epoch 238/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.5379 - val_loss: 0.4902\n",
      "Epoch 239/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5382 - val_loss: 0.4866\n",
      "Epoch 240/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5378 - val_loss: 0.4855\n",
      "Epoch 241/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5372 - val_loss: 0.4862\n",
      "Epoch 242/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5373 - val_loss: 0.4862\n",
      "Epoch 243/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5369 - val_loss: 0.4854\n",
      "Epoch 244/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5346 - val_loss: 0.4833\n",
      "Epoch 245/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5357 - val_loss: 0.4862\n",
      "Epoch 246/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5360 - val_loss: 0.4833\n",
      "Epoch 247/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5341 - val_loss: 0.4855\n",
      "Epoch 248/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.5342 - val_loss: 0.4872\n",
      "Epoch 249/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5333 - val_loss: 0.4802\n",
      "Epoch 250/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5337 - val_loss: 0.4857\n",
      "Epoch 251/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5324 - val_loss: 0.4839\n",
      "Epoch 252/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5317 - val_loss: 0.4800\n",
      "Epoch 253/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5315 - val_loss: 0.4815\n",
      "Epoch 254/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5315 - val_loss: 0.4806\n",
      "Epoch 255/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5305 - val_loss: 0.4796\n",
      "Epoch 256/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5294 - val_loss: 0.4780\n",
      "Epoch 257/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5304 - val_loss: 0.4780\n",
      "Epoch 258/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5301 - val_loss: 0.4759\n",
      "Epoch 259/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5274 - val_loss: 0.4795\n",
      "Epoch 260/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5285 - val_loss: 0.4743\n",
      "Epoch 261/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5273 - val_loss: 0.4745\n",
      "Epoch 262/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5275 - val_loss: 0.4761\n",
      "Epoch 263/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5254 - val_loss: 0.4749\n",
      "Epoch 264/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5269 - val_loss: 0.4740\n",
      "Epoch 265/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5262 - val_loss: 0.4749\n",
      "Epoch 266/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5253 - val_loss: 0.4707\n",
      "Epoch 267/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5255 - val_loss: 0.4702\n",
      "Epoch 268/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5241 - val_loss: 0.4694\n",
      "Epoch 269/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5234 - val_loss: 0.4718\n",
      "Epoch 270/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5229 - val_loss: 0.4718\n",
      "Epoch 271/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.5233 - val_loss: 0.4666\n",
      "Epoch 272/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5239 - val_loss: 0.4681\n",
      "Epoch 273/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5225 - val_loss: 0.4673\n",
      "Epoch 274/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5214 - val_loss: 0.4677\n",
      "Epoch 275/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5215 - val_loss: 0.4682\n",
      "Epoch 276/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5214 - val_loss: 0.4690\n",
      "Epoch 277/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5206 - val_loss: 0.4672\n",
      "Epoch 278/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5207 - val_loss: 0.4660\n",
      "Epoch 279/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5189 - val_loss: 0.4652\n",
      "Epoch 280/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5185 - val_loss: 0.4671\n",
      "Epoch 281/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5193 - val_loss: 0.4701\n",
      "Epoch 282/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5192 - val_loss: 0.4659\n",
      "Epoch 283/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5177 - val_loss: 0.4656\n",
      "Epoch 284/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.5185 - val_loss: 0.4665\n",
      "Epoch 285/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5169 - val_loss: 0.4657\n",
      "Epoch 286/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5166 - val_loss: 0.4627\n",
      "Epoch 287/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5167 - val_loss: 0.4606\n",
      "Epoch 288/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5162 - val_loss: 0.4634\n",
      "Epoch 289/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5141 - val_loss: 0.4595\n",
      "Epoch 290/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5143 - val_loss: 0.4580\n",
      "Epoch 291/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5143 - val_loss: 0.4587\n",
      "Epoch 292/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.5137 - val_loss: 0.4608\n",
      "Epoch 293/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5140 - val_loss: 0.4566\n",
      "Epoch 294/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5138 - val_loss: 0.4573\n",
      "Epoch 295/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5128 - val_loss: 0.4600\n",
      "Epoch 296/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5121 - val_loss: 0.4586\n",
      "Epoch 297/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5117 - val_loss: 0.4581\n",
      "Epoch 298/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.5121 - val_loss: 0.4601\n",
      "Epoch 299/2000\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.5118 - val_loss: 0.4603\n",
      "Epoch 300/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5113 - val_loss: 0.4574\n",
      "Epoch 301/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5106 - val_loss: 0.4591\n",
      "Epoch 302/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5105 - val_loss: 0.4555\n",
      "Epoch 303/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5104 - val_loss: 0.4553\n",
      "Epoch 304/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5087 - val_loss: 0.4518\n",
      "Epoch 305/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5095 - val_loss: 0.4556\n",
      "Epoch 306/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5086 - val_loss: 0.4574\n",
      "Epoch 307/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5080 - val_loss: 0.4523\n",
      "Epoch 308/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5077 - val_loss: 0.4495\n",
      "Epoch 309/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5074 - val_loss: 0.4535\n",
      "Epoch 310/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5083 - val_loss: 0.4498\n",
      "Epoch 311/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.5068 - val_loss: 0.4500\n",
      "Epoch 312/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5069 - val_loss: 0.4555\n",
      "Epoch 313/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5067 - val_loss: 0.4542\n",
      "Epoch 314/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5074 - val_loss: 0.4502\n",
      "Epoch 315/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5053 - val_loss: 0.4477\n",
      "Epoch 316/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5053 - val_loss: 0.4493\n",
      "Epoch 317/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5056 - val_loss: 0.4500\n",
      "Epoch 318/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5039 - val_loss: 0.4497\n",
      "Epoch 319/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.5028 - val_loss: 0.4504\n",
      "Epoch 320/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.5037 - val_loss: 0.4493\n",
      "Epoch 321/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5037 - val_loss: 0.4483\n",
      "Epoch 322/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.5017 - val_loss: 0.4483\n",
      "Epoch 323/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5017 - val_loss: 0.4498\n",
      "Epoch 324/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5028 - val_loss: 0.4481\n",
      "Epoch 325/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.5014 - val_loss: 0.4456\n",
      "Epoch 326/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5006 - val_loss: 0.4467\n",
      "Epoch 327/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.5001 - val_loss: 0.4478\n",
      "Epoch 328/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.5006 - val_loss: 0.4489\n",
      "Epoch 329/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5004 - val_loss: 0.4448\n",
      "Epoch 330/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.5004 - val_loss: 0.4465\n",
      "Epoch 331/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4995 - val_loss: 0.4464\n",
      "Epoch 332/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4987 - val_loss: 0.4435\n",
      "Epoch 333/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4981 - val_loss: 0.4433\n",
      "Epoch 334/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4986 - val_loss: 0.4426\n",
      "Epoch 335/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.4976 - val_loss: 0.4419\n",
      "Epoch 336/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4968 - val_loss: 0.4431\n",
      "Epoch 337/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4965 - val_loss: 0.4410\n",
      "Epoch 338/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4975 - val_loss: 0.4425\n",
      "Epoch 339/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4969 - val_loss: 0.4432\n",
      "Epoch 340/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4953 - val_loss: 0.4400\n",
      "Epoch 341/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4979 - val_loss: 0.4407\n",
      "Epoch 342/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4950 - val_loss: 0.4388\n",
      "Epoch 343/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4955 - val_loss: 0.4422\n",
      "Epoch 344/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4951 - val_loss: 0.4379\n",
      "Epoch 345/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4946 - val_loss: 0.4397\n",
      "Epoch 346/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4946 - val_loss: 0.4411\n",
      "Epoch 347/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.4932 - val_loss: 0.4373\n",
      "Epoch 348/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4929 - val_loss: 0.4356\n",
      "Epoch 349/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4931 - val_loss: 0.4374\n",
      "Epoch 350/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4931 - val_loss: 0.4392\n",
      "Epoch 351/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4913 - val_loss: 0.4353\n",
      "Epoch 352/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.4928 - val_loss: 0.4359\n",
      "Epoch 353/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4920 - val_loss: 0.4380\n",
      "Epoch 354/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4928 - val_loss: 0.4356\n",
      "Epoch 355/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4908 - val_loss: 0.4353\n",
      "Epoch 356/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4902 - val_loss: 0.4354\n",
      "Epoch 357/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4891 - val_loss: 0.4328\n",
      "Epoch 358/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4890 - val_loss: 0.4338\n",
      "Epoch 359/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.4893 - val_loss: 0.4297\n",
      "Epoch 360/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4892 - val_loss: 0.4302\n",
      "Epoch 361/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4884 - val_loss: 0.4335\n",
      "Epoch 362/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4876 - val_loss: 0.4330\n",
      "Epoch 363/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4880 - val_loss: 0.4306\n",
      "Epoch 364/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4879 - val_loss: 0.4319\n",
      "Epoch 365/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4877 - val_loss: 0.4291\n",
      "Epoch 366/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4870 - val_loss: 0.4338\n",
      "Epoch 367/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4866 - val_loss: 0.4313\n",
      "Epoch 368/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.4876 - val_loss: 0.4289\n",
      "Epoch 369/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4862 - val_loss: 0.4279\n",
      "Epoch 370/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4860 - val_loss: 0.4272\n",
      "Epoch 371/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4846 - val_loss: 0.4288\n",
      "Epoch 372/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4852 - val_loss: 0.4290\n",
      "Epoch 373/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4850 - val_loss: 0.4292\n",
      "Epoch 374/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4840 - val_loss: 0.4278\n",
      "Epoch 375/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4840 - val_loss: 0.4241\n",
      "Epoch 376/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4838 - val_loss: 0.4272\n",
      "Epoch 377/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4836 - val_loss: 0.4294\n",
      "Epoch 378/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4836 - val_loss: 0.4266\n",
      "Epoch 379/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4807 - val_loss: 0.4308\n",
      "Epoch 380/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4813 - val_loss: 0.4247\n",
      "Epoch 381/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4821 - val_loss: 0.4221\n",
      "Epoch 382/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4817 - val_loss: 0.4261\n",
      "Epoch 383/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4812 - val_loss: 0.4219\n",
      "Epoch 384/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4823 - val_loss: 0.4258\n",
      "Epoch 385/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4809 - val_loss: 0.4237\n",
      "Epoch 386/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4808 - val_loss: 0.4246\n",
      "Epoch 387/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4793 - val_loss: 0.4204\n",
      "Epoch 388/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4790 - val_loss: 0.4262\n",
      "Epoch 389/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4784 - val_loss: 0.4248\n",
      "Epoch 390/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4789 - val_loss: 0.4239\n",
      "Epoch 391/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4793 - val_loss: 0.4197\n",
      "Epoch 392/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4778 - val_loss: 0.4253\n",
      "Epoch 393/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4777 - val_loss: 0.4225\n",
      "Epoch 394/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4769 - val_loss: 0.4237\n",
      "Epoch 395/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4776 - val_loss: 0.4196\n",
      "Epoch 396/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4768 - val_loss: 0.4180\n",
      "Epoch 397/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4773 - val_loss: 0.4200\n",
      "Epoch 398/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4762 - val_loss: 0.4166\n",
      "Epoch 399/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4764 - val_loss: 0.4212\n",
      "Epoch 400/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4748 - val_loss: 0.4197\n",
      "Epoch 401/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4752 - val_loss: 0.4182\n",
      "Epoch 402/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4751 - val_loss: 0.4198\n",
      "Epoch 403/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4752 - val_loss: 0.4188\n",
      "Epoch 404/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4752 - val_loss: 0.4156\n",
      "Epoch 405/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4746 - val_loss: 0.4157\n",
      "Epoch 406/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4745 - val_loss: 0.4164\n",
      "Epoch 407/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4746 - val_loss: 0.4146\n",
      "Epoch 408/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4721 - val_loss: 0.4185\n",
      "Epoch 409/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4724 - val_loss: 0.4129\n",
      "Epoch 410/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4720 - val_loss: 0.4151\n",
      "Epoch 411/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4727 - val_loss: 0.4137\n",
      "Epoch 412/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4714 - val_loss: 0.4150\n",
      "Epoch 413/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4716 - val_loss: 0.4153\n",
      "Epoch 414/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.4710 - val_loss: 0.4154\n",
      "Epoch 415/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4722 - val_loss: 0.4123\n",
      "Epoch 416/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4717 - val_loss: 0.4104\n",
      "Epoch 417/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4698 - val_loss: 0.4111\n",
      "Epoch 418/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4707 - val_loss: 0.4122\n",
      "Epoch 419/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4713 - val_loss: 0.4110\n",
      "Epoch 420/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4709 - val_loss: 0.4122\n",
      "Epoch 421/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4684 - val_loss: 0.4149\n",
      "Epoch 422/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4691 - val_loss: 0.4125\n",
      "Epoch 423/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4679 - val_loss: 0.4079\n",
      "Epoch 424/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.4674 - val_loss: 0.4080\n",
      "Epoch 425/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4677 - val_loss: 0.4088\n",
      "Epoch 426/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4669 - val_loss: 0.4067\n",
      "Epoch 427/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4681 - val_loss: 0.4100\n",
      "Epoch 428/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4674 - val_loss: 0.4035\n",
      "Epoch 429/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4658 - val_loss: 0.4084\n",
      "Epoch 430/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4660 - val_loss: 0.4124\n",
      "Epoch 431/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4664 - val_loss: 0.4065\n",
      "Epoch 432/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4665 - val_loss: 0.4095\n",
      "Epoch 433/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4647 - val_loss: 0.4059\n",
      "Epoch 434/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4662 - val_loss: 0.4069\n",
      "Epoch 435/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4654 - val_loss: 0.4064\n",
      "Epoch 436/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.4640 - val_loss: 0.4049\n",
      "Epoch 437/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4639 - val_loss: 0.4038\n",
      "Epoch 438/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4630 - val_loss: 0.4052\n",
      "Epoch 439/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4639 - val_loss: 0.4040\n",
      "Epoch 440/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4623 - val_loss: 0.4045\n",
      "Epoch 441/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4639 - val_loss: 0.4027\n",
      "Epoch 442/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4626 - val_loss: 0.4017\n",
      "Epoch 443/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4632 - val_loss: 0.4053\n",
      "Epoch 444/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4628 - val_loss: 0.4061\n",
      "Epoch 445/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4627 - val_loss: 0.4017\n",
      "Epoch 446/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4618 - val_loss: 0.4024\n",
      "Epoch 447/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.4612 - val_loss: 0.4037\n",
      "Epoch 448/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4612 - val_loss: 0.4030\n",
      "Epoch 449/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.4620 - val_loss: 0.4037\n",
      "Epoch 450/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4602 - val_loss: 0.4039\n",
      "Epoch 451/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4607 - val_loss: 0.4031\n",
      "Epoch 452/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4597 - val_loss: 0.3973\n",
      "Epoch 453/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4595 - val_loss: 0.4004\n",
      "Epoch 454/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4588 - val_loss: 0.3988\n",
      "Epoch 455/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4592 - val_loss: 0.4006\n",
      "Epoch 456/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.4595 - val_loss: 0.3977\n",
      "Epoch 457/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4593 - val_loss: 0.3964\n",
      "Epoch 458/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4574 - val_loss: 0.4009\n",
      "Epoch 459/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4575 - val_loss: 0.3978\n",
      "Epoch 460/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4578 - val_loss: 0.3977\n",
      "Epoch 461/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4572 - val_loss: 0.4001\n",
      "Epoch 462/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4565 - val_loss: 0.3969\n",
      "Epoch 463/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4578 - val_loss: 0.3938\n",
      "Epoch 464/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4567 - val_loss: 0.3958\n",
      "Epoch 465/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.4561 - val_loss: 0.3947\n",
      "Epoch 466/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4566 - val_loss: 0.3950\n",
      "Epoch 467/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4562 - val_loss: 0.3987\n",
      "Epoch 468/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4554 - val_loss: 0.3972\n",
      "Epoch 469/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4550 - val_loss: 0.3972\n",
      "Epoch 470/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4553 - val_loss: 0.3959\n",
      "Epoch 471/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4545 - val_loss: 0.3970\n",
      "Epoch 472/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4533 - val_loss: 0.3961\n",
      "Epoch 473/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4541 - val_loss: 0.3944\n",
      "Epoch 474/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4518 - val_loss: 0.3921\n",
      "Epoch 475/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4534 - val_loss: 0.3976\n",
      "Epoch 476/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4524 - val_loss: 0.3960\n",
      "Epoch 477/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4529 - val_loss: 0.3919\n",
      "Epoch 478/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4517 - val_loss: 0.3951\n",
      "Epoch 479/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4518 - val_loss: 0.3934\n",
      "Epoch 480/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4516 - val_loss: 0.3911\n",
      "Epoch 481/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4513 - val_loss: 0.3918\n",
      "Epoch 482/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4510 - val_loss: 0.3928\n",
      "Epoch 483/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.4509 - val_loss: 0.3916\n",
      "Epoch 484/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.4498 - val_loss: 0.3919\n",
      "Epoch 485/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4495 - val_loss: 0.3927\n",
      "Epoch 486/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4488 - val_loss: 0.3894\n",
      "Epoch 487/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4518 - val_loss: 0.3896\n",
      "Epoch 488/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4494 - val_loss: 0.3911\n",
      "Epoch 489/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4484 - val_loss: 0.3914\n",
      "Epoch 490/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.4485 - val_loss: 0.3905\n",
      "Epoch 491/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4494 - val_loss: 0.3868\n",
      "Epoch 492/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4495 - val_loss: 0.3941\n",
      "Epoch 493/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4469 - val_loss: 0.3882\n",
      "Epoch 494/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.4477 - val_loss: 0.3864\n",
      "Epoch 495/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4476 - val_loss: 0.3893\n",
      "Epoch 496/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4482 - val_loss: 0.3881\n",
      "Epoch 497/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4467 - val_loss: 0.3869\n",
      "Epoch 498/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4463 - val_loss: 0.3837\n",
      "Epoch 499/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4463 - val_loss: 0.3888\n",
      "Epoch 500/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4461 - val_loss: 0.3878\n",
      "Epoch 501/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4459 - val_loss: 0.3859\n",
      "Epoch 502/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4454 - val_loss: 0.3856\n",
      "Epoch 503/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.4456 - val_loss: 0.3847\n",
      "Epoch 504/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4451 - val_loss: 0.3849\n",
      "Epoch 505/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4445 - val_loss: 0.3838\n",
      "Epoch 506/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4445 - val_loss: 0.3836\n",
      "Epoch 507/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4442 - val_loss: 0.3856\n",
      "Epoch 508/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.4452 - val_loss: 0.3848\n",
      "Epoch 509/2000\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.4447 - val_loss: 0.3879\n",
      "Epoch 510/2000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.4441 - val_loss: 0.3872\n",
      "Epoch 511/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4426 - val_loss: 0.3822\n",
      "Epoch 512/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4435 - val_loss: 0.3838\n",
      "Epoch 513/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4431 - val_loss: 0.3819\n",
      "Epoch 514/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4428 - val_loss: 0.3819\n",
      "Epoch 515/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.4434 - val_loss: 0.3838\n",
      "Epoch 516/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4415 - val_loss: 0.3847\n",
      "Epoch 517/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4408 - val_loss: 0.3827\n",
      "Epoch 518/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.4422 - val_loss: 0.3789\n",
      "Epoch 519/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4411 - val_loss: 0.3810\n",
      "Epoch 520/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.4410 - val_loss: 0.3798\n",
      "Epoch 521/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4404 - val_loss: 0.3795\n",
      "Epoch 522/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4399 - val_loss: 0.3800\n",
      "Epoch 523/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4393 - val_loss: 0.3812\n",
      "Epoch 524/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4399 - val_loss: 0.3787\n",
      "Epoch 525/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4405 - val_loss: 0.3824\n",
      "Epoch 526/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4398 - val_loss: 0.3798\n",
      "Epoch 527/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4400 - val_loss: 0.3775\n",
      "Epoch 528/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4392 - val_loss: 0.3782\n",
      "Epoch 529/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4383 - val_loss: 0.3813\n",
      "Epoch 530/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4383 - val_loss: 0.3788\n",
      "Epoch 531/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4375 - val_loss: 0.3760\n",
      "Epoch 532/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4378 - val_loss: 0.3733\n",
      "Epoch 533/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.4370 - val_loss: 0.3761\n",
      "Epoch 534/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4367 - val_loss: 0.3753\n",
      "Epoch 535/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4379 - val_loss: 0.3810\n",
      "Epoch 536/2000\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.4362 - val_loss: 0.3775\n",
      "Epoch 537/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4360 - val_loss: 0.3754\n",
      "Epoch 538/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4358 - val_loss: 0.3794\n",
      "Epoch 539/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4360 - val_loss: 0.3791\n",
      "Epoch 540/2000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.4361 - val_loss: 0.3755\n",
      "Epoch 541/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4364 - val_loss: 0.3783\n",
      "Epoch 542/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4341 - val_loss: 0.3763\n",
      "Epoch 543/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4347 - val_loss: 0.3799\n",
      "Epoch 544/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4346 - val_loss: 0.3745\n",
      "Epoch 545/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4346 - val_loss: 0.3769\n",
      "Epoch 546/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4337 - val_loss: 0.3756\n",
      "Epoch 547/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4339 - val_loss: 0.3752\n",
      "Epoch 548/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4340 - val_loss: 0.3764\n",
      "Epoch 549/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.4333 - val_loss: 0.3704\n",
      "Epoch 550/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.4326 - val_loss: 0.3748\n",
      "Epoch 551/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4327 - val_loss: 0.3756\n",
      "Epoch 552/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.4333 - val_loss: 0.3769\n",
      "Epoch 553/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4321 - val_loss: 0.3749\n",
      "Epoch 554/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.4315 - val_loss: 0.3725\n",
      "Epoch 555/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4321 - val_loss: 0.3774\n",
      "Epoch 556/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4311 - val_loss: 0.3731\n",
      "Epoch 557/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.4311 - val_loss: 0.3712\n",
      "Epoch 558/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4299 - val_loss: 0.3705\n",
      "Epoch 559/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4316 - val_loss: 0.3762\n",
      "Epoch 560/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4324 - val_loss: 0.3738\n",
      "Epoch 561/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.4313 - val_loss: 0.3673\n",
      "Epoch 562/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.4302 - val_loss: 0.3705\n",
      "Epoch 563/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4299 - val_loss: 0.3690\n",
      "Epoch 564/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4303 - val_loss: 0.3729\n",
      "Epoch 565/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4304 - val_loss: 0.3744\n",
      "Epoch 566/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4295 - val_loss: 0.3721\n",
      "Epoch 567/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4294 - val_loss: 0.3728\n",
      "Epoch 568/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4280 - val_loss: 0.3697\n",
      "Epoch 569/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4289 - val_loss: 0.3715\n",
      "Epoch 570/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4279 - val_loss: 0.3705\n",
      "Epoch 571/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4295 - val_loss: 0.3695\n",
      "Epoch 572/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.4270 - val_loss: 0.3677\n",
      "Epoch 573/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4291 - val_loss: 0.3705\n",
      "Epoch 574/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.4280 - val_loss: 0.3678\n",
      "Epoch 575/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4282 - val_loss: 0.3659\n",
      "Epoch 576/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4285 - val_loss: 0.3678\n",
      "Epoch 577/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4267 - val_loss: 0.3674\n",
      "Epoch 578/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4270 - val_loss: 0.3689\n",
      "Epoch 579/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4269 - val_loss: 0.3681\n",
      "Epoch 580/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4258 - val_loss: 0.3713\n",
      "Epoch 581/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4244 - val_loss: 0.3663\n",
      "Epoch 582/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4254 - val_loss: 0.3677\n",
      "Epoch 583/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4238 - val_loss: 0.3681\n",
      "Epoch 584/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4255 - val_loss: 0.3681\n",
      "Epoch 585/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4224 - val_loss: 0.3675\n",
      "Epoch 586/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.4244 - val_loss: 0.3649\n",
      "Epoch 587/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4254 - val_loss: 0.3661\n",
      "Epoch 588/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4234 - val_loss: 0.3669\n",
      "Epoch 589/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4235 - val_loss: 0.3682\n",
      "Epoch 590/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4234 - val_loss: 0.3627\n",
      "Epoch 591/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4251 - val_loss: 0.3638\n",
      "Epoch 592/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.4231 - val_loss: 0.3675\n",
      "Epoch 593/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4229 - val_loss: 0.3644\n",
      "Epoch 594/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4230 - val_loss: 0.3631\n",
      "Epoch 595/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4237 - val_loss: 0.3619\n",
      "Epoch 596/2000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.4226 - val_loss: 0.3645\n",
      "Epoch 597/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4221 - val_loss: 0.3662\n",
      "Epoch 598/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4219 - val_loss: 0.3600\n",
      "Epoch 599/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4212 - val_loss: 0.3624\n",
      "Epoch 600/2000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.4218 - val_loss: 0.3629\n",
      "Epoch 601/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.4211 - val_loss: 0.3600\n",
      "Epoch 602/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4202 - val_loss: 0.3612\n",
      "Epoch 603/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4209 - val_loss: 0.3639\n",
      "Epoch 604/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4204 - val_loss: 0.3612\n",
      "Epoch 605/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4205 - val_loss: 0.3618\n",
      "Epoch 606/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4196 - val_loss: 0.3639\n",
      "Epoch 607/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4189 - val_loss: 0.3615\n",
      "Epoch 608/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4199 - val_loss: 0.3589\n",
      "Epoch 609/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4186 - val_loss: 0.3658\n",
      "Epoch 610/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4187 - val_loss: 0.3613\n",
      "Epoch 611/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4180 - val_loss: 0.3584\n",
      "Epoch 612/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.4204 - val_loss: 0.3616\n",
      "Epoch 613/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4178 - val_loss: 0.3627\n",
      "Epoch 614/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.4179 - val_loss: 0.3571\n",
      "Epoch 615/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.4174 - val_loss: 0.3596\n",
      "Epoch 616/2000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.4169 - val_loss: 0.3572\n",
      "Epoch 617/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.4169 - val_loss: 0.3599\n",
      "Epoch 618/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4169 - val_loss: 0.3570\n",
      "Epoch 619/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4173 - val_loss: 0.3626\n",
      "Epoch 620/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4171 - val_loss: 0.3639\n",
      "Epoch 621/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.4173 - val_loss: 0.3593\n",
      "Epoch 622/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4158 - val_loss: 0.3557\n",
      "Epoch 623/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.4165 - val_loss: 0.3577\n",
      "Epoch 624/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4170 - val_loss: 0.3549\n",
      "Epoch 625/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4162 - val_loss: 0.3573\n",
      "Epoch 626/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.4145 - val_loss: 0.3561\n",
      "Epoch 627/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4144 - val_loss: 0.3526\n",
      "Epoch 628/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4154 - val_loss: 0.3578\n",
      "Epoch 629/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4153 - val_loss: 0.3517\n",
      "Epoch 630/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.4136 - val_loss: 0.3579\n",
      "Epoch 631/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4140 - val_loss: 0.3539\n",
      "Epoch 632/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4138 - val_loss: 0.3549\n",
      "Epoch 633/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.4143 - val_loss: 0.3574\n",
      "Epoch 634/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4139 - val_loss: 0.3542\n",
      "Epoch 635/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4133 - val_loss: 0.3559\n",
      "Epoch 636/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4129 - val_loss: 0.3550\n",
      "Epoch 637/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4109 - val_loss: 0.3563\n",
      "Epoch 638/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4118 - val_loss: 0.3559\n",
      "Epoch 639/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.4116 - val_loss: 0.3538\n",
      "Epoch 640/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.4123 - val_loss: 0.3515\n",
      "Epoch 641/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4117 - val_loss: 0.3552\n",
      "Epoch 642/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4115 - val_loss: 0.3593\n",
      "Epoch 643/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.4121 - val_loss: 0.3499\n",
      "Epoch 644/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4123 - val_loss: 0.3523\n",
      "Epoch 645/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4101 - val_loss: 0.3507\n",
      "Epoch 646/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4112 - val_loss: 0.3498\n",
      "Epoch 647/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4096 - val_loss: 0.3522\n",
      "Epoch 648/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4103 - val_loss: 0.3510\n",
      "Epoch 649/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4096 - val_loss: 0.3492\n",
      "Epoch 650/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.4099 - val_loss: 0.3521\n",
      "Epoch 651/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4087 - val_loss: 0.3538\n",
      "Epoch 652/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4106 - val_loss: 0.3489\n",
      "Epoch 653/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4082 - val_loss: 0.3519\n",
      "Epoch 654/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4090 - val_loss: 0.3522\n",
      "Epoch 655/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4092 - val_loss: 0.3503\n",
      "Epoch 656/2000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.4087 - val_loss: 0.3481\n",
      "Epoch 657/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4081 - val_loss: 0.3505\n",
      "Epoch 658/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4080 - val_loss: 0.3523\n",
      "Epoch 659/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4084 - val_loss: 0.3487\n",
      "Epoch 660/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.4070 - val_loss: 0.3478\n",
      "Epoch 661/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4067 - val_loss: 0.3494\n",
      "Epoch 662/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4058 - val_loss: 0.3501\n",
      "Epoch 663/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4063 - val_loss: 0.3488\n",
      "Epoch 664/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4067 - val_loss: 0.3490\n",
      "Epoch 665/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4060 - val_loss: 0.3473\n",
      "Epoch 666/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.4063 - val_loss: 0.3501\n",
      "Epoch 667/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.4056 - val_loss: 0.3460\n",
      "Epoch 668/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4063 - val_loss: 0.3473\n",
      "Epoch 669/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4066 - val_loss: 0.3450\n",
      "Epoch 670/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4058 - val_loss: 0.3463\n",
      "Epoch 671/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4051 - val_loss: 0.3451\n",
      "Epoch 672/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4049 - val_loss: 0.3468\n",
      "Epoch 673/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4046 - val_loss: 0.3415\n",
      "Epoch 674/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4042 - val_loss: 0.3445\n",
      "Epoch 675/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4028 - val_loss: 0.3462\n",
      "Epoch 676/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4041 - val_loss: 0.3447\n",
      "Epoch 677/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4037 - val_loss: 0.3466\n",
      "Epoch 678/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.4033 - val_loss: 0.3462\n",
      "Epoch 679/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.4029 - val_loss: 0.3433\n",
      "Epoch 680/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.4031 - val_loss: 0.3454\n",
      "Epoch 681/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4025 - val_loss: 0.3489\n",
      "Epoch 682/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.4035 - val_loss: 0.3441\n",
      "Epoch 683/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.4014 - val_loss: 0.3450\n",
      "Epoch 684/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.4024 - val_loss: 0.3427\n",
      "Epoch 685/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.4001 - val_loss: 0.3438\n",
      "Epoch 686/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.4023 - val_loss: 0.3391\n",
      "Epoch 687/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.4002 - val_loss: 0.3418\n",
      "Epoch 688/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.4015 - val_loss: 0.3422\n",
      "Epoch 689/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.4012 - val_loss: 0.3414\n",
      "Epoch 690/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.4003 - val_loss: 0.3397\n",
      "Epoch 691/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.4018 - val_loss: 0.3415\n",
      "Epoch 692/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.4004 - val_loss: 0.3459\n",
      "Epoch 693/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.4007 - val_loss: 0.3390\n",
      "Epoch 694/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.4003 - val_loss: 0.3421\n",
      "Epoch 695/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.4001 - val_loss: 0.3423\n",
      "Epoch 696/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3999 - val_loss: 0.3405\n",
      "Epoch 697/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3982 - val_loss: 0.3389\n",
      "Epoch 698/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3992 - val_loss: 0.3425\n",
      "Epoch 699/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3993 - val_loss: 0.3422\n",
      "Epoch 700/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3977 - val_loss: 0.3402\n",
      "Epoch 701/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3978 - val_loss: 0.3372\n",
      "Epoch 702/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3978 - val_loss: 0.3399\n",
      "Epoch 703/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3980 - val_loss: 0.3385\n",
      "Epoch 704/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3990 - val_loss: 0.3420\n",
      "Epoch 705/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3970 - val_loss: 0.3378\n",
      "Epoch 706/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3974 - val_loss: 0.3396\n",
      "Epoch 707/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3979 - val_loss: 0.3342\n",
      "Epoch 708/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3971 - val_loss: 0.3405\n",
      "Epoch 709/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3962 - val_loss: 0.3395\n",
      "Epoch 710/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3959 - val_loss: 0.3389\n",
      "Epoch 711/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3964 - val_loss: 0.3399\n",
      "Epoch 712/2000\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.3956 - val_loss: 0.3318\n",
      "Epoch 713/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3961 - val_loss: 0.3324\n",
      "Epoch 714/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3961 - val_loss: 0.3357\n",
      "Epoch 715/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3957 - val_loss: 0.3370\n",
      "Epoch 716/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3947 - val_loss: 0.3352\n",
      "Epoch 717/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3966 - val_loss: 0.3366\n",
      "Epoch 718/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3962 - val_loss: 0.3322\n",
      "Epoch 719/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3957 - val_loss: 0.3365\n",
      "Epoch 720/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3946 - val_loss: 0.3329\n",
      "Epoch 721/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3953 - val_loss: 0.3339\n",
      "Epoch 722/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3942 - val_loss: 0.3351\n",
      "Epoch 723/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3944 - val_loss: 0.3349\n",
      "Epoch 724/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3941 - val_loss: 0.3319\n",
      "Epoch 725/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3935 - val_loss: 0.3366\n",
      "Epoch 726/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3923 - val_loss: 0.3327\n",
      "Epoch 727/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3929 - val_loss: 0.3327\n",
      "Epoch 728/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3933 - val_loss: 0.3323\n",
      "Epoch 729/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3929 - val_loss: 0.3319\n",
      "Epoch 730/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3906 - val_loss: 0.3330\n",
      "Epoch 731/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3932 - val_loss: 0.3347\n",
      "Epoch 732/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3924 - val_loss: 0.3346\n",
      "Epoch 733/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3913 - val_loss: 0.3345\n",
      "Epoch 734/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3920 - val_loss: 0.3333\n",
      "Epoch 735/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3920 - val_loss: 0.3322\n",
      "Epoch 736/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3910 - val_loss: 0.3327\n",
      "Epoch 737/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3901 - val_loss: 0.3307\n",
      "Epoch 738/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3903 - val_loss: 0.3349\n",
      "Epoch 739/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3901 - val_loss: 0.3336\n",
      "Epoch 740/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3914 - val_loss: 0.3296\n",
      "Epoch 741/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3898 - val_loss: 0.3310\n",
      "Epoch 742/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3899 - val_loss: 0.3311\n",
      "Epoch 743/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3893 - val_loss: 0.3328\n",
      "Epoch 744/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3900 - val_loss: 0.3294\n",
      "Epoch 745/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3889 - val_loss: 0.3284\n",
      "Epoch 746/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3903 - val_loss: 0.3316\n",
      "Epoch 747/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3896 - val_loss: 0.3321\n",
      "Epoch 748/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3886 - val_loss: 0.3301\n",
      "Epoch 749/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3881 - val_loss: 0.3297\n",
      "Epoch 750/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3876 - val_loss: 0.3265\n",
      "Epoch 751/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3882 - val_loss: 0.3313\n",
      "Epoch 752/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3874 - val_loss: 0.3295\n",
      "Epoch 753/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3870 - val_loss: 0.3294\n",
      "Epoch 754/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.3866 - val_loss: 0.3264\n",
      "Epoch 755/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3875 - val_loss: 0.3230\n",
      "Epoch 756/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3867 - val_loss: 0.3303\n",
      "Epoch 757/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3877 - val_loss: 0.3309\n",
      "Epoch 758/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3871 - val_loss: 0.3296\n",
      "Epoch 759/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3854 - val_loss: 0.3266\n",
      "Epoch 760/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3862 - val_loss: 0.3272\n",
      "Epoch 761/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3857 - val_loss: 0.3263\n",
      "Epoch 762/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3871 - val_loss: 0.3251\n",
      "Epoch 763/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3859 - val_loss: 0.3275\n",
      "Epoch 764/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3856 - val_loss: 0.3272\n",
      "Epoch 765/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3864 - val_loss: 0.3276\n",
      "Epoch 766/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3851 - val_loss: 0.3259\n",
      "Epoch 767/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3853 - val_loss: 0.3243\n",
      "Epoch 768/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3844 - val_loss: 0.3234\n",
      "Epoch 769/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3855 - val_loss: 0.3226\n",
      "Epoch 770/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3850 - val_loss: 0.3268\n",
      "Epoch 771/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3835 - val_loss: 0.3253\n",
      "Epoch 772/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3840 - val_loss: 0.3257\n",
      "Epoch 773/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3843 - val_loss: 0.3218\n",
      "Epoch 774/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3827 - val_loss: 0.3237\n",
      "Epoch 775/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3834 - val_loss: 0.3257\n",
      "Epoch 776/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3842 - val_loss: 0.3286\n",
      "Epoch 777/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3833 - val_loss: 0.3241\n",
      "Epoch 778/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3823 - val_loss: 0.3217\n",
      "Epoch 779/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3820 - val_loss: 0.3258\n",
      "Epoch 780/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3829 - val_loss: 0.3235\n",
      "Epoch 781/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3820 - val_loss: 0.3236\n",
      "Epoch 782/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3815 - val_loss: 0.3208\n",
      "Epoch 783/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3837 - val_loss: 0.3202\n",
      "Epoch 784/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3812 - val_loss: 0.3223\n",
      "Epoch 785/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3832 - val_loss: 0.3192\n",
      "Epoch 786/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3823 - val_loss: 0.3223\n",
      "Epoch 787/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3814 - val_loss: 0.3236\n",
      "Epoch 788/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3802 - val_loss: 0.3192\n",
      "Epoch 789/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3806 - val_loss: 0.3206\n",
      "Epoch 790/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3818 - val_loss: 0.3199\n",
      "Epoch 791/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3801 - val_loss: 0.3228\n",
      "Epoch 792/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3806 - val_loss: 0.3203\n",
      "Epoch 793/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3798 - val_loss: 0.3196\n",
      "Epoch 794/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3802 - val_loss: 0.3211\n",
      "Epoch 795/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3791 - val_loss: 0.3194\n",
      "Epoch 796/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3795 - val_loss: 0.3203\n",
      "Epoch 797/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3794 - val_loss: 0.3209\n",
      "Epoch 798/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3793 - val_loss: 0.3228\n",
      "Epoch 799/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3788 - val_loss: 0.3185\n",
      "Epoch 800/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3787 - val_loss: 0.3200\n",
      "Epoch 801/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3780 - val_loss: 0.3210\n",
      "Epoch 802/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3787 - val_loss: 0.3208\n",
      "Epoch 803/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3779 - val_loss: 0.3169\n",
      "Epoch 804/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3770 - val_loss: 0.3209\n",
      "Epoch 805/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3768 - val_loss: 0.3182\n",
      "Epoch 806/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3779 - val_loss: 0.3156\n",
      "Epoch 807/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3782 - val_loss: 0.3166\n",
      "Epoch 808/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3774 - val_loss: 0.3168\n",
      "Epoch 809/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3771 - val_loss: 0.3174\n",
      "Epoch 810/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3770 - val_loss: 0.3185\n",
      "Epoch 811/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3769 - val_loss: 0.3159\n",
      "Epoch 812/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3762 - val_loss: 0.3170\n",
      "Epoch 813/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3760 - val_loss: 0.3197\n",
      "Epoch 814/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3761 - val_loss: 0.3186\n",
      "Epoch 815/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3762 - val_loss: 0.3183\n",
      "Epoch 816/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3756 - val_loss: 0.3173\n",
      "Epoch 817/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3742 - val_loss: 0.3182\n",
      "Epoch 818/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3752 - val_loss: 0.3154\n",
      "Epoch 819/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3759 - val_loss: 0.3173\n",
      "Epoch 820/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3755 - val_loss: 0.3147\n",
      "Epoch 821/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3754 - val_loss: 0.3147\n",
      "Epoch 822/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3758 - val_loss: 0.3155\n",
      "Epoch 823/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3739 - val_loss: 0.3178\n",
      "Epoch 824/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3736 - val_loss: 0.3129\n",
      "Epoch 825/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3741 - val_loss: 0.3160\n",
      "Epoch 826/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3730 - val_loss: 0.3134\n",
      "Epoch 827/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.3745 - val_loss: 0.3111\n",
      "Epoch 828/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3742 - val_loss: 0.3135\n",
      "Epoch 829/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3738 - val_loss: 0.3158\n",
      "Epoch 830/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3726 - val_loss: 0.3162\n",
      "Epoch 831/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3730 - val_loss: 0.3124\n",
      "Epoch 832/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3733 - val_loss: 0.3146\n",
      "Epoch 833/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3736 - val_loss: 0.3160\n",
      "Epoch 834/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3728 - val_loss: 0.3141\n",
      "Epoch 835/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3725 - val_loss: 0.3148\n",
      "Epoch 836/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3728 - val_loss: 0.3116\n",
      "Epoch 837/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3723 - val_loss: 0.3148\n",
      "Epoch 838/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3716 - val_loss: 0.3122\n",
      "Epoch 839/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3716 - val_loss: 0.3114\n",
      "Epoch 840/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3713 - val_loss: 0.3127\n",
      "Epoch 841/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3712 - val_loss: 0.3111\n",
      "Epoch 842/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3707 - val_loss: 0.3166\n",
      "Epoch 843/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3713 - val_loss: 0.3118\n",
      "Epoch 844/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3715 - val_loss: 0.3133\n",
      "Epoch 845/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3709 - val_loss: 0.3137\n",
      "Epoch 846/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3693 - val_loss: 0.3103\n",
      "Epoch 847/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3711 - val_loss: 0.3115\n",
      "Epoch 848/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3708 - val_loss: 0.3114\n",
      "Epoch 849/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3706 - val_loss: 0.3096\n",
      "Epoch 850/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3711 - val_loss: 0.3094\n",
      "Epoch 851/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3706 - val_loss: 0.3127\n",
      "Epoch 852/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3710 - val_loss: 0.3101\n",
      "Epoch 853/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3701 - val_loss: 0.3092\n",
      "Epoch 854/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3696 - val_loss: 0.3102\n",
      "Epoch 855/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3706 - val_loss: 0.3125\n",
      "Epoch 856/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3688 - val_loss: 0.3095\n",
      "Epoch 857/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3690 - val_loss: 0.3099\n",
      "Epoch 858/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3682 - val_loss: 0.3119\n",
      "Epoch 859/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3680 - val_loss: 0.3098\n",
      "Epoch 860/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3675 - val_loss: 0.3118\n",
      "Epoch 861/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3677 - val_loss: 0.3124\n",
      "Epoch 862/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3687 - val_loss: 0.3099\n",
      "Epoch 863/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3683 - val_loss: 0.3095\n",
      "Epoch 864/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3673 - val_loss: 0.3105\n",
      "Epoch 865/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3675 - val_loss: 0.3054\n",
      "Epoch 866/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.3668 - val_loss: 0.3076\n",
      "Epoch 867/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3675 - val_loss: 0.3098\n",
      "Epoch 868/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3668 - val_loss: 0.3084\n",
      "Epoch 869/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3676 - val_loss: 0.3107\n",
      "Epoch 870/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3663 - val_loss: 0.3035\n",
      "Epoch 871/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3648 - val_loss: 0.3088\n",
      "Epoch 872/2000\n",
      "2/2 [==============================] - 1s 571ms/step - loss: 0.3661 - val_loss: 0.3070\n",
      "Epoch 873/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3667 - val_loss: 0.3074\n",
      "Epoch 874/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3670 - val_loss: 0.3024\n",
      "Epoch 875/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3654 - val_loss: 0.3060\n",
      "Epoch 876/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3644 - val_loss: 0.3082\n",
      "Epoch 877/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3665 - val_loss: 0.3071\n",
      "Epoch 878/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3656 - val_loss: 0.3075\n",
      "Epoch 879/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3655 - val_loss: 0.3053\n",
      "Epoch 880/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3654 - val_loss: 0.3071\n",
      "Epoch 881/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3649 - val_loss: 0.3039\n",
      "Epoch 882/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3655 - val_loss: 0.3081\n",
      "Epoch 883/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3635 - val_loss: 0.3086\n",
      "Epoch 884/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3640 - val_loss: 0.3074\n",
      "Epoch 885/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3627 - val_loss: 0.3077\n",
      "Epoch 886/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3641 - val_loss: 0.3096\n",
      "Epoch 887/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3642 - val_loss: 0.3028\n",
      "Epoch 888/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3646 - val_loss: 0.3029\n",
      "Epoch 889/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3643 - val_loss: 0.3062\n",
      "Epoch 890/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3642 - val_loss: 0.3080\n",
      "Epoch 891/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3643 - val_loss: 0.3029\n",
      "Epoch 892/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3630 - val_loss: 0.3042\n",
      "Epoch 893/2000\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.3624 - val_loss: 0.3014\n",
      "Epoch 894/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3631 - val_loss: 0.3078\n",
      "Epoch 895/2000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.3627 - val_loss: 0.3041\n",
      "Epoch 896/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3634 - val_loss: 0.3062\n",
      "Epoch 897/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3633 - val_loss: 0.3043\n",
      "Epoch 898/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3617 - val_loss: 0.3066\n",
      "Epoch 899/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3611 - val_loss: 0.3037\n",
      "Epoch 900/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3631 - val_loss: 0.3046\n",
      "Epoch 901/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3617 - val_loss: 0.3013\n",
      "Epoch 902/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3622 - val_loss: 0.3021\n",
      "Epoch 903/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3602 - val_loss: 0.3069\n",
      "Epoch 904/2000\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.3621 - val_loss: 0.3027\n",
      "Epoch 905/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3603 - val_loss: 0.3028\n",
      "Epoch 906/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3615 - val_loss: 0.3018\n",
      "Epoch 907/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3608 - val_loss: 0.3016\n",
      "Epoch 908/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.3608 - val_loss: 0.3013\n",
      "Epoch 909/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3606 - val_loss: 0.3011\n",
      "Epoch 910/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3606 - val_loss: 0.3025\n",
      "Epoch 911/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3595 - val_loss: 0.3041\n",
      "Epoch 912/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3602 - val_loss: 0.3005\n",
      "Epoch 913/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3607 - val_loss: 0.2988\n",
      "Epoch 914/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3608 - val_loss: 0.3025\n",
      "Epoch 915/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3594 - val_loss: 0.3017\n",
      "Epoch 916/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3599 - val_loss: 0.3015\n",
      "Epoch 917/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3592 - val_loss: 0.3011\n",
      "Epoch 918/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3587 - val_loss: 0.3044\n",
      "Epoch 919/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3584 - val_loss: 0.3013\n",
      "Epoch 920/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3589 - val_loss: 0.3031\n",
      "Epoch 921/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3598 - val_loss: 0.3011\n",
      "Epoch 922/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3580 - val_loss: 0.2986\n",
      "Epoch 923/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3572 - val_loss: 0.3036\n",
      "Epoch 924/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3590 - val_loss: 0.2972\n",
      "Epoch 925/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3586 - val_loss: 0.2985\n",
      "Epoch 926/2000\n",
      "2/2 [==============================] - 1s 489ms/step - loss: 0.3578 - val_loss: 0.3008\n",
      "Epoch 927/2000\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.3585 - val_loss: 0.2999\n",
      "Epoch 928/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3579 - val_loss: 0.2997\n",
      "Epoch 929/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3578 - val_loss: 0.2990\n",
      "Epoch 930/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3581 - val_loss: 0.2989\n",
      "Epoch 931/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3574 - val_loss: 0.3012\n",
      "Epoch 932/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3565 - val_loss: 0.3010\n",
      "Epoch 933/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3562 - val_loss: 0.2988\n",
      "Epoch 934/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3565 - val_loss: 0.2983\n",
      "Epoch 935/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3569 - val_loss: 0.3009\n",
      "Epoch 936/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3560 - val_loss: 0.2967\n",
      "Epoch 937/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3560 - val_loss: 0.3007\n",
      "Epoch 938/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3569 - val_loss: 0.3000\n",
      "Epoch 939/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3570 - val_loss: 0.2990\n",
      "Epoch 940/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3559 - val_loss: 0.2979\n",
      "Epoch 941/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3554 - val_loss: 0.2954\n",
      "Epoch 942/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3564 - val_loss: 0.3012\n",
      "Epoch 943/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3574 - val_loss: 0.2985\n",
      "Epoch 944/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3550 - val_loss: 0.2996\n",
      "Epoch 945/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3556 - val_loss: 0.3000\n",
      "Epoch 946/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3561 - val_loss: 0.2972\n",
      "Epoch 947/2000\n",
      "2/2 [==============================] - 1s 509ms/step - loss: 0.3539 - val_loss: 0.2971\n",
      "Epoch 948/2000\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 0.3548 - val_loss: 0.2995\n",
      "Epoch 949/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3551 - val_loss: 0.2987\n",
      "Epoch 950/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3544 - val_loss: 0.2974\n",
      "Epoch 951/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3546 - val_loss: 0.2951\n",
      "Epoch 952/2000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.3540 - val_loss: 0.2965\n",
      "Epoch 953/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3548 - val_loss: 0.2964\n",
      "Epoch 954/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3545 - val_loss: 0.2987\n",
      "Epoch 955/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3543 - val_loss: 0.2956\n",
      "Epoch 956/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3557 - val_loss: 0.3002\n",
      "Epoch 957/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3537 - val_loss: 0.2950\n",
      "Epoch 958/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3530 - val_loss: 0.2984\n",
      "Epoch 959/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3530 - val_loss: 0.2959\n",
      "Epoch 960/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3527 - val_loss: 0.2970\n",
      "Epoch 961/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3526 - val_loss: 0.2949\n",
      "Epoch 962/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3524 - val_loss: 0.2953\n",
      "Epoch 963/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3531 - val_loss: 0.2931\n",
      "Epoch 964/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3528 - val_loss: 0.2931\n",
      "Epoch 965/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3533 - val_loss: 0.2996\n",
      "Epoch 966/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3534 - val_loss: 0.2924\n",
      "Epoch 967/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3523 - val_loss: 0.2978\n",
      "Epoch 968/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3532 - val_loss: 0.2978\n",
      "Epoch 969/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3521 - val_loss: 0.2943\n",
      "Epoch 970/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3524 - val_loss: 0.2943\n",
      "Epoch 971/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3514 - val_loss: 0.2948\n",
      "Epoch 972/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3518 - val_loss: 0.2969\n",
      "Epoch 973/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3520 - val_loss: 0.2953\n",
      "Epoch 974/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3506 - val_loss: 0.2911\n",
      "Epoch 975/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3515 - val_loss: 0.2957\n",
      "Epoch 976/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3519 - val_loss: 0.2938\n",
      "Epoch 977/2000\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.3518 - val_loss: 0.2935\n",
      "Epoch 978/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3505 - val_loss: 0.2969\n",
      "Epoch 979/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3501 - val_loss: 0.2957\n",
      "Epoch 980/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3507 - val_loss: 0.2971\n",
      "Epoch 981/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3502 - val_loss: 0.2949\n",
      "Epoch 982/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3509 - val_loss: 0.2968\n",
      "Epoch 983/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3509 - val_loss: 0.2930\n",
      "Epoch 984/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3502 - val_loss: 0.2929\n",
      "Epoch 985/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3507 - val_loss: 0.2946\n",
      "Epoch 986/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3506 - val_loss: 0.2946\n",
      "Epoch 987/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3491 - val_loss: 0.2972\n",
      "Epoch 988/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3500 - val_loss: 0.2923\n",
      "Epoch 989/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3489 - val_loss: 0.2896\n",
      "Epoch 990/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3504 - val_loss: 0.2921\n",
      "Epoch 991/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3505 - val_loss: 0.2932\n",
      "Epoch 992/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3494 - val_loss: 0.2885\n",
      "Epoch 993/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3492 - val_loss: 0.2949\n",
      "Epoch 994/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3489 - val_loss: 0.2929\n",
      "Epoch 995/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3490 - val_loss: 0.2924\n",
      "Epoch 996/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3480 - val_loss: 0.2906\n",
      "Epoch 997/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3494 - val_loss: 0.2937\n",
      "Epoch 998/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3489 - val_loss: 0.2945\n",
      "Epoch 999/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3495 - val_loss: 0.2934\n",
      "Epoch 1000/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3481 - val_loss: 0.2922\n",
      "Epoch 1001/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3484 - val_loss: 0.2947\n",
      "Epoch 1002/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3483 - val_loss: 0.2948\n",
      "Epoch 1003/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3476 - val_loss: 0.2917\n",
      "Epoch 1004/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3487 - val_loss: 0.2880\n",
      "Epoch 1005/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3471 - val_loss: 0.2935\n",
      "Epoch 1006/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3479 - val_loss: 0.2911\n",
      "Epoch 1007/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3480 - val_loss: 0.2945\n",
      "Epoch 1008/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3476 - val_loss: 0.2933\n",
      "Epoch 1009/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3471 - val_loss: 0.2921\n",
      "Epoch 1010/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3466 - val_loss: 0.2891\n",
      "Epoch 1011/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3472 - val_loss: 0.2892\n",
      "Epoch 1012/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3462 - val_loss: 0.2924\n",
      "Epoch 1013/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3459 - val_loss: 0.2956\n",
      "Epoch 1014/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3483 - val_loss: 0.2896\n",
      "Epoch 1015/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3472 - val_loss: 0.2895\n",
      "Epoch 1016/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3466 - val_loss: 0.2909\n",
      "Epoch 1017/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3479 - val_loss: 0.2945\n",
      "Epoch 1018/2000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.3476 - val_loss: 0.2915\n",
      "Epoch 1019/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3465 - val_loss: 0.2904\n",
      "Epoch 1020/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3470 - val_loss: 0.2899\n",
      "Epoch 1021/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3464 - val_loss: 0.2907\n",
      "Epoch 1022/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3468 - val_loss: 0.2922\n",
      "Epoch 1023/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3451 - val_loss: 0.2902\n",
      "Epoch 1024/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3455 - val_loss: 0.2927\n",
      "Epoch 1025/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3466 - val_loss: 0.2908\n",
      "Epoch 1026/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3454 - val_loss: 0.2897\n",
      "Epoch 1027/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3460 - val_loss: 0.2915\n",
      "Epoch 1028/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3450 - val_loss: 0.2920\n",
      "Epoch 1029/2000\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 0.3448 - val_loss: 0.2906\n",
      "Epoch 1030/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3430 - val_loss: 0.2861\n",
      "Epoch 1031/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3446 - val_loss: 0.2889\n",
      "Epoch 1032/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3442 - val_loss: 0.2898\n",
      "Epoch 1033/2000\n",
      "2/2 [==============================] - 1s 471ms/step - loss: 0.3443 - val_loss: 0.2890\n",
      "Epoch 1034/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3445 - val_loss: 0.2903\n",
      "Epoch 1035/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3449 - val_loss: 0.2911\n",
      "Epoch 1036/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3441 - val_loss: 0.2917\n",
      "Epoch 1037/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3451 - val_loss: 0.2877\n",
      "Epoch 1038/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3453 - val_loss: 0.2871\n",
      "Epoch 1039/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3442 - val_loss: 0.2894\n",
      "Epoch 1040/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3441 - val_loss: 0.2902\n",
      "Epoch 1041/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3430 - val_loss: 0.2896\n",
      "Epoch 1042/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3436 - val_loss: 0.2928\n",
      "Epoch 1043/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3438 - val_loss: 0.2878\n",
      "Epoch 1044/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3428 - val_loss: 0.2896\n",
      "Epoch 1045/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3443 - val_loss: 0.2885\n",
      "Epoch 1046/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3430 - val_loss: 0.2888\n",
      "Epoch 1047/2000\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.3435 - val_loss: 0.2853\n",
      "Epoch 1048/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3429 - val_loss: 0.2881\n",
      "Epoch 1049/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3424 - val_loss: 0.2908\n",
      "Epoch 1050/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3417 - val_loss: 0.2878\n",
      "Epoch 1051/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3422 - val_loss: 0.2898\n",
      "Epoch 1052/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3416 - val_loss: 0.2905\n",
      "Epoch 1053/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3409 - val_loss: 0.2886\n",
      "Epoch 1054/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3421 - val_loss: 0.2868\n",
      "Epoch 1055/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3430 - val_loss: 0.2876\n",
      "Epoch 1056/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3425 - val_loss: 0.2906\n",
      "Epoch 1057/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3424 - val_loss: 0.2891\n",
      "Epoch 1058/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3418 - val_loss: 0.2876\n",
      "Epoch 1059/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3411 - val_loss: 0.2879\n",
      "Epoch 1060/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3432 - val_loss: 0.2900\n",
      "Epoch 1061/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3424 - val_loss: 0.2885\n",
      "Epoch 1062/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3414 - val_loss: 0.2906\n",
      "Epoch 1063/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3418 - val_loss: 0.2870\n",
      "Epoch 1064/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3419 - val_loss: 0.2859\n",
      "Epoch 1065/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3425 - val_loss: 0.2897\n",
      "Epoch 1066/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3415 - val_loss: 0.2854\n",
      "Epoch 1067/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3410 - val_loss: 0.2847\n",
      "Epoch 1068/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3399 - val_loss: 0.2894\n",
      "Epoch 1069/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3401 - val_loss: 0.2873\n",
      "Epoch 1070/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3411 - val_loss: 0.2876\n",
      "Epoch 1071/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3412 - val_loss: 0.2870\n",
      "Epoch 1072/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3407 - val_loss: 0.2846\n",
      "Epoch 1073/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3403 - val_loss: 0.2909\n",
      "Epoch 1074/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3391 - val_loss: 0.2838\n",
      "Epoch 1075/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3404 - val_loss: 0.2857\n",
      "Epoch 1076/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3402 - val_loss: 0.2870\n",
      "Epoch 1077/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3402 - val_loss: 0.2875\n",
      "Epoch 1078/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3407 - val_loss: 0.2862\n",
      "Epoch 1079/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3380 - val_loss: 0.2859\n",
      "Epoch 1080/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3399 - val_loss: 0.2851\n",
      "Epoch 1081/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3406 - val_loss: 0.2854\n",
      "Epoch 1082/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3394 - val_loss: 0.2896\n",
      "Epoch 1083/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3382 - val_loss: 0.2843\n",
      "Epoch 1084/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3390 - val_loss: 0.2850\n",
      "Epoch 1085/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3391 - val_loss: 0.2841\n",
      "Epoch 1086/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3383 - val_loss: 0.2839\n",
      "Epoch 1087/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3375 - val_loss: 0.2889\n",
      "Epoch 1088/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3390 - val_loss: 0.2866\n",
      "Epoch 1089/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3374 - val_loss: 0.2864\n",
      "Epoch 1090/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3384 - val_loss: 0.2828\n",
      "Epoch 1091/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3381 - val_loss: 0.2825\n",
      "Epoch 1092/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3393 - val_loss: 0.2848\n",
      "Epoch 1093/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3384 - val_loss: 0.2828\n",
      "Epoch 1094/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3387 - val_loss: 0.2857\n",
      "Epoch 1095/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3380 - val_loss: 0.2851\n",
      "Epoch 1096/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3378 - val_loss: 0.2846\n",
      "Epoch 1097/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3385 - val_loss: 0.2841\n",
      "Epoch 1098/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3377 - val_loss: 0.2848\n",
      "Epoch 1099/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3363 - val_loss: 0.2843\n",
      "Epoch 1100/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3387 - val_loss: 0.2833\n",
      "Epoch 1101/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3393 - val_loss: 0.2842\n",
      "Epoch 1102/2000\n",
      "2/2 [==============================] - 1s 395ms/step - loss: 0.3372 - val_loss: 0.2821\n",
      "Epoch 1103/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3375 - val_loss: 0.2827\n",
      "Epoch 1104/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3372 - val_loss: 0.2819\n",
      "Epoch 1105/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3375 - val_loss: 0.2827\n",
      "Epoch 1106/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3374 - val_loss: 0.2836\n",
      "Epoch 1107/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3375 - val_loss: 0.2841\n",
      "Epoch 1108/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3373 - val_loss: 0.2857\n",
      "Epoch 1109/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3374 - val_loss: 0.2802\n",
      "Epoch 1110/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3367 - val_loss: 0.2807\n",
      "Epoch 1111/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3362 - val_loss: 0.2859\n",
      "Epoch 1112/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3364 - val_loss: 0.2839\n",
      "Epoch 1113/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3372 - val_loss: 0.2823\n",
      "Epoch 1114/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3349 - val_loss: 0.2837\n",
      "Epoch 1115/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3367 - val_loss: 0.2811\n",
      "Epoch 1116/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3371 - val_loss: 0.2857\n",
      "Epoch 1117/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3358 - val_loss: 0.2833\n",
      "Epoch 1118/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3357 - val_loss: 0.2818\n",
      "Epoch 1119/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3351 - val_loss: 0.2814\n",
      "Epoch 1120/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3358 - val_loss: 0.2808\n",
      "Epoch 1121/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3351 - val_loss: 0.2795\n",
      "Epoch 1122/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3359 - val_loss: 0.2822\n",
      "Epoch 1123/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3360 - val_loss: 0.2816\n",
      "Epoch 1124/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3358 - val_loss: 0.2854\n",
      "Epoch 1125/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3348 - val_loss: 0.2781\n",
      "Epoch 1126/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3361 - val_loss: 0.2810\n",
      "Epoch 1127/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3354 - val_loss: 0.2826\n",
      "Epoch 1128/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3351 - val_loss: 0.2828\n",
      "Epoch 1129/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3345 - val_loss: 0.2802\n",
      "Epoch 1130/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3356 - val_loss: 0.2816\n",
      "Epoch 1131/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3350 - val_loss: 0.2813\n",
      "Epoch 1132/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3350 - val_loss: 0.2851\n",
      "Epoch 1133/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3341 - val_loss: 0.2828\n",
      "Epoch 1134/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3347 - val_loss: 0.2806\n",
      "Epoch 1135/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3350 - val_loss: 0.2773\n",
      "Epoch 1136/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3345 - val_loss: 0.2830\n",
      "Epoch 1137/2000\n",
      "2/2 [==============================] - 1s 440ms/step - loss: 0.3341 - val_loss: 0.2826\n",
      "Epoch 1138/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3336 - val_loss: 0.2819\n",
      "Epoch 1139/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3331 - val_loss: 0.2845\n",
      "Epoch 1140/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3337 - val_loss: 0.2802\n",
      "Epoch 1141/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3333 - val_loss: 0.2802\n",
      "Epoch 1142/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3329 - val_loss: 0.2832\n",
      "Epoch 1143/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3324 - val_loss: 0.2806\n",
      "Epoch 1144/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3334 - val_loss: 0.2809\n",
      "Epoch 1145/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3345 - val_loss: 0.2812\n",
      "Epoch 1146/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3336 - val_loss: 0.2793\n",
      "Epoch 1147/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3332 - val_loss: 0.2813\n",
      "Epoch 1148/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3326 - val_loss: 0.2768\n",
      "Epoch 1149/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3340 - val_loss: 0.2830\n",
      "Epoch 1150/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3329 - val_loss: 0.2800\n",
      "Epoch 1151/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3336 - val_loss: 0.2820\n",
      "Epoch 1152/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3322 - val_loss: 0.2785\n",
      "Epoch 1153/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3324 - val_loss: 0.2798\n",
      "Epoch 1154/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3332 - val_loss: 0.2840\n",
      "Epoch 1155/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3330 - val_loss: 0.2802\n",
      "Epoch 1156/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3328 - val_loss: 0.2835\n",
      "Epoch 1157/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3330 - val_loss: 0.2815\n",
      "Epoch 1158/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3326 - val_loss: 0.2824\n",
      "Epoch 1159/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3316 - val_loss: 0.2813\n",
      "Epoch 1160/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3321 - val_loss: 0.2820\n",
      "Epoch 1161/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3317 - val_loss: 0.2779\n",
      "Epoch 1162/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3318 - val_loss: 0.2794\n",
      "Epoch 1163/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3319 - val_loss: 0.2803\n",
      "Epoch 1164/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3324 - val_loss: 0.2800\n",
      "Epoch 1165/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3312 - val_loss: 0.2805\n",
      "Epoch 1166/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3319 - val_loss: 0.2795\n",
      "Epoch 1167/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3313 - val_loss: 0.2769\n",
      "Epoch 1168/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3310 - val_loss: 0.2805\n",
      "Epoch 1169/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3311 - val_loss: 0.2804\n",
      "Epoch 1170/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3302 - val_loss: 0.2792\n",
      "Epoch 1171/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3315 - val_loss: 0.2803\n",
      "Epoch 1172/2000\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.3321 - val_loss: 0.2801\n",
      "Epoch 1173/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3317 - val_loss: 0.2761\n",
      "Epoch 1174/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3316 - val_loss: 0.2771\n",
      "Epoch 1175/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3312 - val_loss: 0.2788\n",
      "Epoch 1176/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3314 - val_loss: 0.2789\n",
      "Epoch 1177/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3312 - val_loss: 0.2821\n",
      "Epoch 1178/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3300 - val_loss: 0.2793\n",
      "Epoch 1179/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3314 - val_loss: 0.2811\n",
      "Epoch 1180/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3306 - val_loss: 0.2794\n",
      "Epoch 1181/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3301 - val_loss: 0.2813\n",
      "Epoch 1182/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3316 - val_loss: 0.2792\n",
      "Epoch 1183/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3313 - val_loss: 0.2786\n",
      "Epoch 1184/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3303 - val_loss: 0.2791\n",
      "Epoch 1185/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3314 - val_loss: 0.2784\n",
      "Epoch 1186/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3314 - val_loss: 0.2794\n",
      "Epoch 1187/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3293 - val_loss: 0.2803\n",
      "Epoch 1188/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3301 - val_loss: 0.2780\n",
      "Epoch 1189/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3291 - val_loss: 0.2785\n",
      "Epoch 1190/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3304 - val_loss: 0.2784\n",
      "Epoch 1191/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3301 - val_loss: 0.2775\n",
      "Epoch 1192/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3292 - val_loss: 0.2790\n",
      "Epoch 1193/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3285 - val_loss: 0.2742\n",
      "Epoch 1194/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3299 - val_loss: 0.2772\n",
      "Epoch 1195/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3301 - val_loss: 0.2783\n",
      "Epoch 1196/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3277 - val_loss: 0.2783\n",
      "Epoch 1197/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3297 - val_loss: 0.2785\n",
      "Epoch 1198/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3291 - val_loss: 0.2788\n",
      "Epoch 1199/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3295 - val_loss: 0.2789\n",
      "Epoch 1200/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3286 - val_loss: 0.2742\n",
      "Epoch 1201/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3284 - val_loss: 0.2750\n",
      "Epoch 1202/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3282 - val_loss: 0.2812\n",
      "Epoch 1203/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3292 - val_loss: 0.2774\n",
      "Epoch 1204/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3293 - val_loss: 0.2764\n",
      "Epoch 1205/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3289 - val_loss: 0.2773\n",
      "Epoch 1206/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3294 - val_loss: 0.2793\n",
      "Epoch 1207/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3296 - val_loss: 0.2778\n",
      "Epoch 1208/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3286 - val_loss: 0.2798\n",
      "Epoch 1209/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3279 - val_loss: 0.2793\n",
      "Epoch 1210/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3287 - val_loss: 0.2801\n",
      "Epoch 1211/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3277 - val_loss: 0.2776\n",
      "Epoch 1212/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3284 - val_loss: 0.2764\n",
      "Epoch 1213/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3294 - val_loss: 0.2771\n",
      "Epoch 1214/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3286 - val_loss: 0.2801\n",
      "Epoch 1215/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3277 - val_loss: 0.2773\n",
      "Epoch 1216/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3275 - val_loss: 0.2771\n",
      "Epoch 1217/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3264 - val_loss: 0.2778\n",
      "Epoch 1218/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3288 - val_loss: 0.2794\n",
      "Epoch 1219/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3279 - val_loss: 0.2780\n",
      "Epoch 1220/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3273 - val_loss: 0.2789\n",
      "Epoch 1221/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3282 - val_loss: 0.2772\n",
      "Epoch 1222/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3275 - val_loss: 0.2760\n",
      "Epoch 1223/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3269 - val_loss: 0.2790\n",
      "Epoch 1224/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3268 - val_loss: 0.2761\n",
      "Epoch 1225/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3275 - val_loss: 0.2756\n",
      "Epoch 1226/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3280 - val_loss: 0.2791\n",
      "Epoch 1227/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3268 - val_loss: 0.2762\n",
      "Epoch 1228/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3261 - val_loss: 0.2779\n",
      "Epoch 1229/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3267 - val_loss: 0.2785\n",
      "Epoch 1230/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3255 - val_loss: 0.2788\n",
      "Epoch 1231/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3256 - val_loss: 0.2770\n",
      "Epoch 1232/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3271 - val_loss: 0.2725\n",
      "Epoch 1233/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3253 - val_loss: 0.2769\n",
      "Epoch 1234/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3271 - val_loss: 0.2762\n",
      "Epoch 1235/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3274 - val_loss: 0.2754\n",
      "Epoch 1236/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3254 - val_loss: 0.2752\n",
      "Epoch 1237/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3254 - val_loss: 0.2785\n",
      "Epoch 1238/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3252 - val_loss: 0.2791\n",
      "Epoch 1239/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3255 - val_loss: 0.2761\n",
      "Epoch 1240/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3263 - val_loss: 0.2822\n",
      "Epoch 1241/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3261 - val_loss: 0.2736\n",
      "Epoch 1242/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3259 - val_loss: 0.2776\n",
      "Epoch 1243/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3253 - val_loss: 0.2753\n",
      "Epoch 1244/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3250 - val_loss: 0.2767\n",
      "Epoch 1245/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3252 - val_loss: 0.2749\n",
      "Epoch 1246/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3255 - val_loss: 0.2778\n",
      "Epoch 1247/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3256 - val_loss: 0.2759\n",
      "Epoch 1248/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3260 - val_loss: 0.2779\n",
      "Epoch 1249/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3257 - val_loss: 0.2787\n",
      "Epoch 1250/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3257 - val_loss: 0.2726\n",
      "Epoch 1251/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3260 - val_loss: 0.2773\n",
      "Epoch 1252/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3257 - val_loss: 0.2756\n",
      "Epoch 1253/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3242 - val_loss: 0.2747\n",
      "Epoch 1254/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3255 - val_loss: 0.2742\n",
      "Epoch 1255/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3254 - val_loss: 0.2784\n",
      "Epoch 1256/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3247 - val_loss: 0.2745\n",
      "Epoch 1257/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3246 - val_loss: 0.2769\n",
      "Epoch 1258/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3261 - val_loss: 0.2780\n",
      "Epoch 1259/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3248 - val_loss: 0.2777\n",
      "Epoch 1260/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3252 - val_loss: 0.2747\n",
      "Epoch 1261/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3251 - val_loss: 0.2790\n",
      "Epoch 1262/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3235 - val_loss: 0.2759\n",
      "Epoch 1263/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3241 - val_loss: 0.2760\n",
      "Epoch 1264/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3258 - val_loss: 0.2750\n",
      "Epoch 1265/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3241 - val_loss: 0.2768\n",
      "Epoch 1266/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3244 - val_loss: 0.2764\n",
      "Epoch 1267/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3242 - val_loss: 0.2747\n",
      "Epoch 1268/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3231 - val_loss: 0.2766\n",
      "Epoch 1269/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3246 - val_loss: 0.2762\n",
      "Epoch 1270/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3239 - val_loss: 0.2710\n",
      "Epoch 1271/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3255 - val_loss: 0.2773\n",
      "Epoch 1272/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3241 - val_loss: 0.2755\n",
      "Epoch 1273/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3244 - val_loss: 0.2730\n",
      "Epoch 1274/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3237 - val_loss: 0.2736\n",
      "Epoch 1275/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3240 - val_loss: 0.2719\n",
      "Epoch 1276/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3243 - val_loss: 0.2755\n",
      "Epoch 1277/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3239 - val_loss: 0.2761\n",
      "Epoch 1278/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3238 - val_loss: 0.2764\n",
      "Epoch 1279/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3237 - val_loss: 0.2747\n",
      "Epoch 1280/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3235 - val_loss: 0.2762\n",
      "Epoch 1281/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3231 - val_loss: 0.2763\n",
      "Epoch 1282/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3225 - val_loss: 0.2763\n",
      "Epoch 1283/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3238 - val_loss: 0.2716\n",
      "Epoch 1284/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3234 - val_loss: 0.2702\n",
      "Epoch 1285/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3235 - val_loss: 0.2775\n",
      "Epoch 1286/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3241 - val_loss: 0.2728\n",
      "Epoch 1287/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3236 - val_loss: 0.2737\n",
      "Epoch 1288/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3235 - val_loss: 0.2725\n",
      "Epoch 1289/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3228 - val_loss: 0.2745\n",
      "Epoch 1290/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3231 - val_loss: 0.2728\n",
      "Epoch 1291/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3230 - val_loss: 0.2754\n",
      "Epoch 1292/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3226 - val_loss: 0.2757\n",
      "Epoch 1293/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3245 - val_loss: 0.2758\n",
      "Epoch 1294/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3215 - val_loss: 0.2726\n",
      "Epoch 1295/2000\n",
      "2/2 [==============================] - 1s 418ms/step - loss: 0.3227 - val_loss: 0.2759\n",
      "Epoch 1296/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3233 - val_loss: 0.2742\n",
      "Epoch 1297/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3227 - val_loss: 0.2739\n",
      "Epoch 1298/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3220 - val_loss: 0.2735\n",
      "Epoch 1299/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3231 - val_loss: 0.2759\n",
      "Epoch 1300/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3226 - val_loss: 0.2730\n",
      "Epoch 1301/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3226 - val_loss: 0.2738\n",
      "Epoch 1302/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3222 - val_loss: 0.2766\n",
      "Epoch 1303/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3222 - val_loss: 0.2748\n",
      "Epoch 1304/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3222 - val_loss: 0.2753\n",
      "Epoch 1305/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3218 - val_loss: 0.2729\n",
      "Epoch 1306/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3221 - val_loss: 0.2726\n",
      "Epoch 1307/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3217 - val_loss: 0.2717\n",
      "Epoch 1308/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3213 - val_loss: 0.2727\n",
      "Epoch 1309/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3209 - val_loss: 0.2717\n",
      "Epoch 1310/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3226 - val_loss: 0.2739\n",
      "Epoch 1311/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3210 - val_loss: 0.2743\n",
      "Epoch 1312/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3223 - val_loss: 0.2762\n",
      "Epoch 1313/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3210 - val_loss: 0.2733\n",
      "Epoch 1314/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3211 - val_loss: 0.2766\n",
      "Epoch 1315/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3207 - val_loss: 0.2731\n",
      "Epoch 1316/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3201 - val_loss: 0.2738\n",
      "Epoch 1317/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3211 - val_loss: 0.2722\n",
      "Epoch 1318/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3222 - val_loss: 0.2711\n",
      "Epoch 1319/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3221 - val_loss: 0.2729\n",
      "Epoch 1320/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3213 - val_loss: 0.2710\n",
      "Epoch 1321/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3211 - val_loss: 0.2702\n",
      "Epoch 1322/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3203 - val_loss: 0.2729\n",
      "Epoch 1323/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3190 - val_loss: 0.2723\n",
      "Epoch 1324/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3214 - val_loss: 0.2695\n",
      "Epoch 1325/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3199 - val_loss: 0.2732\n",
      "Epoch 1326/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3207 - val_loss: 0.2732\n",
      "Epoch 1327/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3204 - val_loss: 0.2707\n",
      "Epoch 1328/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3201 - val_loss: 0.2723\n",
      "Epoch 1329/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3221 - val_loss: 0.2737\n",
      "Epoch 1330/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3200 - val_loss: 0.2714\n",
      "Epoch 1331/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3211 - val_loss: 0.2716\n",
      "Epoch 1332/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3200 - val_loss: 0.2736\n",
      "Epoch 1333/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3207 - val_loss: 0.2736\n",
      "Epoch 1334/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3193 - val_loss: 0.2741\n",
      "Epoch 1335/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3207 - val_loss: 0.2763\n",
      "Epoch 1336/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3218 - val_loss: 0.2712\n",
      "Epoch 1337/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3204 - val_loss: 0.2705\n",
      "Epoch 1338/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3196 - val_loss: 0.2715\n",
      "Epoch 1339/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3201 - val_loss: 0.2735\n",
      "Epoch 1340/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3202 - val_loss: 0.2702\n",
      "Epoch 1341/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3202 - val_loss: 0.2730\n",
      "Epoch 1342/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3195 - val_loss: 0.2732\n",
      "Epoch 1343/2000\n",
      "2/2 [==============================] - 1s 436ms/step - loss: 0.3193 - val_loss: 0.2670\n",
      "Epoch 1344/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3216 - val_loss: 0.2737\n",
      "Epoch 1345/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3201 - val_loss: 0.2742\n",
      "Epoch 1346/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3203 - val_loss: 0.2721\n",
      "Epoch 1347/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3188 - val_loss: 0.2714\n",
      "Epoch 1348/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3185 - val_loss: 0.2714\n",
      "Epoch 1349/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3188 - val_loss: 0.2727\n",
      "Epoch 1350/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3194 - val_loss: 0.2741\n",
      "Epoch 1351/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3184 - val_loss: 0.2716\n",
      "Epoch 1352/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3190 - val_loss: 0.2738\n",
      "Epoch 1353/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3188 - val_loss: 0.2706\n",
      "Epoch 1354/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3187 - val_loss: 0.2725\n",
      "Epoch 1355/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3192 - val_loss: 0.2699\n",
      "Epoch 1356/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3190 - val_loss: 0.2689\n",
      "Epoch 1357/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3181 - val_loss: 0.2714\n",
      "Epoch 1358/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3191 - val_loss: 0.2704\n",
      "Epoch 1359/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3188 - val_loss: 0.2719\n",
      "Epoch 1360/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3175 - val_loss: 0.2725\n",
      "Epoch 1361/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3199 - val_loss: 0.2712\n",
      "Epoch 1362/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3176 - val_loss: 0.2717\n",
      "Epoch 1363/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3184 - val_loss: 0.2717\n",
      "Epoch 1364/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3181 - val_loss: 0.2740\n",
      "Epoch 1365/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3180 - val_loss: 0.2717\n",
      "Epoch 1366/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3179 - val_loss: 0.2691\n",
      "Epoch 1367/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3188 - val_loss: 0.2700\n",
      "Epoch 1368/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3183 - val_loss: 0.2716\n",
      "Epoch 1369/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3183 - val_loss: 0.2697\n",
      "Epoch 1370/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3160 - val_loss: 0.2698\n",
      "Epoch 1371/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3182 - val_loss: 0.2704\n",
      "Epoch 1372/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3180 - val_loss: 0.2732\n",
      "Epoch 1373/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3180 - val_loss: 0.2720\n",
      "Epoch 1374/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3191 - val_loss: 0.2695\n",
      "Epoch 1375/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3193 - val_loss: 0.2702\n",
      "Epoch 1376/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3163 - val_loss: 0.2714\n",
      "Epoch 1377/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3185 - val_loss: 0.2743\n",
      "Epoch 1378/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3174 - val_loss: 0.2716\n",
      "Epoch 1379/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3182 - val_loss: 0.2702\n",
      "Epoch 1380/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3175 - val_loss: 0.2677\n",
      "Epoch 1381/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3173 - val_loss: 0.2731\n",
      "Epoch 1382/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3163 - val_loss: 0.2717\n",
      "Epoch 1383/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3179 - val_loss: 0.2743\n",
      "Epoch 1384/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3168 - val_loss: 0.2738\n",
      "Epoch 1385/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3165 - val_loss: 0.2710\n",
      "Epoch 1386/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3179 - val_loss: 0.2712\n",
      "Epoch 1387/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3176 - val_loss: 0.2741\n",
      "Epoch 1388/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3178 - val_loss: 0.2696\n",
      "Epoch 1389/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3184 - val_loss: 0.2707\n",
      "Epoch 1390/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3177 - val_loss: 0.2703\n",
      "Epoch 1391/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3165 - val_loss: 0.2737\n",
      "Epoch 1392/2000\n",
      "2/2 [==============================] - 1s 444ms/step - loss: 0.3161 - val_loss: 0.2703\n",
      "Epoch 1393/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3168 - val_loss: 0.2699\n",
      "Epoch 1394/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3160 - val_loss: 0.2695\n",
      "Epoch 1395/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3153 - val_loss: 0.2701\n",
      "Epoch 1396/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3162 - val_loss: 0.2726\n",
      "Epoch 1397/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3165 - val_loss: 0.2734\n",
      "Epoch 1398/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3167 - val_loss: 0.2687\n",
      "Epoch 1399/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3170 - val_loss: 0.2722\n",
      "Epoch 1400/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3165 - val_loss: 0.2731\n",
      "Epoch 1401/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3168 - val_loss: 0.2676\n",
      "Epoch 1402/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3161 - val_loss: 0.2722\n",
      "Epoch 1403/2000\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.3171 - val_loss: 0.2683\n",
      "Epoch 1404/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3161 - val_loss: 0.2717\n",
      "Epoch 1405/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3165 - val_loss: 0.2686\n",
      "Epoch 1406/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3155 - val_loss: 0.2702\n",
      "Epoch 1407/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3159 - val_loss: 0.2711\n",
      "Epoch 1408/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3174 - val_loss: 0.2733\n",
      "Epoch 1409/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3159 - val_loss: 0.2707\n",
      "Epoch 1410/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3170 - val_loss: 0.2709\n",
      "Epoch 1411/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3166 - val_loss: 0.2682\n",
      "Epoch 1412/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3152 - val_loss: 0.2678\n",
      "Epoch 1413/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3161 - val_loss: 0.2696\n",
      "Epoch 1414/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3158 - val_loss: 0.2697\n",
      "Epoch 1415/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3164 - val_loss: 0.2661\n",
      "Epoch 1416/2000\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.3178 - val_loss: 0.2734\n",
      "Epoch 1417/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3162 - val_loss: 0.2686\n",
      "Epoch 1418/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3162 - val_loss: 0.2697\n",
      "Epoch 1419/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3162 - val_loss: 0.2683\n",
      "Epoch 1420/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3169 - val_loss: 0.2713\n",
      "Epoch 1421/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3152 - val_loss: 0.2692\n",
      "Epoch 1422/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3138 - val_loss: 0.2666\n",
      "Epoch 1423/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3157 - val_loss: 0.2669\n",
      "Epoch 1424/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3152 - val_loss: 0.2673\n",
      "Epoch 1425/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3158 - val_loss: 0.2658\n",
      "Epoch 1426/2000\n",
      "2/2 [==============================] - 1s 428ms/step - loss: 0.3159 - val_loss: 0.2702\n",
      "Epoch 1427/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3151 - val_loss: 0.2755\n",
      "Epoch 1428/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3155 - val_loss: 0.2708\n",
      "Epoch 1429/2000\n",
      "2/2 [==============================] - 1s 573ms/step - loss: 0.3147 - val_loss: 0.2688\n",
      "Epoch 1430/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3158 - val_loss: 0.2717\n",
      "Epoch 1431/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3162 - val_loss: 0.2688\n",
      "Epoch 1432/2000\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.3158 - val_loss: 0.2668\n",
      "Epoch 1433/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3155 - val_loss: 0.2710\n",
      "Epoch 1434/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3138 - val_loss: 0.2698\n",
      "Epoch 1435/2000\n",
      "2/2 [==============================] - 1s 494ms/step - loss: 0.3144 - val_loss: 0.2707\n",
      "Epoch 1436/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3132 - val_loss: 0.2668\n",
      "Epoch 1437/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3147 - val_loss: 0.2701\n",
      "Epoch 1438/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3152 - val_loss: 0.2687\n",
      "Epoch 1439/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3146 - val_loss: 0.2702\n",
      "Epoch 1440/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3151 - val_loss: 0.2660\n",
      "Epoch 1441/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3144 - val_loss: 0.2700\n",
      "Epoch 1442/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3140 - val_loss: 0.2688\n",
      "Epoch 1443/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3143 - val_loss: 0.2672\n",
      "Epoch 1444/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3146 - val_loss: 0.2715\n",
      "Epoch 1445/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3143 - val_loss: 0.2693\n",
      "Epoch 1446/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3147 - val_loss: 0.2683\n",
      "Epoch 1447/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3143 - val_loss: 0.2709\n",
      "Epoch 1448/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3130 - val_loss: 0.2683\n",
      "Epoch 1449/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3146 - val_loss: 0.2707\n",
      "Epoch 1450/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3141 - val_loss: 0.2689\n",
      "Epoch 1451/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3132 - val_loss: 0.2681\n",
      "Epoch 1452/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3147 - val_loss: 0.2678\n",
      "Epoch 1453/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3143 - val_loss: 0.2661\n",
      "Epoch 1454/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3142 - val_loss: 0.2686\n",
      "Epoch 1455/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3143 - val_loss: 0.2696\n",
      "Epoch 1456/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3142 - val_loss: 0.2677\n",
      "Epoch 1457/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3139 - val_loss: 0.2662\n",
      "Epoch 1458/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3129 - val_loss: 0.2714\n",
      "Epoch 1459/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3148 - val_loss: 0.2674\n",
      "Epoch 1460/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3137 - val_loss: 0.2676\n",
      "Epoch 1461/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3126 - val_loss: 0.2694\n",
      "Epoch 1462/2000\n",
      "2/2 [==============================] - 1s 450ms/step - loss: 0.3140 - val_loss: 0.2691\n",
      "Epoch 1463/2000\n",
      "2/2 [==============================] - 1s 426ms/step - loss: 0.3134 - val_loss: 0.2652\n",
      "Epoch 1464/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3135 - val_loss: 0.2668\n",
      "Epoch 1465/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3126 - val_loss: 0.2664\n",
      "Epoch 1466/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3124 - val_loss: 0.2688\n",
      "Epoch 1467/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3122 - val_loss: 0.2662\n",
      "Epoch 1468/2000\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.3136 - val_loss: 0.2661\n",
      "Epoch 1469/2000\n",
      "2/2 [==============================] - 1s 490ms/step - loss: 0.3123 - val_loss: 0.2670\n",
      "Epoch 1470/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3136 - val_loss: 0.2714\n",
      "Epoch 1471/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3120 - val_loss: 0.2683\n",
      "Epoch 1472/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3124 - val_loss: 0.2683\n",
      "Epoch 1473/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3134 - val_loss: 0.2701\n",
      "Epoch 1474/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3133 - val_loss: 0.2667\n",
      "Epoch 1475/2000\n",
      "2/2 [==============================] - 1s 442ms/step - loss: 0.3120 - val_loss: 0.2689\n",
      "Epoch 1476/2000\n",
      "2/2 [==============================] - 1s 423ms/step - loss: 0.3123 - val_loss: 0.2676\n",
      "Epoch 1477/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3117 - val_loss: 0.2704\n",
      "Epoch 1478/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3121 - val_loss: 0.2670\n",
      "Epoch 1479/2000\n",
      "2/2 [==============================] - 1s 446ms/step - loss: 0.3118 - val_loss: 0.2683\n",
      "Epoch 1480/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3118 - val_loss: 0.2694\n",
      "Epoch 1481/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3120 - val_loss: 0.2662\n",
      "Epoch 1482/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3109 - val_loss: 0.2702\n",
      "Epoch 1483/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3133 - val_loss: 0.2674\n",
      "Epoch 1484/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3119 - val_loss: 0.2676\n",
      "Epoch 1485/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3120 - val_loss: 0.2658\n",
      "Epoch 1486/2000\n",
      "2/2 [==============================] - 1s 438ms/step - loss: 0.3127 - val_loss: 0.2662\n",
      "Epoch 1487/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3111 - val_loss: 0.2688\n",
      "Epoch 1488/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3126 - val_loss: 0.2680\n",
      "Epoch 1489/2000\n",
      "2/2 [==============================] - 1s 447ms/step - loss: 0.3125 - val_loss: 0.2659\n",
      "Epoch 1490/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3116 - val_loss: 0.2649\n",
      "Epoch 1491/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3128 - val_loss: 0.2673\n",
      "Epoch 1492/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3120 - val_loss: 0.2676\n",
      "Epoch 1493/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3120 - val_loss: 0.2648\n",
      "Epoch 1494/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3115 - val_loss: 0.2691\n",
      "Epoch 1495/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3116 - val_loss: 0.2658\n",
      "Epoch 1496/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3111 - val_loss: 0.2676\n",
      "Epoch 1497/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3110 - val_loss: 0.2677\n",
      "Epoch 1498/2000\n",
      "2/2 [==============================] - 1s 449ms/step - loss: 0.3121 - val_loss: 0.2679\n",
      "Epoch 1499/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3113 - val_loss: 0.2641\n",
      "Epoch 1500/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3114 - val_loss: 0.2685\n",
      "Epoch 1501/2000\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.3121 - val_loss: 0.2661\n",
      "Epoch 1502/2000\n",
      "2/2 [==============================] - 1s 514ms/step - loss: 0.3117 - val_loss: 0.2693\n",
      "Epoch 1503/2000\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 0.3112 - val_loss: 0.2685\n",
      "Epoch 1504/2000\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.3108 - val_loss: 0.2660\n",
      "Epoch 1505/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3118 - val_loss: 0.2645\n",
      "Epoch 1506/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3115 - val_loss: 0.2684\n",
      "Epoch 1507/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3112 - val_loss: 0.2676\n",
      "Epoch 1508/2000\n",
      "2/2 [==============================] - 1s 511ms/step - loss: 0.3118 - val_loss: 0.2674\n",
      "Epoch 1509/2000\n",
      "2/2 [==============================] - 1s 628ms/step - loss: 0.3107 - val_loss: 0.2663\n",
      "Epoch 1510/2000\n",
      "2/2 [==============================] - 1s 509ms/step - loss: 0.3127 - val_loss: 0.2644\n",
      "Epoch 1511/2000\n",
      "2/2 [==============================] - 1s 512ms/step - loss: 0.3117 - val_loss: 0.2665\n",
      "Epoch 1512/2000\n",
      "2/2 [==============================] - 1s 572ms/step - loss: 0.3118 - val_loss: 0.2674\n",
      "Epoch 1513/2000\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 0.3125 - val_loss: 0.2671\n",
      "Epoch 1514/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3110 - val_loss: 0.2676\n",
      "Epoch 1515/2000\n",
      "2/2 [==============================] - 1s 430ms/step - loss: 0.3111 - val_loss: 0.2656\n",
      "Epoch 1516/2000\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 0.3118 - val_loss: 0.2716\n",
      "Epoch 1517/2000\n",
      "2/2 [==============================] - 1s 694ms/step - loss: 0.3111 - val_loss: 0.2665\n",
      "Epoch 1518/2000\n",
      "2/2 [==============================] - 1s 587ms/step - loss: 0.3107 - val_loss: 0.2686\n",
      "Epoch 1519/2000\n",
      "2/2 [==============================] - 1s 574ms/step - loss: 0.3111 - val_loss: 0.2682\n",
      "Epoch 1520/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3108 - val_loss: 0.2678\n",
      "Epoch 1521/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.3095 - val_loss: 0.2687\n",
      "Epoch 1522/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3093 - val_loss: 0.2674\n",
      "Epoch 1523/2000\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 0.3096 - val_loss: 0.2680\n",
      "Epoch 1524/2000\n",
      "2/2 [==============================] - 1s 530ms/step - loss: 0.3109 - val_loss: 0.2650\n",
      "Epoch 1525/2000\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 0.3105 - val_loss: 0.2622\n",
      "Epoch 1526/2000\n",
      "2/2 [==============================] - 1s 586ms/step - loss: 0.3111 - val_loss: 0.2685\n",
      "Epoch 1527/2000\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 0.3101 - val_loss: 0.2655\n",
      "Epoch 1528/2000\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.3112 - val_loss: 0.2655\n",
      "Epoch 1529/2000\n",
      "2/2 [==============================] - 1s 474ms/step - loss: 0.3111 - val_loss: 0.2678\n",
      "Epoch 1530/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3103 - val_loss: 0.2626\n",
      "Epoch 1531/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3102 - val_loss: 0.2654\n",
      "Epoch 1532/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3093 - val_loss: 0.2676\n",
      "Epoch 1533/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3104 - val_loss: 0.2650\n",
      "Epoch 1534/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3106 - val_loss: 0.2702\n",
      "Epoch 1535/2000\n",
      "2/2 [==============================] - 1s 509ms/step - loss: 0.3094 - val_loss: 0.2670\n",
      "Epoch 1536/2000\n",
      "2/2 [==============================] - 1s 505ms/step - loss: 0.3106 - val_loss: 0.2654\n",
      "Epoch 1537/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3111 - val_loss: 0.2655\n",
      "Epoch 1538/2000\n",
      "2/2 [==============================] - 1s 504ms/step - loss: 0.3098 - val_loss: 0.2658\n",
      "Epoch 1539/2000\n",
      "2/2 [==============================] - 1s 441ms/step - loss: 0.3116 - val_loss: 0.2683\n",
      "Epoch 1540/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3107 - val_loss: 0.2676\n",
      "Epoch 1541/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3100 - val_loss: 0.2646\n",
      "Epoch 1542/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.3112 - val_loss: 0.2687\n",
      "Epoch 1543/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3099 - val_loss: 0.2655\n",
      "Epoch 1544/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3094 - val_loss: 0.2673\n",
      "Epoch 1545/2000\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.3095 - val_loss: 0.2673\n",
      "Epoch 1546/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3092 - val_loss: 0.2612\n",
      "Epoch 1547/2000\n",
      "2/2 [==============================] - 1s 502ms/step - loss: 0.3108 - val_loss: 0.2663\n",
      "Epoch 1548/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3079 - val_loss: 0.2665\n",
      "Epoch 1549/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3095 - val_loss: 0.2666\n",
      "Epoch 1550/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3094 - val_loss: 0.2677\n",
      "Epoch 1551/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3085 - val_loss: 0.2645\n",
      "Epoch 1552/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3099 - val_loss: 0.2663\n",
      "Epoch 1553/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3087 - val_loss: 0.2653\n",
      "Epoch 1554/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3085 - val_loss: 0.2663\n",
      "Epoch 1555/2000\n",
      "2/2 [==============================] - 1s 472ms/step - loss: 0.3078 - val_loss: 0.2621\n",
      "Epoch 1556/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3096 - val_loss: 0.2653\n",
      "Epoch 1557/2000\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 0.3079 - val_loss: 0.2654\n",
      "Epoch 1558/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3101 - val_loss: 0.2648\n",
      "Epoch 1559/2000\n",
      "2/2 [==============================] - 1s 496ms/step - loss: 0.3082 - val_loss: 0.2673\n",
      "Epoch 1560/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3094 - val_loss: 0.2661\n",
      "Epoch 1561/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3079 - val_loss: 0.2651\n",
      "Epoch 1562/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3092 - val_loss: 0.2650\n",
      "Epoch 1563/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3092 - val_loss: 0.2669\n",
      "Epoch 1564/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3093 - val_loss: 0.2673\n",
      "Epoch 1565/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3095 - val_loss: 0.2660\n",
      "Epoch 1566/2000\n",
      "2/2 [==============================] - 1s 486ms/step - loss: 0.3088 - val_loss: 0.2678\n",
      "Epoch 1567/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3083 - val_loss: 0.2645\n",
      "Epoch 1568/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3092 - val_loss: 0.2659\n",
      "Epoch 1569/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3093 - val_loss: 0.2609\n",
      "Epoch 1570/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3073 - val_loss: 0.2649\n",
      "Epoch 1571/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3079 - val_loss: 0.2696\n",
      "Epoch 1572/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3070 - val_loss: 0.2655\n",
      "Epoch 1573/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3073 - val_loss: 0.2657\n",
      "Epoch 1574/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3069 - val_loss: 0.2687\n",
      "Epoch 1575/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3082 - val_loss: 0.2654\n",
      "Epoch 1576/2000\n",
      "2/2 [==============================] - 1s 452ms/step - loss: 0.3080 - val_loss: 0.2657\n",
      "Epoch 1577/2000\n",
      "2/2 [==============================] - 1s 515ms/step - loss: 0.3092 - val_loss: 0.2632\n",
      "Epoch 1578/2000\n",
      "2/2 [==============================] - 1s 523ms/step - loss: 0.3098 - val_loss: 0.2634\n",
      "Epoch 1579/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3090 - val_loss: 0.2674\n",
      "Epoch 1580/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3079 - val_loss: 0.2636\n",
      "Epoch 1581/2000\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.3087 - val_loss: 0.2623\n",
      "Epoch 1582/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3082 - val_loss: 0.2652\n",
      "Epoch 1583/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3065 - val_loss: 0.2642\n",
      "Epoch 1584/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3089 - val_loss: 0.2662\n",
      "Epoch 1585/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3075 - val_loss: 0.2642\n",
      "Epoch 1586/2000\n",
      "2/2 [==============================] - 1s 517ms/step - loss: 0.3076 - val_loss: 0.2622\n",
      "Epoch 1587/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3078 - val_loss: 0.2650\n",
      "Epoch 1588/2000\n",
      "2/2 [==============================] - 1s 483ms/step - loss: 0.3071 - val_loss: 0.2645\n",
      "Epoch 1589/2000\n",
      "2/2 [==============================] - 1s 499ms/step - loss: 0.3079 - val_loss: 0.2645\n",
      "Epoch 1590/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3068 - val_loss: 0.2626\n",
      "Epoch 1591/2000\n",
      "2/2 [==============================] - 1s 469ms/step - loss: 0.3070 - val_loss: 0.2631\n",
      "Epoch 1592/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.3083 - val_loss: 0.2647\n",
      "Epoch 1593/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3064 - val_loss: 0.2652\n",
      "Epoch 1594/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3068 - val_loss: 0.2651\n",
      "Epoch 1595/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3071 - val_loss: 0.2642\n",
      "Epoch 1596/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3075 - val_loss: 0.2651\n",
      "Epoch 1597/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3069 - val_loss: 0.2635\n",
      "Epoch 1598/2000\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.3090 - val_loss: 0.2685\n",
      "Epoch 1599/2000\n",
      "2/2 [==============================] - 1s 479ms/step - loss: 0.3072 - val_loss: 0.2619\n",
      "Epoch 1600/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3082 - val_loss: 0.2655\n",
      "Epoch 1601/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3067 - val_loss: 0.2638\n",
      "Epoch 1602/2000\n",
      "2/2 [==============================] - 1s 561ms/step - loss: 0.3060 - val_loss: 0.2632\n",
      "Epoch 1603/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3070 - val_loss: 0.2593\n",
      "Epoch 1604/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3079 - val_loss: 0.2654\n",
      "Epoch 1605/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3069 - val_loss: 0.2630\n",
      "Epoch 1606/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3075 - val_loss: 0.2668\n",
      "Epoch 1607/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3069 - val_loss: 0.2638\n",
      "Epoch 1608/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3071 - val_loss: 0.2639\n",
      "Epoch 1609/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3073 - val_loss: 0.2659\n",
      "Epoch 1610/2000\n",
      "2/2 [==============================] - 1s 468ms/step - loss: 0.3066 - val_loss: 0.2655\n",
      "Epoch 1611/2000\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 0.3078 - val_loss: 0.2642\n",
      "Epoch 1612/2000\n",
      "2/2 [==============================] - 1s 495ms/step - loss: 0.3068 - val_loss: 0.2632\n",
      "Epoch 1613/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3069 - val_loss: 0.2665\n",
      "Epoch 1614/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3064 - val_loss: 0.2662\n",
      "Epoch 1615/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3060 - val_loss: 0.2653\n",
      "Epoch 1616/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3057 - val_loss: 0.2648\n",
      "Epoch 1617/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3073 - val_loss: 0.2641\n",
      "Epoch 1618/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3063 - val_loss: 0.2652\n",
      "Epoch 1619/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3067 - val_loss: 0.2643\n",
      "Epoch 1620/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3070 - val_loss: 0.2647\n",
      "Epoch 1621/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3064 - val_loss: 0.2618\n",
      "Epoch 1622/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3066 - val_loss: 0.2613\n",
      "Epoch 1623/2000\n",
      "2/2 [==============================] - 1s 510ms/step - loss: 0.3060 - val_loss: 0.2625\n",
      "Epoch 1624/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3052 - val_loss: 0.2626\n",
      "Epoch 1625/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3054 - val_loss: 0.2641\n",
      "Epoch 1626/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3057 - val_loss: 0.2657\n",
      "Epoch 1627/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3070 - val_loss: 0.2625\n",
      "Epoch 1628/2000\n",
      "2/2 [==============================] - 1s 424ms/step - loss: 0.3062 - val_loss: 0.2605\n",
      "Epoch 1629/2000\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.3061 - val_loss: 0.2636\n",
      "Epoch 1630/2000\n",
      "2/2 [==============================] - 1s 498ms/step - loss: 0.3059 - val_loss: 0.2626\n",
      "Epoch 1631/2000\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 0.3068 - val_loss: 0.2617\n",
      "Epoch 1632/2000\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.3058 - val_loss: 0.2603\n",
      "Epoch 1633/2000\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.3050 - val_loss: 0.2600\n",
      "Epoch 1634/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3049 - val_loss: 0.2626\n",
      "Epoch 1635/2000\n",
      "2/2 [==============================] - 1s 421ms/step - loss: 0.3061 - val_loss: 0.2630\n",
      "Epoch 1636/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3066 - val_loss: 0.2630\n",
      "Epoch 1637/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3061 - val_loss: 0.2645\n",
      "Epoch 1638/2000\n",
      "2/2 [==============================] - 1s 432ms/step - loss: 0.3053 - val_loss: 0.2634\n",
      "Epoch 1639/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3065 - val_loss: 0.2619\n",
      "Epoch 1640/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3060 - val_loss: 0.2633\n",
      "Epoch 1641/2000\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.3056 - val_loss: 0.2641\n",
      "Epoch 1642/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3063 - val_loss: 0.2662\n",
      "Epoch 1643/2000\n",
      "2/2 [==============================] - 1s 463ms/step - loss: 0.3055 - val_loss: 0.2606\n",
      "Epoch 1644/2000\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.3044 - val_loss: 0.2646\n",
      "Epoch 1645/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3047 - val_loss: 0.2629\n",
      "Epoch 1646/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3051 - val_loss: 0.2639\n",
      "Epoch 1647/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3057 - val_loss: 0.2623\n",
      "Epoch 1648/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3060 - val_loss: 0.2604\n",
      "Epoch 1649/2000\n",
      "2/2 [==============================] - 1s 437ms/step - loss: 0.3049 - val_loss: 0.2618\n",
      "Epoch 1650/2000\n",
      "2/2 [==============================] - 1s 451ms/step - loss: 0.3042 - val_loss: 0.2644\n",
      "Epoch 1651/2000\n",
      "2/2 [==============================] - 1s 500ms/step - loss: 0.3045 - val_loss: 0.2661\n",
      "Epoch 1652/2000\n",
      "2/2 [==============================] - 1s 465ms/step - loss: 0.3040 - val_loss: 0.2668\n",
      "Epoch 1653/2000\n",
      "2/2 [==============================] - 1s 460ms/step - loss: 0.3047 - val_loss: 0.2618\n",
      "Epoch 1654/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3054 - val_loss: 0.2624\n",
      "Epoch 1655/2000\n",
      "2/2 [==============================] - 1s 524ms/step - loss: 0.3052 - val_loss: 0.2602\n",
      "Epoch 1656/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3060 - val_loss: 0.2614\n",
      "Epoch 1657/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3052 - val_loss: 0.2633\n",
      "Epoch 1658/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3038 - val_loss: 0.2625\n",
      "Epoch 1659/2000\n",
      "2/2 [==============================] - 1s 435ms/step - loss: 0.3050 - val_loss: 0.2616\n",
      "Epoch 1660/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3041 - val_loss: 0.2645\n",
      "Epoch 1661/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3049 - val_loss: 0.2643\n",
      "Epoch 1662/2000\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.3042 - val_loss: 0.2623\n",
      "Epoch 1663/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3056 - val_loss: 0.2621\n",
      "Epoch 1664/2000\n",
      "2/2 [==============================] - 1s 467ms/step - loss: 0.3043 - val_loss: 0.2584\n",
      "Epoch 1665/2000\n",
      "2/2 [==============================] - 1s 445ms/step - loss: 0.3044 - val_loss: 0.2636\n",
      "Epoch 1666/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3045 - val_loss: 0.2613\n",
      "Epoch 1667/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3037 - val_loss: 0.2617\n",
      "Epoch 1668/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3042 - val_loss: 0.2644\n",
      "Epoch 1669/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3044 - val_loss: 0.2637\n",
      "Epoch 1670/2000\n",
      "2/2 [==============================] - 1s 459ms/step - loss: 0.3048 - val_loss: 0.2610\n",
      "Epoch 1671/2000\n",
      "2/2 [==============================] - 1s 461ms/step - loss: 0.3048 - val_loss: 0.2607\n",
      "Epoch 1672/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3038 - val_loss: 0.2598\n",
      "Epoch 1673/2000\n",
      "2/2 [==============================] - 1s 503ms/step - loss: 0.3023 - val_loss: 0.2633\n",
      "Epoch 1674/2000\n",
      "2/2 [==============================] - 1s 484ms/step - loss: 0.3043 - val_loss: 0.2593\n",
      "Epoch 1675/2000\n",
      "2/2 [==============================] - 1s 455ms/step - loss: 0.3041 - val_loss: 0.2619\n",
      "Epoch 1676/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3044 - val_loss: 0.2623\n",
      "Epoch 1677/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3035 - val_loss: 0.2629\n",
      "Epoch 1678/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3045 - val_loss: 0.2589\n",
      "Epoch 1679/2000\n",
      "2/2 [==============================] - 1s 443ms/step - loss: 0.3037 - val_loss: 0.2601\n",
      "Epoch 1680/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3041 - val_loss: 0.2614\n",
      "Epoch 1681/2000\n",
      "2/2 [==============================] - 1s 473ms/step - loss: 0.3034 - val_loss: 0.2588\n",
      "Epoch 1682/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3045 - val_loss: 0.2634\n",
      "Epoch 1683/2000\n",
      "2/2 [==============================] - 1s 497ms/step - loss: 0.3033 - val_loss: 0.2602\n",
      "Epoch 1684/2000\n",
      "2/2 [==============================] - 1s 466ms/step - loss: 0.3035 - val_loss: 0.2632\n",
      "Epoch 1685/2000\n",
      "2/2 [==============================] - 1s 522ms/step - loss: 0.3029 - val_loss: 0.2620\n",
      "Epoch 1686/2000\n",
      "2/2 [==============================] - 1s 431ms/step - loss: 0.3036 - val_loss: 0.2635\n",
      "Epoch 1687/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3038 - val_loss: 0.2619\n",
      "Epoch 1688/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3047 - val_loss: 0.2609\n",
      "Epoch 1689/2000\n",
      "2/2 [==============================] - 1s 427ms/step - loss: 0.3048 - val_loss: 0.2621\n",
      "Epoch 1690/2000\n",
      "2/2 [==============================] - 1s 434ms/step - loss: 0.3040 - val_loss: 0.2599\n",
      "Epoch 1691/2000\n",
      "2/2 [==============================] - 1s 464ms/step - loss: 0.3043 - val_loss: 0.2615\n",
      "Epoch 1692/2000\n",
      "2/2 [==============================] - 1s 462ms/step - loss: 0.3046 - val_loss: 0.2605\n",
      "Epoch 1693/2000\n",
      "2/2 [==============================] - 1s 457ms/step - loss: 0.3038 - val_loss: 0.2587\n",
      "Epoch 1694/2000\n",
      "2/2 [==============================] - 1s 454ms/step - loss: 0.3026 - val_loss: 0.2651\n",
      "Epoch 1695/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3041 - val_loss: 0.2618\n",
      "Epoch 1696/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3043 - val_loss: 0.2600\n",
      "Epoch 1697/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3039 - val_loss: 0.2640\n",
      "Epoch 1698/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3030 - val_loss: 0.2609\n",
      "Epoch 1699/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3033 - val_loss: 0.2619\n",
      "Epoch 1700/2000\n",
      "2/2 [==============================] - 1s 433ms/step - loss: 0.3032 - val_loss: 0.2636\n",
      "Epoch 1701/2000\n",
      "2/2 [==============================] - 1s 456ms/step - loss: 0.3037 - val_loss: 0.2598\n",
      "Epoch 1702/2000\n",
      "2/2 [==============================] - 1s 453ms/step - loss: 0.3033 - val_loss: 0.2613\n",
      "Epoch 1703/2000\n",
      "2/2 [==============================] - 1s 470ms/step - loss: 0.3032 - val_loss: 0.2612\n",
      "Epoch 1704/2000\n",
      "2/2 [==============================] - 1s 502ms/step - loss: 0.3033 - val_loss: 0.2606\n",
      "Epoch 1705/2000\n",
      "2/2 [==============================] - 1s 458ms/step - loss: 0.3038 - val_loss: 0.2618\n",
      "Epoch 1706/2000\n",
      "2/2 [==============================] - 1s 448ms/step - loss: 0.3020 - val_loss: 0.2619\n",
      "Epoch 1707/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3031 - val_loss: 0.2634\n",
      "Epoch 1708/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3021 - val_loss: 0.2633\n",
      "Epoch 1709/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3027 - val_loss: 0.2619\n",
      "Epoch 1710/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3026 - val_loss: 0.2613\n",
      "Epoch 1711/2000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.3037 - val_loss: 0.2591\n",
      "Epoch 1712/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3028 - val_loss: 0.2600\n",
      "Epoch 1713/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.3023 - val_loss: 0.2623\n",
      "Epoch 1714/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3027 - val_loss: 0.2611\n",
      "Epoch 1715/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3034 - val_loss: 0.2617\n",
      "Epoch 1716/2000\n",
      "2/2 [==============================] - 1s 396ms/step - loss: 0.3021 - val_loss: 0.2620\n",
      "Epoch 1717/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.3032 - val_loss: 0.2602\n",
      "Epoch 1718/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3022 - val_loss: 0.2627\n",
      "Epoch 1719/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3024 - val_loss: 0.2635\n",
      "Epoch 1720/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3025 - val_loss: 0.2612\n",
      "Epoch 1721/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3017 - val_loss: 0.2594\n",
      "Epoch 1722/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3020 - val_loss: 0.2594\n",
      "Epoch 1723/2000\n",
      "2/2 [==============================] - 1s 420ms/step - loss: 0.3034 - val_loss: 0.2619\n",
      "Epoch 1724/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3016 - val_loss: 0.2627\n",
      "Epoch 1725/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3024 - val_loss: 0.2604\n",
      "Epoch 1726/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3031 - val_loss: 0.2581\n",
      "Epoch 1727/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3024 - val_loss: 0.2603\n",
      "Epoch 1728/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3026 - val_loss: 0.2620\n",
      "Epoch 1729/2000\n",
      "2/2 [==============================] - 1s 429ms/step - loss: 0.3025 - val_loss: 0.2586\n",
      "Epoch 1730/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3015 - val_loss: 0.2613\n",
      "Epoch 1731/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3017 - val_loss: 0.2610\n",
      "Epoch 1732/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3003 - val_loss: 0.2591\n",
      "Epoch 1733/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.3025 - val_loss: 0.2583\n",
      "Epoch 1734/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3006 - val_loss: 0.2603\n",
      "Epoch 1735/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3029 - val_loss: 0.2615\n",
      "Epoch 1736/2000\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.3023 - val_loss: 0.2614\n",
      "Epoch 1737/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3015 - val_loss: 0.2607\n",
      "Epoch 1738/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3009 - val_loss: 0.2594\n",
      "Epoch 1739/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3014 - val_loss: 0.2606\n",
      "Epoch 1740/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3024 - val_loss: 0.2608\n",
      "Epoch 1741/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3022 - val_loss: 0.2575\n",
      "Epoch 1742/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3017 - val_loss: 0.2621\n",
      "Epoch 1743/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3008 - val_loss: 0.2622\n",
      "Epoch 1744/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3013 - val_loss: 0.2613\n",
      "Epoch 1745/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.3036 - val_loss: 0.2644\n",
      "Epoch 1746/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.3026 - val_loss: 0.2619\n",
      "Epoch 1747/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3012 - val_loss: 0.2611\n",
      "Epoch 1748/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3019 - val_loss: 0.2628\n",
      "Epoch 1749/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3004 - val_loss: 0.2590\n",
      "Epoch 1750/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2999 - val_loss: 0.2582\n",
      "Epoch 1751/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3018 - val_loss: 0.2589\n",
      "Epoch 1752/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3008 - val_loss: 0.2609\n",
      "Epoch 1753/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3009 - val_loss: 0.2604\n",
      "Epoch 1754/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3015 - val_loss: 0.2582\n",
      "Epoch 1755/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3010 - val_loss: 0.2591\n",
      "Epoch 1756/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3005 - val_loss: 0.2634\n",
      "Epoch 1757/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3010 - val_loss: 0.2571\n",
      "Epoch 1758/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3015 - val_loss: 0.2604\n",
      "Epoch 1759/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2996 - val_loss: 0.2603\n",
      "Epoch 1760/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3003 - val_loss: 0.2600\n",
      "Epoch 1761/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3001 - val_loss: 0.2633\n",
      "Epoch 1762/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3010 - val_loss: 0.2620\n",
      "Epoch 1763/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3021 - val_loss: 0.2624\n",
      "Epoch 1764/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3017 - val_loss: 0.2591\n",
      "Epoch 1765/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.3016 - val_loss: 0.2597\n",
      "Epoch 1766/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.3001 - val_loss: 0.2596\n",
      "Epoch 1767/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3010 - val_loss: 0.2620\n",
      "Epoch 1768/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2999 - val_loss: 0.2602\n",
      "Epoch 1769/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.3015 - val_loss: 0.2583\n",
      "Epoch 1770/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3005 - val_loss: 0.2586\n",
      "Epoch 1771/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.3002 - val_loss: 0.2618\n",
      "Epoch 1772/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3017 - val_loss: 0.2567\n",
      "Epoch 1773/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.3013 - val_loss: 0.2584\n",
      "Epoch 1774/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.3017 - val_loss: 0.2606\n",
      "Epoch 1775/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.3009 - val_loss: 0.2580\n",
      "Epoch 1776/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2997 - val_loss: 0.2572\n",
      "Epoch 1777/2000\n",
      "2/2 [==============================] - 1s 415ms/step - loss: 0.3014 - val_loss: 0.2598\n",
      "Epoch 1778/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3002 - val_loss: 0.2572\n",
      "Epoch 1779/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3007 - val_loss: 0.2587\n",
      "Epoch 1780/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3016 - val_loss: 0.2580\n",
      "Epoch 1781/2000\n",
      "2/2 [==============================] - 1s 398ms/step - loss: 0.2993 - val_loss: 0.2612\n",
      "Epoch 1782/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2992 - val_loss: 0.2598\n",
      "Epoch 1783/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2992 - val_loss: 0.2615\n",
      "Epoch 1784/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2995 - val_loss: 0.2556\n",
      "Epoch 1785/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3004 - val_loss: 0.2581\n",
      "Epoch 1786/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.3006 - val_loss: 0.2601\n",
      "Epoch 1787/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2998 - val_loss: 0.2604\n",
      "Epoch 1788/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.3011 - val_loss: 0.2576\n",
      "Epoch 1789/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2988 - val_loss: 0.2579\n",
      "Epoch 1790/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.3000 - val_loss: 0.2558\n",
      "Epoch 1791/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.3003 - val_loss: 0.2573\n",
      "Epoch 1792/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.3000 - val_loss: 0.2601\n",
      "Epoch 1793/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2998 - val_loss: 0.2583\n",
      "Epoch 1794/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2997 - val_loss: 0.2590\n",
      "Epoch 1795/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.3001 - val_loss: 0.2599\n",
      "Epoch 1796/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2996 - val_loss: 0.2591\n",
      "Epoch 1797/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2997 - val_loss: 0.2601\n",
      "Epoch 1798/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2954 - val_loss: 0.2574\n",
      "Epoch 1947/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2946 - val_loss: 0.2548\n",
      "Epoch 1948/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2944 - val_loss: 0.2562\n",
      "Epoch 1949/2000\n",
      "2/2 [==============================] - 1s 417ms/step - loss: 0.2938 - val_loss: 0.2532\n",
      "Epoch 1950/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2937 - val_loss: 0.2570\n",
      "Epoch 1951/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2946 - val_loss: 0.2531\n",
      "Epoch 1952/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2943 - val_loss: 0.2575\n",
      "Epoch 1953/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2946 - val_loss: 0.2541\n",
      "Epoch 1954/2000\n",
      "2/2 [==============================] - 1s 411ms/step - loss: 0.2951 - val_loss: 0.2521\n",
      "Epoch 1955/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2947 - val_loss: 0.2571\n",
      "Epoch 1956/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2939 - val_loss: 0.2524\n",
      "Epoch 1957/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2935 - val_loss: 0.2565\n",
      "Epoch 1958/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2939 - val_loss: 0.2573\n",
      "Epoch 1959/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2947 - val_loss: 0.2570\n",
      "Epoch 1960/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2933 - val_loss: 0.2554\n",
      "Epoch 1961/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2940 - val_loss: 0.2568\n",
      "Epoch 1962/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2946 - val_loss: 0.2575\n",
      "Epoch 1963/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2933 - val_loss: 0.2568\n",
      "Epoch 1964/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2927 - val_loss: 0.2555\n",
      "Epoch 1965/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2951 - val_loss: 0.2569\n",
      "Epoch 1966/2000\n",
      "2/2 [==============================] - 1s 413ms/step - loss: 0.2936 - val_loss: 0.2564\n",
      "Epoch 1967/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2943 - val_loss: 0.2537\n",
      "Epoch 1968/2000\n",
      "2/2 [==============================] - 1s 408ms/step - loss: 0.2940 - val_loss: 0.2539\n",
      "Epoch 1969/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2938 - val_loss: 0.2594\n",
      "Epoch 1970/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2937 - val_loss: 0.2518\n",
      "Epoch 1971/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2939 - val_loss: 0.2527\n",
      "Epoch 1972/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2936 - val_loss: 0.2540\n",
      "Epoch 1973/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2948 - val_loss: 0.2550\n",
      "Epoch 1974/2000\n",
      "2/2 [==============================] - 1s 403ms/step - loss: 0.2934 - val_loss: 0.2509\n",
      "Epoch 1975/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2925 - val_loss: 0.2569\n",
      "Epoch 1976/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2923 - val_loss: 0.2545\n",
      "Epoch 1977/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.2923 - val_loss: 0.2525\n",
      "Epoch 1978/2000\n",
      "2/2 [==============================] - 1s 416ms/step - loss: 0.2942 - val_loss: 0.2545\n",
      "Epoch 1979/2000\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.2940 - val_loss: 0.2568\n",
      "Epoch 1980/2000\n",
      "2/2 [==============================] - 1s 414ms/step - loss: 0.2928 - val_loss: 0.2562\n",
      "Epoch 1981/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2939 - val_loss: 0.2548\n",
      "Epoch 1982/2000\n",
      "2/2 [==============================] - 1s 402ms/step - loss: 0.2927 - val_loss: 0.2563\n",
      "Epoch 1983/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.2931 - val_loss: 0.2550\n",
      "Epoch 1984/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2932 - val_loss: 0.2544\n",
      "Epoch 1985/2000\n",
      "2/2 [==============================] - 1s 409ms/step - loss: 0.2922 - val_loss: 0.2541\n",
      "Epoch 1986/2000\n",
      "2/2 [==============================] - 1s 412ms/step - loss: 0.2929 - val_loss: 0.2537\n",
      "Epoch 1987/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2937 - val_loss: 0.2519\n",
      "Epoch 1988/2000\n",
      "2/2 [==============================] - 1s 400ms/step - loss: 0.2935 - val_loss: 0.2554\n",
      "Epoch 1989/2000\n",
      "2/2 [==============================] - 1s 419ms/step - loss: 0.2934 - val_loss: 0.2560\n",
      "Epoch 1990/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2932 - val_loss: 0.2555\n",
      "Epoch 1991/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2934 - val_loss: 0.2560\n",
      "Epoch 1992/2000\n",
      "2/2 [==============================] - 1s 407ms/step - loss: 0.2931 - val_loss: 0.2572\n",
      "Epoch 1993/2000\n",
      "2/2 [==============================] - 1s 397ms/step - loss: 0.2922 - val_loss: 0.2534\n",
      "Epoch 1994/2000\n",
      "2/2 [==============================] - 1s 399ms/step - loss: 0.2924 - val_loss: 0.2563\n",
      "Epoch 1995/2000\n",
      "2/2 [==============================] - 1s 422ms/step - loss: 0.2926 - val_loss: 0.2578\n",
      "Epoch 1996/2000\n",
      "2/2 [==============================] - 1s 425ms/step - loss: 0.2944 - val_loss: 0.2570\n",
      "Epoch 1997/2000\n",
      "2/2 [==============================] - 1s 406ms/step - loss: 0.2932 - val_loss: 0.2539\n",
      "Epoch 1998/2000\n",
      "2/2 [==============================] - 1s 410ms/step - loss: 0.2930 - val_loss: 0.2588\n",
      "Epoch 1999/2000\n",
      "2/2 [==============================] - 1s 404ms/step - loss: 0.2944 - val_loss: 0.2534\n",
      "Epoch 2000/2000\n",
      "2/2 [==============================] - 1s 401ms/step - loss: 0.2929 - val_loss: 0.2559\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "outdir = './FoundationModel/'\n",
    "\n",
    "trainer = Trainer(\n",
    "    out_dir = outdir,\n",
    "    max_features_percentile=max_features_percentile,\n",
    "    test_size=test_size,\n",
    "    mode=mode,\n",
    "    model=SelfSupervisedTransformer, \n",
    "    dataloader=SelfSupervisedDataGenerator,\n",
    "    loss=loss,\n",
    "    metrics=[]\n",
    ")\n",
    "\n",
    "trainer.setup_data(\n",
    "    data, \n",
    "    discrete_features = [],\n",
    "    continuous_features = features,\n",
    ")\n",
    "\n",
    "trainer.setup_model(\n",
    "    embedding_size=embedding_size, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size_max = False,\n",
    "    batch_size=4000, # This will take a batch with the size of the training / testing shape,\n",
    "    save_best_only=False\n",
    ")\n",
    "\n",
    "trainer.fit(repetitions=repetitions, epochs=epochs, verbose=verbose, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a83c8c-3843-48f1-a19b-70100b1e7a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ./FoundationModel/\n"
     ]
    }
   ],
   "source": [
    "from xai.models import clean_run\n",
    "\n",
    "clean_run(\n",
    "    path='./FoundationModel/',\n",
    "    keep=[1, 10, 100, 500, 1000,1500,2000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7ee409a-6c80-46c3-a638-a3f21b32afac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='white', font_scale=1.5)\n",
    "dfh = pd.read_csv(\"./FoundationModel/fold-0_id-0/history.csv\", index_col=0)\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "dfh.plot(ax=ax)\n",
    "ax.tick_params(bottom=True, left=True)\n",
    "ax.set_ylabel('Self-supervised loss')\n",
    "ax.set_title('COMPASS + Clinical Transformer')\n",
    "sns.despine()\n",
    "fig.savefig('./FoundationModel/fold-0_id-0/history.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e2fb1-5dfe-46f7-af05-8a5a3430b393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e54a99f-0749-4a53-b914-416728a4d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234924e-1b0b-4c61-ad8b-fcd330f4a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26eb7a-1bb4-4af5-9e06-aee84fd47e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
