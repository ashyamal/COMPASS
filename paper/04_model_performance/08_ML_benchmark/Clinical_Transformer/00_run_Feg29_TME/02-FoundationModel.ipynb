{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cf5663a-f421-439d-a3e5-cb89329a3db6",
   "metadata": {},
   "source": [
    "# Self supervised Learning\n",
    "\n",
    "Pretrain a \"foundation model\" using data without outcomes, from a large unlabeled dataset. Note that the idea behind the pre-training is to guide the fine tunning task (survival prediction). **Note that given this is a toy example the pre-training effect may be marginal.** \n",
    "\n",
    "The input to the mode are the features from the dataset and **not the outcomes**\n",
    "\n",
    "**Note that this notebook may take a while to complete when number of epochs is large**: For self supervised learning it is recommended to use a large  number of epochs. For illustration, we ran it for 1,000 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10b49798-69ad-401a-ad69-4d071232e805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 19:03:52.743305: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-12 19:03:52.799583: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-12 19:03:53.147957: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shenwanxiang/anaconda3/lib:\n",
      "2025-08-12 19:03:53.147990: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/shenwanxiang/anaconda3/lib:\n",
      "2025-08-12 19:03:53.147992: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1662269/3348793700.py:4: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 19:03:53.557343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-12 19:03:53.674002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:53.674108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:53.690109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:53.690204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:53.690265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:53.690319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 9998 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-08-12 19:03:54.246745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:54.246802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:1 with 991 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:05:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=/home/shenwanxiang/anaconda3\"\n",
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d7a0eb-e2ff-4541-bb5a-54325c0cfa56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../codeocean/environment/clinical_transformer/')\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "from xai.models.SimplifiedClinicalTransformer.Trainer import Trainer\n",
    "from xai.losses.survival import cIndex_SigmoidApprox as cindex_loss\n",
    "from xai.metrics.survival import sigmoid_concordance as cindex\n",
    "\n",
    "from xai.models import Trainer\n",
    "from xai.models import SelfSupervisedTransformer\n",
    "from xai.models import OptimizedSelfSupervisedDataGenerator as SelfSupervisedDataGenerator\n",
    "\n",
    "from xai.losses.selfsupervision.classifier_regression import CompositeLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b96e3cfe-bac1-4be7-9949-282247270dc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from samecode.random import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcde68ad-6c9c-4297-80a5-3afb7d02548b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308bd8fc-9636-4b16-9e2c-a6ce858d5af0",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "568c797d-141f-47eb-9648-29130f706f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_features_percentile=100\n",
    "test_size=0.1\n",
    "mode='self-supervision'\n",
    "learning_rate=0.0001\n",
    "repetitions=1\n",
    "epochs=2000\n",
    "verbose=2\n",
    "seed=0\n",
    "embedding_size = 128\n",
    "num_heads = 2\n",
    "num_layers = 2\n",
    "\n",
    "loss = CompositeLoss(feature_w=1, value_w=0.1) # Contribution of individual losses (predicts keys, values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc549ab4-af3f-4652-85ec-bfe9d635441b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/dataset-pretrain.data.csv')\n",
    "features = data.columns[-29:].tolist()\n",
    "#data = data[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a05e947-03f0-4ce0-abaf-bd27fd4258bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: /home/shenwanxiang/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/sh)\n"
     ]
    }
   ],
   "source": [
    "!rm -r ./results/runs/FoundationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea6168-00ff-4b1c-88d7-7aeed37802be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shenwanxiang/.local/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO\t2025-08-12 19:03:55,466\tSetting up working directory: ./results/runs/FoundationModel/\n",
      "INFO\t2025-08-12 19:03:55,467\tNumber of continuous features: 29\n",
      "INFO\t2025-08-12 19:03:55,468\tNumber of discrete features: 0\n",
      "INFO\t2025-08-12 19:03:55,468\tNumber of samples: 10184\n",
      "INFO\t2025-08-12 19:03:55,478\tNumber of classes: 37\n",
      "INFO\t2025-08-12 19:03:55,478\tRUN ID: fold-0_id-0\n",
      "INFO\t2025-08-12 19:03:55,479\tRUN ID out directory: ./results/runs/FoundationModel//fold-0_id-0/\n",
      "INFO\t2025-08-12 19:03:56,130\tTraining samples: 9165\n",
      "INFO\t2025-08-12 19:03:56,131\tTesting samples: 1019\n",
      "INFO\t2025-08-12 19:03:56,148\tNumber of features at 100th percentile: 29 that are non nans\n",
      "2025-08-12 19:03:56.149381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.149527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.149597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.149651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.149703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.149756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.150903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.151009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.151064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.151117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.151168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9998 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2025-08-12 19:03:56.151185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2025-08-12 19:03:56.151229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 991 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080 Ti, pci bus id: 0000:05:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9d5a44b9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 19:03:56.814223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "WARNING\t2025-08-12 19:03:56,954\tAutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9d5a44b9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f9d5a44b9e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f9d6a666390>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,960\tAutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f9d6a666390>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method SelfSupervisedTransformer.call of <xai.models.SimplifiedClinicalTransformer.Topologies.BertLikeTransformer.BertLikeAttention.SelfSupervisedTransformer object at 0x7f9d6a666390>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f9d6a666c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,968\tAutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f9d6a666c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Encoder.call of <xai.models.SimplifiedClinicalTransformer.utils.Encoder object at 0x7f9d6a666c90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f9d6a6667d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,973\tAutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f9d6a6667d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method NumericalEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.NumericalEmbeddingLayer object at 0x7f9d6a6667d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f9d6a5f2c50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,982\tAutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f9d6a5f2c50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method TokenEmbeddingLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.TokenEmbeddingLayer object at 0x7f9d6a5f2c50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f9d6a5fbf90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,991\tAutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f9d6a5fbf90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method EncoderLayer.call of <xai.models.SimplifiedClinicalTransformer.utils.EncoderLayer object at 0x7f9d6a5fbf90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f9d6a607790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:56,996\tAutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f9d6a607790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method MultiHeadAttention.call of <xai.models.SimplifiedClinicalTransformer.utils.MultiHeadAttention object at 0x7f9d6a607790>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f9d78c19690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:57,135\tAutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f9d78c19690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method CompositeLoss.call of <xai.losses.selfsupervision.classifier_regression.CompositeLoss object at 0x7f9d78c19690>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Constant'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f9d5a1532d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:03:57,338\tAutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f9d5a1532d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method _BaseOptimizer._update_step_xla of <tensorflow.python.eager.polymorphic_function.tracing_compiler.TfMethodTarget object at 0x7f9d5a1532d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 19:03:58.788800: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f9b4003d320 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-12 19:03:58.788865: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2025-08-12 19:03:58.788878: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): NVIDIA GeForce RTX 3080 Ti, Compute Capability 8.6\n",
      "2025-08-12 19:03:58.799440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-12 19:03:58.838803: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-08-12 19:03:58.858175: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9c12527ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING\t2025-08-12 19:04:00,332\tAutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9c12527ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x7f9c12527ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1/1 - 4s - loss: 0.9035 - val_loss: 0.8550 - 4s/epoch - 4s/step\n",
      "Epoch 2/2000\n",
      "1/1 - 0s - loss: 0.8872 - val_loss: 0.8372 - 342ms/epoch - 342ms/step\n",
      "Epoch 3/2000\n",
      "1/1 - 0s - loss: 0.8678 - val_loss: 0.8145 - 335ms/epoch - 335ms/step\n",
      "Epoch 4/2000\n",
      "1/1 - 0s - loss: 0.8511 - val_loss: 0.7951 - 337ms/epoch - 337ms/step\n",
      "Epoch 5/2000\n",
      "1/1 - 0s - loss: 0.8373 - val_loss: 0.7758 - 374ms/epoch - 374ms/step\n",
      "Epoch 6/2000\n",
      "1/1 - 0s - loss: 0.8236 - val_loss: 0.7571 - 335ms/epoch - 335ms/step\n",
      "Epoch 7/2000\n",
      "1/1 - 0s - loss: 0.8091 - val_loss: 0.7405 - 339ms/epoch - 339ms/step\n",
      "Epoch 8/2000\n",
      "1/1 - 0s - loss: 0.7989 - val_loss: 0.7269 - 339ms/epoch - 339ms/step\n",
      "Epoch 9/2000\n",
      "1/1 - 0s - loss: 0.7894 - val_loss: 0.7179 - 350ms/epoch - 350ms/step\n",
      "Epoch 10/2000\n",
      "1/1 - 0s - loss: 0.7798 - val_loss: 0.7062 - 334ms/epoch - 334ms/step\n",
      "Epoch 11/2000\n",
      "1/1 - 0s - loss: 0.7710 - val_loss: 0.6962 - 333ms/epoch - 333ms/step\n",
      "Epoch 12/2000\n",
      "1/1 - 0s - loss: 0.7630 - val_loss: 0.6918 - 334ms/epoch - 334ms/step\n",
      "Epoch 13/2000\n",
      "1/1 - 0s - loss: 0.7575 - val_loss: 0.6877 - 337ms/epoch - 337ms/step\n",
      "Epoch 14/2000\n",
      "1/1 - 0s - loss: 0.7505 - val_loss: 0.6820 - 338ms/epoch - 338ms/step\n",
      "Epoch 15/2000\n",
      "1/1 - 0s - loss: 0.7444 - val_loss: 0.6782 - 359ms/epoch - 359ms/step\n",
      "Epoch 16/2000\n",
      "1/1 - 0s - loss: 0.7400 - val_loss: 0.6731 - 339ms/epoch - 339ms/step\n",
      "Epoch 17/2000\n",
      "1/1 - 0s - loss: 0.7348 - val_loss: 0.6679 - 334ms/epoch - 334ms/step\n",
      "Epoch 18/2000\n",
      "1/1 - 0s - loss: 0.7296 - val_loss: 0.6651 - 336ms/epoch - 336ms/step\n",
      "Epoch 19/2000\n",
      "1/1 - 0s - loss: 0.7266 - val_loss: 0.6596 - 343ms/epoch - 343ms/step\n",
      "Epoch 20/2000\n",
      "1/1 - 0s - loss: 0.7212 - val_loss: 0.6544 - 332ms/epoch - 332ms/step\n",
      "Epoch 21/2000\n",
      "1/1 - 0s - loss: 0.7175 - val_loss: 0.6516 - 333ms/epoch - 333ms/step\n",
      "Epoch 22/2000\n",
      "1/1 - 0s - loss: 0.7159 - val_loss: 0.6444 - 335ms/epoch - 335ms/step\n",
      "Epoch 23/2000\n",
      "1/1 - 0s - loss: 0.7111 - val_loss: 0.6405 - 335ms/epoch - 335ms/step\n",
      "Epoch 24/2000\n",
      "1/1 - 0s - loss: 0.7062 - val_loss: 0.6390 - 335ms/epoch - 335ms/step\n",
      "Epoch 25/2000\n",
      "1/1 - 0s - loss: 0.7042 - val_loss: 0.6356 - 336ms/epoch - 336ms/step\n",
      "Epoch 26/2000\n",
      "1/1 - 0s - loss: 0.7017 - val_loss: 0.6312 - 332ms/epoch - 332ms/step\n",
      "Epoch 27/2000\n",
      "1/1 - 0s - loss: 0.6983 - val_loss: 0.6295 - 346ms/epoch - 346ms/step\n",
      "Epoch 28/2000\n",
      "1/1 - 0s - loss: 0.6951 - val_loss: 0.6272 - 342ms/epoch - 342ms/step\n",
      "Epoch 29/2000\n",
      "1/1 - 0s - loss: 0.6923 - val_loss: 0.6255 - 335ms/epoch - 335ms/step\n",
      "Epoch 30/2000\n",
      "1/1 - 0s - loss: 0.6902 - val_loss: 0.6217 - 355ms/epoch - 355ms/step\n",
      "Epoch 31/2000\n",
      "1/1 - 0s - loss: 0.6883 - val_loss: 0.6194 - 338ms/epoch - 338ms/step\n",
      "Epoch 32/2000\n",
      "1/1 - 0s - loss: 0.6853 - val_loss: 0.6173 - 359ms/epoch - 359ms/step\n",
      "Epoch 33/2000\n",
      "1/1 - 0s - loss: 0.6828 - val_loss: 0.6154 - 332ms/epoch - 332ms/step\n",
      "Epoch 34/2000\n",
      "1/1 - 0s - loss: 0.6806 - val_loss: 0.6139 - 342ms/epoch - 342ms/step\n",
      "Epoch 35/2000\n",
      "1/1 - 0s - loss: 0.6776 - val_loss: 0.6123 - 344ms/epoch - 344ms/step\n",
      "Epoch 36/2000\n",
      "1/1 - 0s - loss: 0.6758 - val_loss: 0.6104 - 339ms/epoch - 339ms/step\n",
      "Epoch 37/2000\n",
      "1/1 - 0s - loss: 0.6737 - val_loss: 0.6084 - 347ms/epoch - 347ms/step\n",
      "Epoch 38/2000\n",
      "1/1 - 0s - loss: 0.6716 - val_loss: 0.6068 - 338ms/epoch - 338ms/step\n",
      "Epoch 39/2000\n",
      "1/1 - 0s - loss: 0.6701 - val_loss: 0.6040 - 344ms/epoch - 344ms/step\n",
      "Epoch 40/2000\n",
      "1/1 - 0s - loss: 0.6670 - val_loss: 0.6026 - 335ms/epoch - 335ms/step\n",
      "Epoch 41/2000\n",
      "1/1 - 0s - loss: 0.6661 - val_loss: 0.6014 - 337ms/epoch - 337ms/step\n",
      "Epoch 42/2000\n",
      "1/1 - 0s - loss: 0.6642 - val_loss: 0.5989 - 354ms/epoch - 354ms/step\n",
      "Epoch 43/2000\n",
      "1/1 - 0s - loss: 0.6620 - val_loss: 0.5979 - 343ms/epoch - 343ms/step\n",
      "Epoch 44/2000\n",
      "1/1 - 0s - loss: 0.6591 - val_loss: 0.5960 - 332ms/epoch - 332ms/step\n",
      "Epoch 45/2000\n",
      "1/1 - 0s - loss: 0.6579 - val_loss: 0.5954 - 348ms/epoch - 348ms/step\n",
      "Epoch 46/2000\n",
      "1/1 - 0s - loss: 0.6562 - val_loss: 0.5934 - 340ms/epoch - 340ms/step\n",
      "Epoch 47/2000\n",
      "1/1 - 0s - loss: 0.6536 - val_loss: 0.5912 - 339ms/epoch - 339ms/step\n",
      "Epoch 48/2000\n",
      "1/1 - 0s - loss: 0.6514 - val_loss: 0.5897 - 338ms/epoch - 338ms/step\n",
      "Epoch 49/2000\n",
      "1/1 - 0s - loss: 0.6503 - val_loss: 0.5884 - 334ms/epoch - 334ms/step\n",
      "Epoch 50/2000\n",
      "1/1 - 0s - loss: 0.6483 - val_loss: 0.5872 - 337ms/epoch - 337ms/step\n",
      "Epoch 51/2000\n",
      "1/1 - 0s - loss: 0.6459 - val_loss: 0.5843 - 350ms/epoch - 350ms/step\n",
      "Epoch 52/2000\n",
      "1/1 - 0s - loss: 0.6434 - val_loss: 0.5828 - 337ms/epoch - 337ms/step\n",
      "Epoch 53/2000\n",
      "1/1 - 0s - loss: 0.6426 - val_loss: 0.5812 - 346ms/epoch - 346ms/step\n",
      "Epoch 54/2000\n",
      "1/1 - 0s - loss: 0.6403 - val_loss: 0.5801 - 347ms/epoch - 347ms/step\n",
      "Epoch 55/2000\n",
      "1/1 - 0s - loss: 0.6394 - val_loss: 0.5799 - 331ms/epoch - 331ms/step\n",
      "Epoch 56/2000\n",
      "1/1 - 0s - loss: 0.6360 - val_loss: 0.5777 - 331ms/epoch - 331ms/step\n",
      "Epoch 57/2000\n",
      "1/1 - 0s - loss: 0.6350 - val_loss: 0.5758 - 338ms/epoch - 338ms/step\n",
      "Epoch 58/2000\n",
      "1/1 - 0s - loss: 0.6325 - val_loss: 0.5745 - 336ms/epoch - 336ms/step\n",
      "Epoch 59/2000\n",
      "1/1 - 0s - loss: 0.6321 - val_loss: 0.5733 - 339ms/epoch - 339ms/step\n",
      "Epoch 60/2000\n",
      "1/1 - 0s - loss: 0.6305 - val_loss: 0.5715 - 347ms/epoch - 347ms/step\n",
      "Epoch 61/2000\n",
      "1/1 - 0s - loss: 0.6278 - val_loss: 0.5698 - 346ms/epoch - 346ms/step\n",
      "Epoch 62/2000\n",
      "1/1 - 0s - loss: 0.6262 - val_loss: 0.5682 - 334ms/epoch - 334ms/step\n",
      "Epoch 63/2000\n",
      "1/1 - 0s - loss: 0.6244 - val_loss: 0.5675 - 340ms/epoch - 340ms/step\n",
      "Epoch 64/2000\n",
      "1/1 - 0s - loss: 0.6231 - val_loss: 0.5666 - 340ms/epoch - 340ms/step\n",
      "Epoch 65/2000\n",
      "1/1 - 0s - loss: 0.6210 - val_loss: 0.5650 - 342ms/epoch - 342ms/step\n",
      "Epoch 66/2000\n",
      "1/1 - 0s - loss: 0.6192 - val_loss: 0.5645 - 345ms/epoch - 345ms/step\n",
      "Epoch 67/2000\n",
      "1/1 - 0s - loss: 0.6172 - val_loss: 0.5628 - 342ms/epoch - 342ms/step\n",
      "Epoch 68/2000\n",
      "1/1 - 0s - loss: 0.6159 - val_loss: 0.5617 - 341ms/epoch - 341ms/step\n",
      "Epoch 69/2000\n",
      "1/1 - 0s - loss: 0.6147 - val_loss: 0.5608 - 337ms/epoch - 337ms/step\n",
      "Epoch 70/2000\n",
      "1/1 - 0s - loss: 0.6146 - val_loss: 0.5590 - 342ms/epoch - 342ms/step\n",
      "Epoch 71/2000\n",
      "1/1 - 0s - loss: 0.6120 - val_loss: 0.5588 - 335ms/epoch - 335ms/step\n",
      "Epoch 72/2000\n",
      "1/1 - 0s - loss: 0.6107 - val_loss: 0.5573 - 343ms/epoch - 343ms/step\n",
      "Epoch 73/2000\n",
      "1/1 - 0s - loss: 0.6099 - val_loss: 0.5559 - 333ms/epoch - 333ms/step\n",
      "Epoch 74/2000\n",
      "1/1 - 0s - loss: 0.6076 - val_loss: 0.5546 - 338ms/epoch - 338ms/step\n",
      "Epoch 75/2000\n",
      "1/1 - 0s - loss: 0.6068 - val_loss: 0.5552 - 344ms/epoch - 344ms/step\n",
      "Epoch 76/2000\n",
      "1/1 - 0s - loss: 0.6052 - val_loss: 0.5531 - 339ms/epoch - 339ms/step\n",
      "Epoch 77/2000\n",
      "1/1 - 0s - loss: 0.6036 - val_loss: 0.5524 - 338ms/epoch - 338ms/step\n",
      "Epoch 78/2000\n",
      "1/1 - 0s - loss: 0.6023 - val_loss: 0.5518 - 332ms/epoch - 332ms/step\n",
      "Epoch 79/2000\n",
      "1/1 - 0s - loss: 0.6011 - val_loss: 0.5504 - 338ms/epoch - 338ms/step\n",
      "Epoch 80/2000\n",
      "1/1 - 0s - loss: 0.5994 - val_loss: 0.5506 - 338ms/epoch - 338ms/step\n",
      "Epoch 81/2000\n",
      "1/1 - 0s - loss: 0.5998 - val_loss: 0.5476 - 347ms/epoch - 347ms/step\n",
      "Epoch 82/2000\n",
      "1/1 - 0s - loss: 0.5985 - val_loss: 0.5473 - 349ms/epoch - 349ms/step\n",
      "Epoch 83/2000\n",
      "1/1 - 0s - loss: 0.5973 - val_loss: 0.5469 - 348ms/epoch - 348ms/step\n",
      "Epoch 84/2000\n",
      "1/1 - 0s - loss: 0.5966 - val_loss: 0.5460 - 335ms/epoch - 335ms/step\n",
      "Epoch 85/2000\n",
      "1/1 - 0s - loss: 0.5952 - val_loss: 0.5451 - 329ms/epoch - 329ms/step\n",
      "Epoch 86/2000\n",
      "1/1 - 0s - loss: 0.5936 - val_loss: 0.5449 - 343ms/epoch - 343ms/step\n",
      "Epoch 87/2000\n",
      "1/1 - 0s - loss: 0.5927 - val_loss: 0.5448 - 338ms/epoch - 338ms/step\n",
      "Epoch 88/2000\n",
      "1/1 - 0s - loss: 0.5914 - val_loss: 0.5442 - 331ms/epoch - 331ms/step\n",
      "Epoch 89/2000\n",
      "1/1 - 0s - loss: 0.5911 - val_loss: 0.5428 - 344ms/epoch - 344ms/step\n",
      "Epoch 90/2000\n",
      "1/1 - 0s - loss: 0.5901 - val_loss: 0.5415 - 352ms/epoch - 352ms/step\n",
      "Epoch 91/2000\n",
      "1/1 - 0s - loss: 0.5886 - val_loss: 0.5400 - 358ms/epoch - 358ms/step\n",
      "Epoch 92/2000\n",
      "1/1 - 0s - loss: 0.5874 - val_loss: 0.5399 - 349ms/epoch - 349ms/step\n",
      "Epoch 93/2000\n",
      "1/1 - 0s - loss: 0.5872 - val_loss: 0.5384 - 339ms/epoch - 339ms/step\n",
      "Epoch 94/2000\n",
      "1/1 - 0s - loss: 0.5864 - val_loss: 0.5384 - 335ms/epoch - 335ms/step\n",
      "Epoch 95/2000\n",
      "1/1 - 0s - loss: 0.5859 - val_loss: 0.5380 - 341ms/epoch - 341ms/step\n",
      "Epoch 96/2000\n",
      "1/1 - 0s - loss: 0.5842 - val_loss: 0.5367 - 338ms/epoch - 338ms/step\n",
      "Epoch 97/2000\n",
      "1/1 - 0s - loss: 0.5823 - val_loss: 0.5366 - 338ms/epoch - 338ms/step\n",
      "Epoch 98/2000\n",
      "1/1 - 0s - loss: 0.5821 - val_loss: 0.5355 - 350ms/epoch - 350ms/step\n",
      "Epoch 99/2000\n",
      "1/1 - 0s - loss: 0.5822 - val_loss: 0.5365 - 336ms/epoch - 336ms/step\n",
      "Epoch 100/2000\n",
      "1/1 - 0s - loss: 0.5809 - val_loss: 0.5329 - 342ms/epoch - 342ms/step\n",
      "Epoch 101/2000\n",
      "1/1 - 0s - loss: 0.5788 - val_loss: 0.5335 - 353ms/epoch - 353ms/step\n",
      "Epoch 102/2000\n",
      "1/1 - 0s - loss: 0.5780 - val_loss: 0.5337 - 334ms/epoch - 334ms/step\n",
      "Epoch 103/2000\n",
      "1/1 - 0s - loss: 0.5779 - val_loss: 0.5317 - 336ms/epoch - 336ms/step\n",
      "Epoch 104/2000\n",
      "1/1 - 0s - loss: 0.5774 - val_loss: 0.5314 - 337ms/epoch - 337ms/step\n",
      "Epoch 105/2000\n",
      "1/1 - 0s - loss: 0.5765 - val_loss: 0.5313 - 348ms/epoch - 348ms/step\n",
      "Epoch 106/2000\n",
      "1/1 - 0s - loss: 0.5753 - val_loss: 0.5301 - 340ms/epoch - 340ms/step\n",
      "Epoch 107/2000\n",
      "1/1 - 0s - loss: 0.5745 - val_loss: 0.5317 - 338ms/epoch - 338ms/step\n",
      "Epoch 108/2000\n",
      "1/1 - 0s - loss: 0.5735 - val_loss: 0.5292 - 339ms/epoch - 339ms/step\n",
      "Epoch 109/2000\n",
      "1/1 - 0s - loss: 0.5732 - val_loss: 0.5279 - 341ms/epoch - 341ms/step\n",
      "Epoch 110/2000\n",
      "1/1 - 0s - loss: 0.5717 - val_loss: 0.5274 - 346ms/epoch - 346ms/step\n",
      "Epoch 111/2000\n",
      "1/1 - 0s - loss: 0.5717 - val_loss: 0.5287 - 333ms/epoch - 333ms/step\n",
      "Epoch 112/2000\n",
      "1/1 - 0s - loss: 0.5699 - val_loss: 0.5263 - 339ms/epoch - 339ms/step\n",
      "Epoch 113/2000\n",
      "1/1 - 0s - loss: 0.5700 - val_loss: 0.5270 - 335ms/epoch - 335ms/step\n",
      "Epoch 114/2000\n",
      "1/1 - 0s - loss: 0.5698 - val_loss: 0.5258 - 336ms/epoch - 336ms/step\n",
      "Epoch 115/2000\n",
      "1/1 - 0s - loss: 0.5688 - val_loss: 0.5249 - 350ms/epoch - 350ms/step\n",
      "Epoch 116/2000\n",
      "1/1 - 0s - loss: 0.5679 - val_loss: 0.5249 - 339ms/epoch - 339ms/step\n",
      "Epoch 117/2000\n",
      "1/1 - 0s - loss: 0.5672 - val_loss: 0.5226 - 334ms/epoch - 334ms/step\n",
      "Epoch 118/2000\n",
      "1/1 - 0s - loss: 0.5663 - val_loss: 0.5230 - 356ms/epoch - 356ms/step\n",
      "Epoch 119/2000\n",
      "1/1 - 0s - loss: 0.5667 - val_loss: 0.5216 - 334ms/epoch - 334ms/step\n",
      "Epoch 120/2000\n",
      "1/1 - 0s - loss: 0.5659 - val_loss: 0.5216 - 334ms/epoch - 334ms/step\n",
      "Epoch 121/2000\n",
      "1/1 - 0s - loss: 0.5639 - val_loss: 0.5205 - 337ms/epoch - 337ms/step\n",
      "Epoch 122/2000\n",
      "1/1 - 0s - loss: 0.5647 - val_loss: 0.5196 - 337ms/epoch - 337ms/step\n",
      "Epoch 123/2000\n",
      "1/1 - 0s - loss: 0.5630 - val_loss: 0.5214 - 343ms/epoch - 343ms/step\n",
      "Epoch 124/2000\n",
      "1/1 - 0s - loss: 0.5616 - val_loss: 0.5190 - 332ms/epoch - 332ms/step\n",
      "Epoch 125/2000\n",
      "1/1 - 0s - loss: 0.5619 - val_loss: 0.5188 - 330ms/epoch - 330ms/step\n",
      "Epoch 126/2000\n",
      "1/1 - 0s - loss: 0.5619 - val_loss: 0.5172 - 335ms/epoch - 335ms/step\n",
      "Epoch 127/2000\n",
      "1/1 - 0s - loss: 0.5611 - val_loss: 0.5148 - 338ms/epoch - 338ms/step\n",
      "Epoch 128/2000\n",
      "1/1 - 0s - loss: 0.5582 - val_loss: 0.5161 - 347ms/epoch - 347ms/step\n",
      "Epoch 129/2000\n",
      "1/1 - 0s - loss: 0.5592 - val_loss: 0.5157 - 345ms/epoch - 345ms/step\n",
      "Epoch 130/2000\n",
      "1/1 - 0s - loss: 0.5577 - val_loss: 0.5143 - 336ms/epoch - 336ms/step\n",
      "Epoch 131/2000\n",
      "1/1 - 0s - loss: 0.5571 - val_loss: 0.5148 - 342ms/epoch - 342ms/step\n",
      "Epoch 132/2000\n",
      "1/1 - 0s - loss: 0.5567 - val_loss: 0.5140 - 341ms/epoch - 341ms/step\n",
      "Epoch 133/2000\n",
      "1/1 - 0s - loss: 0.5566 - val_loss: 0.5133 - 340ms/epoch - 340ms/step\n",
      "Epoch 134/2000\n",
      "1/1 - 0s - loss: 0.5559 - val_loss: 0.5124 - 335ms/epoch - 335ms/step\n",
      "Epoch 135/2000\n",
      "1/1 - 0s - loss: 0.5547 - val_loss: 0.5103 - 351ms/epoch - 351ms/step\n",
      "Epoch 136/2000\n",
      "1/1 - 0s - loss: 0.5554 - val_loss: 0.5103 - 339ms/epoch - 339ms/step\n",
      "Epoch 137/2000\n",
      "1/1 - 0s - loss: 0.5536 - val_loss: 0.5116 - 348ms/epoch - 348ms/step\n",
      "Epoch 138/2000\n",
      "1/1 - 0s - loss: 0.5532 - val_loss: 0.5129 - 335ms/epoch - 335ms/step\n",
      "Epoch 139/2000\n",
      "1/1 - 0s - loss: 0.5535 - val_loss: 0.5099 - 339ms/epoch - 339ms/step\n",
      "Epoch 140/2000\n",
      "1/1 - 0s - loss: 0.5519 - val_loss: 0.5102 - 345ms/epoch - 345ms/step\n",
      "Epoch 141/2000\n",
      "1/1 - 0s - loss: 0.5520 - val_loss: 0.5100 - 339ms/epoch - 339ms/step\n",
      "Epoch 142/2000\n",
      "1/1 - 0s - loss: 0.5512 - val_loss: 0.5106 - 341ms/epoch - 341ms/step\n",
      "Epoch 143/2000\n",
      "1/1 - 0s - loss: 0.5507 - val_loss: 0.5075 - 330ms/epoch - 330ms/step\n",
      "Epoch 144/2000\n",
      "1/1 - 0s - loss: 0.5505 - val_loss: 0.5085 - 332ms/epoch - 332ms/step\n",
      "Epoch 145/2000\n",
      "1/1 - 0s - loss: 0.5493 - val_loss: 0.5073 - 335ms/epoch - 335ms/step\n",
      "Epoch 146/2000\n",
      "1/1 - 0s - loss: 0.5496 - val_loss: 0.5086 - 332ms/epoch - 332ms/step\n",
      "Epoch 147/2000\n",
      "1/1 - 0s - loss: 0.5484 - val_loss: 0.5047 - 332ms/epoch - 332ms/step\n",
      "Epoch 148/2000\n",
      "1/1 - 0s - loss: 0.5478 - val_loss: 0.5070 - 334ms/epoch - 334ms/step\n",
      "Epoch 149/2000\n",
      "1/1 - 0s - loss: 0.5479 - val_loss: 0.5042 - 333ms/epoch - 333ms/step\n",
      "Epoch 150/2000\n",
      "1/1 - 0s - loss: 0.5464 - val_loss: 0.5044 - 356ms/epoch - 356ms/step\n",
      "Epoch 151/2000\n",
      "1/1 - 0s - loss: 0.5455 - val_loss: 0.5032 - 339ms/epoch - 339ms/step\n",
      "Epoch 152/2000\n",
      "1/1 - 0s - loss: 0.5450 - val_loss: 0.5045 - 336ms/epoch - 336ms/step\n",
      "Epoch 153/2000\n",
      "1/1 - 0s - loss: 0.5443 - val_loss: 0.5027 - 338ms/epoch - 338ms/step\n",
      "Epoch 154/2000\n",
      "1/1 - 0s - loss: 0.5439 - val_loss: 0.5040 - 349ms/epoch - 349ms/step\n",
      "Epoch 155/2000\n",
      "1/1 - 0s - loss: 0.5438 - val_loss: 0.5037 - 339ms/epoch - 339ms/step\n",
      "Epoch 156/2000\n",
      "1/1 - 0s - loss: 0.5437 - val_loss: 0.5023 - 333ms/epoch - 333ms/step\n",
      "Epoch 157/2000\n",
      "1/1 - 0s - loss: 0.5432 - val_loss: 0.5023 - 336ms/epoch - 336ms/step\n",
      "Epoch 158/2000\n",
      "1/1 - 0s - loss: 0.5419 - val_loss: 0.4993 - 338ms/epoch - 338ms/step\n",
      "Epoch 159/2000\n",
      "1/1 - 0s - loss: 0.5406 - val_loss: 0.5015 - 350ms/epoch - 350ms/step\n",
      "Epoch 160/2000\n",
      "1/1 - 0s - loss: 0.5401 - val_loss: 0.5015 - 345ms/epoch - 345ms/step\n",
      "Epoch 161/2000\n",
      "1/1 - 0s - loss: 0.5409 - val_loss: 0.5005 - 335ms/epoch - 335ms/step\n",
      "Epoch 162/2000\n",
      "1/1 - 0s - loss: 0.5400 - val_loss: 0.5021 - 343ms/epoch - 343ms/step\n",
      "Epoch 163/2000\n",
      "1/1 - 0s - loss: 0.5398 - val_loss: 0.4968 - 344ms/epoch - 344ms/step\n",
      "Epoch 164/2000\n",
      "1/1 - 0s - loss: 0.5385 - val_loss: 0.4968 - 338ms/epoch - 338ms/step\n",
      "Epoch 165/2000\n",
      "1/1 - 0s - loss: 0.5386 - val_loss: 0.4962 - 365ms/epoch - 365ms/step\n",
      "Epoch 166/2000\n",
      "1/1 - 0s - loss: 0.5378 - val_loss: 0.4971 - 334ms/epoch - 334ms/step\n",
      "Epoch 167/2000\n",
      "1/1 - 0s - loss: 0.5371 - val_loss: 0.4979 - 345ms/epoch - 345ms/step\n",
      "Epoch 168/2000\n",
      "1/1 - 0s - loss: 0.5372 - val_loss: 0.4945 - 330ms/epoch - 330ms/step\n",
      "Epoch 169/2000\n",
      "1/1 - 0s - loss: 0.5365 - val_loss: 0.4952 - 345ms/epoch - 345ms/step\n",
      "Epoch 170/2000\n",
      "1/1 - 0s - loss: 0.5365 - val_loss: 0.4953 - 336ms/epoch - 336ms/step\n",
      "Epoch 171/2000\n",
      "1/1 - 0s - loss: 0.5353 - val_loss: 0.4950 - 333ms/epoch - 333ms/step\n",
      "Epoch 172/2000\n",
      "1/1 - 0s - loss: 0.5343 - val_loss: 0.4947 - 335ms/epoch - 335ms/step\n",
      "Epoch 173/2000\n",
      "1/1 - 0s - loss: 0.5348 - val_loss: 0.4940 - 330ms/epoch - 330ms/step\n",
      "Epoch 174/2000\n",
      "1/1 - 0s - loss: 0.5344 - val_loss: 0.4941 - 335ms/epoch - 335ms/step\n",
      "Epoch 175/2000\n",
      "1/1 - 0s - loss: 0.5342 - val_loss: 0.4940 - 354ms/epoch - 354ms/step\n",
      "Epoch 176/2000\n",
      "1/1 - 0s - loss: 0.5340 - val_loss: 0.4927 - 340ms/epoch - 340ms/step\n",
      "Epoch 177/2000\n",
      "1/1 - 0s - loss: 0.5325 - val_loss: 0.4924 - 337ms/epoch - 337ms/step\n",
      "Epoch 178/2000\n",
      "1/1 - 0s - loss: 0.5320 - val_loss: 0.4901 - 333ms/epoch - 333ms/step\n",
      "Epoch 179/2000\n",
      "1/1 - 0s - loss: 0.5313 - val_loss: 0.4933 - 328ms/epoch - 328ms/step\n",
      "Epoch 180/2000\n",
      "1/1 - 0s - loss: 0.5311 - val_loss: 0.4938 - 347ms/epoch - 347ms/step\n",
      "Epoch 181/2000\n",
      "1/1 - 0s - loss: 0.5310 - val_loss: 0.4918 - 335ms/epoch - 335ms/step\n",
      "Epoch 182/2000\n",
      "1/1 - 0s - loss: 0.5298 - val_loss: 0.4938 - 350ms/epoch - 350ms/step\n",
      "Epoch 183/2000\n",
      "1/1 - 0s - loss: 0.5290 - val_loss: 0.4913 - 346ms/epoch - 346ms/step\n",
      "Epoch 184/2000\n",
      "1/1 - 0s - loss: 0.5292 - val_loss: 0.4896 - 334ms/epoch - 334ms/step\n",
      "Epoch 185/2000\n",
      "1/1 - 0s - loss: 0.5282 - val_loss: 0.4898 - 331ms/epoch - 331ms/step\n",
      "Epoch 186/2000\n",
      "1/1 - 0s - loss: 0.5285 - val_loss: 0.4864 - 337ms/epoch - 337ms/step\n",
      "Epoch 187/2000\n",
      "1/1 - 0s - loss: 0.5278 - val_loss: 0.4882 - 333ms/epoch - 333ms/step\n",
      "Epoch 188/2000\n",
      "1/1 - 0s - loss: 0.5263 - val_loss: 0.4873 - 337ms/epoch - 337ms/step\n",
      "Epoch 189/2000\n",
      "1/1 - 0s - loss: 0.5263 - val_loss: 0.4870 - 337ms/epoch - 337ms/step\n",
      "Epoch 190/2000\n",
      "1/1 - 0s - loss: 0.5248 - val_loss: 0.4886 - 334ms/epoch - 334ms/step\n",
      "Epoch 191/2000\n",
      "1/1 - 0s - loss: 0.5257 - val_loss: 0.4868 - 332ms/epoch - 332ms/step\n",
      "Epoch 192/2000\n",
      "1/1 - 0s - loss: 0.5247 - val_loss: 0.4862 - 336ms/epoch - 336ms/step\n",
      "Epoch 193/2000\n",
      "1/1 - 0s - loss: 0.5254 - val_loss: 0.4839 - 342ms/epoch - 342ms/step\n",
      "Epoch 194/2000\n",
      "1/1 - 0s - loss: 0.5239 - val_loss: 0.4860 - 341ms/epoch - 341ms/step\n",
      "Epoch 195/2000\n",
      "1/1 - 0s - loss: 0.5242 - val_loss: 0.4885 - 341ms/epoch - 341ms/step\n",
      "Epoch 196/2000\n",
      "1/1 - 0s - loss: 0.5226 - val_loss: 0.4848 - 343ms/epoch - 343ms/step\n",
      "Epoch 197/2000\n",
      "1/1 - 0s - loss: 0.5231 - val_loss: 0.4831 - 332ms/epoch - 332ms/step\n",
      "Epoch 198/2000\n",
      "1/1 - 0s - loss: 0.5219 - val_loss: 0.4871 - 334ms/epoch - 334ms/step\n",
      "Epoch 199/2000\n",
      "1/1 - 0s - loss: 0.5222 - val_loss: 0.4838 - 337ms/epoch - 337ms/step\n",
      "Epoch 200/2000\n",
      "1/1 - 0s - loss: 0.5225 - val_loss: 0.4824 - 355ms/epoch - 355ms/step\n",
      "Epoch 201/2000\n",
      "1/1 - 0s - loss: 0.5214 - val_loss: 0.4857 - 335ms/epoch - 335ms/step\n",
      "Epoch 202/2000\n",
      "1/1 - 0s - loss: 0.5206 - val_loss: 0.4843 - 342ms/epoch - 342ms/step\n",
      "Epoch 203/2000\n",
      "1/1 - 0s - loss: 0.5209 - val_loss: 0.4815 - 338ms/epoch - 338ms/step\n",
      "Epoch 204/2000\n",
      "1/1 - 0s - loss: 0.5199 - val_loss: 0.4814 - 341ms/epoch - 341ms/step\n",
      "Epoch 205/2000\n",
      "1/1 - 0s - loss: 0.5195 - val_loss: 0.4814 - 337ms/epoch - 337ms/step\n",
      "Epoch 206/2000\n",
      "1/1 - 0s - loss: 0.5202 - val_loss: 0.4836 - 363ms/epoch - 363ms/step\n",
      "Epoch 207/2000\n",
      "1/1 - 0s - loss: 0.5180 - val_loss: 0.4769 - 342ms/epoch - 342ms/step\n",
      "Epoch 208/2000\n",
      "1/1 - 0s - loss: 0.5197 - val_loss: 0.4792 - 337ms/epoch - 337ms/step\n",
      "Epoch 209/2000\n",
      "1/1 - 0s - loss: 0.5170 - val_loss: 0.4810 - 346ms/epoch - 346ms/step\n",
      "Epoch 210/2000\n",
      "1/1 - 0s - loss: 0.5182 - val_loss: 0.4804 - 341ms/epoch - 341ms/step\n",
      "Epoch 211/2000\n",
      "1/1 - 0s - loss: 0.5174 - val_loss: 0.4814 - 343ms/epoch - 343ms/step\n",
      "Epoch 212/2000\n",
      "1/1 - 0s - loss: 0.5172 - val_loss: 0.4769 - 338ms/epoch - 338ms/step\n",
      "Epoch 213/2000\n",
      "1/1 - 0s - loss: 0.5171 - val_loss: 0.4808 - 338ms/epoch - 338ms/step\n",
      "Epoch 214/2000\n",
      "1/1 - 0s - loss: 0.5161 - val_loss: 0.4767 - 346ms/epoch - 346ms/step\n",
      "Epoch 215/2000\n",
      "1/1 - 0s - loss: 0.5157 - val_loss: 0.4777 - 349ms/epoch - 349ms/step\n",
      "Epoch 216/2000\n",
      "1/1 - 0s - loss: 0.5154 - val_loss: 0.4756 - 341ms/epoch - 341ms/step\n",
      "Epoch 217/2000\n",
      "1/1 - 0s - loss: 0.5144 - val_loss: 0.4773 - 334ms/epoch - 334ms/step\n",
      "Epoch 218/2000\n",
      "1/1 - 0s - loss: 0.5158 - val_loss: 0.4758 - 336ms/epoch - 336ms/step\n",
      "Epoch 219/2000\n",
      "1/1 - 0s - loss: 0.5136 - val_loss: 0.4757 - 337ms/epoch - 337ms/step\n",
      "Epoch 220/2000\n",
      "1/1 - 0s - loss: 0.5142 - val_loss: 0.4766 - 345ms/epoch - 345ms/step\n",
      "Epoch 221/2000\n",
      "1/1 - 0s - loss: 0.5131 - val_loss: 0.4782 - 354ms/epoch - 354ms/step\n",
      "Epoch 222/2000\n",
      "1/1 - 0s - loss: 0.5132 - val_loss: 0.4750 - 359ms/epoch - 359ms/step\n",
      "Epoch 223/2000\n",
      "1/1 - 0s - loss: 0.5125 - val_loss: 0.4777 - 341ms/epoch - 341ms/step\n",
      "Epoch 224/2000\n",
      "1/1 - 0s - loss: 0.5122 - val_loss: 0.4683 - 342ms/epoch - 342ms/step\n",
      "Epoch 225/2000\n",
      "1/1 - 0s - loss: 0.5117 - val_loss: 0.4772 - 343ms/epoch - 343ms/step\n",
      "Epoch 226/2000\n",
      "1/1 - 0s - loss: 0.5116 - val_loss: 0.4757 - 351ms/epoch - 351ms/step\n",
      "Epoch 227/2000\n",
      "1/1 - 0s - loss: 0.5104 - val_loss: 0.4764 - 348ms/epoch - 348ms/step\n",
      "Epoch 228/2000\n",
      "1/1 - 0s - loss: 0.5111 - val_loss: 0.4761 - 338ms/epoch - 338ms/step\n",
      "Epoch 229/2000\n",
      "1/1 - 0s - loss: 0.5115 - val_loss: 0.4745 - 340ms/epoch - 340ms/step\n",
      "Epoch 230/2000\n",
      "1/1 - 0s - loss: 0.5095 - val_loss: 0.4720 - 346ms/epoch - 346ms/step\n",
      "Epoch 231/2000\n",
      "1/1 - 0s - loss: 0.5090 - val_loss: 0.4717 - 331ms/epoch - 331ms/step\n",
      "Epoch 232/2000\n",
      "1/1 - 0s - loss: 0.5095 - val_loss: 0.4722 - 343ms/epoch - 343ms/step\n",
      "Epoch 233/2000\n",
      "1/1 - 0s - loss: 0.5099 - val_loss: 0.4718 - 348ms/epoch - 348ms/step\n",
      "Epoch 234/2000\n",
      "1/1 - 0s - loss: 0.5081 - val_loss: 0.4723 - 346ms/epoch - 346ms/step\n",
      "Epoch 235/2000\n",
      "1/1 - 0s - loss: 0.5079 - val_loss: 0.4692 - 338ms/epoch - 338ms/step\n",
      "Epoch 236/2000\n",
      "1/1 - 0s - loss: 0.5075 - val_loss: 0.4700 - 348ms/epoch - 348ms/step\n",
      "Epoch 237/2000\n",
      "1/1 - 0s - loss: 0.5072 - val_loss: 0.4690 - 347ms/epoch - 347ms/step\n",
      "Epoch 238/2000\n",
      "1/1 - 0s - loss: 0.5060 - val_loss: 0.4692 - 346ms/epoch - 346ms/step\n",
      "Epoch 239/2000\n",
      "1/1 - 0s - loss: 0.5059 - val_loss: 0.4671 - 339ms/epoch - 339ms/step\n",
      "Epoch 240/2000\n",
      "1/1 - 0s - loss: 0.5062 - val_loss: 0.4671 - 337ms/epoch - 337ms/step\n",
      "Epoch 241/2000\n",
      "1/1 - 0s - loss: 0.5048 - val_loss: 0.4658 - 351ms/epoch - 351ms/step\n",
      "Epoch 242/2000\n",
      "1/1 - 0s - loss: 0.5060 - val_loss: 0.4684 - 337ms/epoch - 337ms/step\n",
      "Epoch 243/2000\n",
      "1/1 - 0s - loss: 0.5053 - val_loss: 0.4672 - 331ms/epoch - 331ms/step\n",
      "Epoch 244/2000\n",
      "1/1 - 0s - loss: 0.5046 - val_loss: 0.4682 - 361ms/epoch - 361ms/step\n",
      "Epoch 245/2000\n",
      "1/1 - 0s - loss: 0.5042 - val_loss: 0.4657 - 336ms/epoch - 336ms/step\n",
      "Epoch 246/2000\n",
      "1/1 - 0s - loss: 0.5042 - val_loss: 0.4656 - 333ms/epoch - 333ms/step\n",
      "Epoch 247/2000\n",
      "1/1 - 0s - loss: 0.5039 - val_loss: 0.4663 - 329ms/epoch - 329ms/step\n",
      "Epoch 248/2000\n",
      "1/1 - 0s - loss: 0.5037 - val_loss: 0.4612 - 332ms/epoch - 332ms/step\n",
      "Epoch 249/2000\n",
      "1/1 - 0s - loss: 0.5032 - val_loss: 0.4649 - 333ms/epoch - 333ms/step\n",
      "Epoch 250/2000\n",
      "1/1 - 0s - loss: 0.5031 - val_loss: 0.4653 - 335ms/epoch - 335ms/step\n",
      "Epoch 251/2000\n",
      "1/1 - 0s - loss: 0.5023 - val_loss: 0.4647 - 334ms/epoch - 334ms/step\n",
      "Epoch 252/2000\n",
      "1/1 - 0s - loss: 0.5024 - val_loss: 0.4621 - 334ms/epoch - 334ms/step\n",
      "Epoch 253/2000\n",
      "1/1 - 0s - loss: 0.5021 - val_loss: 0.4629 - 346ms/epoch - 346ms/step\n",
      "Epoch 254/2000\n",
      "1/1 - 0s - loss: 0.5012 - val_loss: 0.4622 - 348ms/epoch - 348ms/step\n",
      "Epoch 255/2000\n",
      "1/1 - 0s - loss: 0.5003 - val_loss: 0.4624 - 337ms/epoch - 337ms/step\n",
      "Epoch 256/2000\n",
      "1/1 - 0s - loss: 0.5001 - val_loss: 0.4609 - 335ms/epoch - 335ms/step\n",
      "Epoch 257/2000\n",
      "1/1 - 0s - loss: 0.5005 - val_loss: 0.4628 - 334ms/epoch - 334ms/step\n",
      "Epoch 258/2000\n",
      "1/1 - 0s - loss: 0.4989 - val_loss: 0.4612 - 340ms/epoch - 340ms/step\n",
      "Epoch 259/2000\n",
      "1/1 - 0s - loss: 0.4988 - val_loss: 0.4584 - 348ms/epoch - 348ms/step\n",
      "Epoch 260/2000\n",
      "1/1 - 0s - loss: 0.4988 - val_loss: 0.4575 - 356ms/epoch - 356ms/step\n",
      "Epoch 261/2000\n",
      "1/1 - 0s - loss: 0.4977 - val_loss: 0.4584 - 355ms/epoch - 355ms/step\n",
      "Epoch 262/2000\n",
      "1/1 - 0s - loss: 0.4980 - val_loss: 0.4575 - 340ms/epoch - 340ms/step\n",
      "Epoch 263/2000\n",
      "1/1 - 0s - loss: 0.4970 - val_loss: 0.4584 - 343ms/epoch - 343ms/step\n",
      "Epoch 264/2000\n",
      "1/1 - 0s - loss: 0.4964 - val_loss: 0.4578 - 342ms/epoch - 342ms/step\n",
      "Epoch 265/2000\n",
      "1/1 - 0s - loss: 0.4961 - val_loss: 0.4598 - 336ms/epoch - 336ms/step\n",
      "Epoch 266/2000\n",
      "1/1 - 0s - loss: 0.4973 - val_loss: 0.4573 - 331ms/epoch - 331ms/step\n",
      "Epoch 267/2000\n",
      "1/1 - 0s - loss: 0.4966 - val_loss: 0.4572 - 337ms/epoch - 337ms/step\n",
      "Epoch 268/2000\n",
      "1/1 - 0s - loss: 0.4955 - val_loss: 0.4558 - 351ms/epoch - 351ms/step\n",
      "Epoch 269/2000\n",
      "1/1 - 0s - loss: 0.4944 - val_loss: 0.4565 - 335ms/epoch - 335ms/step\n",
      "Epoch 270/2000\n",
      "1/1 - 0s - loss: 0.4946 - val_loss: 0.4570 - 337ms/epoch - 337ms/step\n",
      "Epoch 271/2000\n",
      "1/1 - 0s - loss: 0.4944 - val_loss: 0.4547 - 344ms/epoch - 344ms/step\n",
      "Epoch 272/2000\n",
      "1/1 - 0s - loss: 0.4940 - val_loss: 0.4554 - 348ms/epoch - 348ms/step\n",
      "Epoch 273/2000\n",
      "1/1 - 0s - loss: 0.4939 - val_loss: 0.4552 - 332ms/epoch - 332ms/step\n",
      "Epoch 274/2000\n",
      "1/1 - 0s - loss: 0.4929 - val_loss: 0.4533 - 352ms/epoch - 352ms/step\n",
      "Epoch 275/2000\n",
      "1/1 - 0s - loss: 0.4923 - val_loss: 0.4514 - 338ms/epoch - 338ms/step\n",
      "Epoch 276/2000\n",
      "1/1 - 0s - loss: 0.4920 - val_loss: 0.4522 - 335ms/epoch - 335ms/step\n",
      "Epoch 277/2000\n",
      "1/1 - 0s - loss: 0.4913 - val_loss: 0.4482 - 339ms/epoch - 339ms/step\n",
      "Epoch 278/2000\n",
      "1/1 - 0s - loss: 0.4916 - val_loss: 0.4520 - 356ms/epoch - 356ms/step\n",
      "Epoch 279/2000\n",
      "1/1 - 0s - loss: 0.4918 - val_loss: 0.4501 - 332ms/epoch - 332ms/step\n",
      "Epoch 280/2000\n",
      "1/1 - 0s - loss: 0.4901 - val_loss: 0.4509 - 330ms/epoch - 330ms/step\n",
      "Epoch 281/2000\n",
      "1/1 - 0s - loss: 0.4897 - val_loss: 0.4500 - 336ms/epoch - 336ms/step\n",
      "Epoch 282/2000\n",
      "1/1 - 0s - loss: 0.4898 - val_loss: 0.4465 - 333ms/epoch - 333ms/step\n",
      "Epoch 283/2000\n",
      "1/1 - 0s - loss: 0.4877 - val_loss: 0.4487 - 336ms/epoch - 336ms/step\n",
      "Epoch 284/2000\n",
      "1/1 - 0s - loss: 0.4871 - val_loss: 0.4472 - 346ms/epoch - 346ms/step\n",
      "Epoch 285/2000\n",
      "1/1 - 0s - loss: 0.4880 - val_loss: 0.4461 - 352ms/epoch - 352ms/step\n",
      "Epoch 286/2000\n",
      "1/1 - 0s - loss: 0.4881 - val_loss: 0.4456 - 343ms/epoch - 343ms/step\n",
      "Epoch 287/2000\n",
      "1/1 - 0s - loss: 0.4873 - val_loss: 0.4476 - 333ms/epoch - 333ms/step\n",
      "Epoch 288/2000\n",
      "1/1 - 0s - loss: 0.4868 - val_loss: 0.4445 - 333ms/epoch - 333ms/step\n",
      "Epoch 289/2000\n",
      "1/1 - 0s - loss: 0.4860 - val_loss: 0.4451 - 345ms/epoch - 345ms/step\n",
      "Epoch 290/2000\n",
      "1/1 - 0s - loss: 0.4871 - val_loss: 0.4430 - 335ms/epoch - 335ms/step\n",
      "Epoch 291/2000\n",
      "1/1 - 0s - loss: 0.4855 - val_loss: 0.4434 - 333ms/epoch - 333ms/step\n",
      "Epoch 292/2000\n",
      "1/1 - 0s - loss: 0.4850 - val_loss: 0.4402 - 334ms/epoch - 334ms/step\n",
      "Epoch 293/2000\n",
      "1/1 - 0s - loss: 0.4846 - val_loss: 0.4435 - 342ms/epoch - 342ms/step\n",
      "Epoch 294/2000\n",
      "1/1 - 0s - loss: 0.4831 - val_loss: 0.4385 - 345ms/epoch - 345ms/step\n",
      "Epoch 295/2000\n",
      "1/1 - 0s - loss: 0.4839 - val_loss: 0.4412 - 334ms/epoch - 334ms/step\n",
      "Epoch 296/2000\n",
      "1/1 - 0s - loss: 0.4829 - val_loss: 0.4382 - 335ms/epoch - 335ms/step\n",
      "Epoch 297/2000\n",
      "1/1 - 0s - loss: 0.4834 - val_loss: 0.4385 - 352ms/epoch - 352ms/step\n",
      "Epoch 298/2000\n",
      "1/1 - 0s - loss: 0.4827 - val_loss: 0.4375 - 340ms/epoch - 340ms/step\n",
      "Epoch 299/2000\n",
      "1/1 - 0s - loss: 0.4821 - val_loss: 0.4378 - 350ms/epoch - 350ms/step\n",
      "Epoch 300/2000\n",
      "1/1 - 0s - loss: 0.4809 - val_loss: 0.4391 - 346ms/epoch - 346ms/step\n",
      "Epoch 301/2000\n",
      "1/1 - 0s - loss: 0.4818 - val_loss: 0.4375 - 346ms/epoch - 346ms/step\n",
      "Epoch 302/2000\n",
      "1/1 - 0s - loss: 0.4806 - val_loss: 0.4392 - 336ms/epoch - 336ms/step\n",
      "Epoch 303/2000\n",
      "1/1 - 0s - loss: 0.4804 - val_loss: 0.4351 - 337ms/epoch - 337ms/step\n",
      "Epoch 304/2000\n",
      "1/1 - 0s - loss: 0.4801 - val_loss: 0.4326 - 339ms/epoch - 339ms/step\n",
      "Epoch 305/2000\n",
      "1/1 - 0s - loss: 0.4800 - val_loss: 0.4336 - 333ms/epoch - 333ms/step\n",
      "Epoch 306/2000\n",
      "1/1 - 0s - loss: 0.4796 - val_loss: 0.4355 - 345ms/epoch - 345ms/step\n",
      "Epoch 307/2000\n",
      "1/1 - 0s - loss: 0.4782 - val_loss: 0.4349 - 337ms/epoch - 337ms/step\n",
      "Epoch 308/2000\n",
      "1/1 - 0s - loss: 0.4783 - val_loss: 0.4311 - 334ms/epoch - 334ms/step\n",
      "Epoch 309/2000\n",
      "1/1 - 0s - loss: 0.4776 - val_loss: 0.4328 - 331ms/epoch - 331ms/step\n",
      "Epoch 310/2000\n",
      "1/1 - 0s - loss: 0.4757 - val_loss: 0.4306 - 338ms/epoch - 338ms/step\n",
      "Epoch 311/2000\n",
      "1/1 - 0s - loss: 0.4759 - val_loss: 0.4290 - 337ms/epoch - 337ms/step\n",
      "Epoch 312/2000\n",
      "1/1 - 0s - loss: 0.4761 - val_loss: 0.4278 - 335ms/epoch - 335ms/step\n",
      "Epoch 313/2000\n",
      "1/1 - 0s - loss: 0.4757 - val_loss: 0.4258 - 340ms/epoch - 340ms/step\n",
      "Epoch 314/2000\n",
      "1/1 - 0s - loss: 0.4747 - val_loss: 0.4277 - 342ms/epoch - 342ms/step\n",
      "Epoch 315/2000\n",
      "1/1 - 0s - loss: 0.4734 - val_loss: 0.4284 - 331ms/epoch - 331ms/step\n",
      "Epoch 316/2000\n",
      "1/1 - 0s - loss: 0.4751 - val_loss: 0.4279 - 345ms/epoch - 345ms/step\n",
      "Epoch 317/2000\n",
      "1/1 - 0s - loss: 0.4742 - val_loss: 0.4272 - 348ms/epoch - 348ms/step\n",
      "Epoch 318/2000\n",
      "1/1 - 0s - loss: 0.4727 - val_loss: 0.4256 - 339ms/epoch - 339ms/step\n",
      "Epoch 319/2000\n",
      "1/1 - 0s - loss: 0.4726 - val_loss: 0.4260 - 349ms/epoch - 349ms/step\n",
      "Epoch 320/2000\n",
      "1/1 - 0s - loss: 0.4723 - val_loss: 0.4220 - 341ms/epoch - 341ms/step\n",
      "Epoch 321/2000\n",
      "1/1 - 0s - loss: 0.4713 - val_loss: 0.4214 - 341ms/epoch - 341ms/step\n",
      "Epoch 322/2000\n",
      "1/1 - 0s - loss: 0.4705 - val_loss: 0.4230 - 338ms/epoch - 338ms/step\n",
      "Epoch 323/2000\n",
      "1/1 - 0s - loss: 0.4708 - val_loss: 0.4221 - 339ms/epoch - 339ms/step\n",
      "Epoch 324/2000\n",
      "1/1 - 0s - loss: 0.4702 - val_loss: 0.4193 - 332ms/epoch - 332ms/step\n",
      "Epoch 325/2000\n",
      "1/1 - 0s - loss: 0.4693 - val_loss: 0.4170 - 335ms/epoch - 335ms/step\n",
      "Epoch 326/2000\n",
      "1/1 - 0s - loss: 0.4702 - val_loss: 0.4194 - 337ms/epoch - 337ms/step\n",
      "Epoch 327/2000\n",
      "1/1 - 0s - loss: 0.4685 - val_loss: 0.4198 - 340ms/epoch - 340ms/step\n",
      "Epoch 328/2000\n",
      "1/1 - 0s - loss: 0.4692 - val_loss: 0.4184 - 357ms/epoch - 357ms/step\n",
      "Epoch 329/2000\n",
      "1/1 - 0s - loss: 0.4688 - val_loss: 0.4208 - 337ms/epoch - 337ms/step\n",
      "Epoch 330/2000\n",
      "1/1 - 0s - loss: 0.4661 - val_loss: 0.4162 - 344ms/epoch - 344ms/step\n",
      "Epoch 331/2000\n",
      "1/1 - 0s - loss: 0.4674 - val_loss: 0.4151 - 330ms/epoch - 330ms/step\n",
      "Epoch 332/2000\n",
      "1/1 - 0s - loss: 0.4660 - val_loss: 0.4158 - 328ms/epoch - 328ms/step\n",
      "Epoch 333/2000\n",
      "1/1 - 0s - loss: 0.4669 - val_loss: 0.4173 - 343ms/epoch - 343ms/step\n",
      "Epoch 334/2000\n",
      "1/1 - 0s - loss: 0.4658 - val_loss: 0.4138 - 342ms/epoch - 342ms/step\n",
      "Epoch 335/2000\n",
      "1/1 - 0s - loss: 0.4646 - val_loss: 0.4148 - 337ms/epoch - 337ms/step\n",
      "Epoch 336/2000\n",
      "1/1 - 0s - loss: 0.4654 - val_loss: 0.4103 - 347ms/epoch - 347ms/step\n",
      "Epoch 337/2000\n",
      "1/1 - 0s - loss: 0.4640 - val_loss: 0.4124 - 342ms/epoch - 342ms/step\n",
      "Epoch 338/2000\n",
      "1/1 - 0s - loss: 0.4639 - val_loss: 0.4094 - 331ms/epoch - 331ms/step\n",
      "Epoch 339/2000\n",
      "1/1 - 0s - loss: 0.4623 - val_loss: 0.4088 - 339ms/epoch - 339ms/step\n",
      "Epoch 340/2000\n",
      "1/1 - 0s - loss: 0.4633 - val_loss: 0.4114 - 330ms/epoch - 330ms/step\n",
      "Epoch 341/2000\n",
      "1/1 - 0s - loss: 0.4619 - val_loss: 0.4095 - 340ms/epoch - 340ms/step\n",
      "Epoch 342/2000\n",
      "1/1 - 0s - loss: 0.4618 - val_loss: 0.4095 - 345ms/epoch - 345ms/step\n",
      "Epoch 343/2000\n",
      "1/1 - 0s - loss: 0.4609 - val_loss: 0.4108 - 347ms/epoch - 347ms/step\n",
      "Epoch 344/2000\n",
      "1/1 - 0s - loss: 0.4603 - val_loss: 0.4080 - 337ms/epoch - 337ms/step\n",
      "Epoch 345/2000\n",
      "1/1 - 0s - loss: 0.4592 - val_loss: 0.4096 - 359ms/epoch - 359ms/step\n",
      "Epoch 346/2000\n",
      "1/1 - 0s - loss: 0.4599 - val_loss: 0.4056 - 336ms/epoch - 336ms/step\n",
      "Epoch 347/2000\n",
      "1/1 - 0s - loss: 0.4579 - val_loss: 0.4085 - 346ms/epoch - 346ms/step\n",
      "Epoch 348/2000\n",
      "1/1 - 0s - loss: 0.4579 - val_loss: 0.4061 - 331ms/epoch - 331ms/step\n",
      "Epoch 349/2000\n",
      "1/1 - 0s - loss: 0.4595 - val_loss: 0.4043 - 335ms/epoch - 335ms/step\n",
      "Epoch 350/2000\n",
      "1/1 - 0s - loss: 0.4575 - val_loss: 0.4042 - 336ms/epoch - 336ms/step\n",
      "Epoch 351/2000\n",
      "1/1 - 0s - loss: 0.4582 - val_loss: 0.4022 - 334ms/epoch - 334ms/step\n",
      "Epoch 352/2000\n",
      "1/1 - 0s - loss: 0.4556 - val_loss: 0.3997 - 341ms/epoch - 341ms/step\n",
      "Epoch 353/2000\n",
      "1/1 - 0s - loss: 0.4566 - val_loss: 0.4024 - 343ms/epoch - 343ms/step\n",
      "Epoch 354/2000\n",
      "1/1 - 0s - loss: 0.4553 - val_loss: 0.4020 - 357ms/epoch - 357ms/step\n",
      "Epoch 355/2000\n",
      "1/1 - 0s - loss: 0.4551 - val_loss: 0.4019 - 335ms/epoch - 335ms/step\n",
      "Epoch 356/2000\n",
      "1/1 - 0s - loss: 0.4541 - val_loss: 0.3975 - 340ms/epoch - 340ms/step\n",
      "Epoch 357/2000\n",
      "1/1 - 0s - loss: 0.4540 - val_loss: 0.3963 - 339ms/epoch - 339ms/step\n",
      "Epoch 358/2000\n",
      "1/1 - 0s - loss: 0.4533 - val_loss: 0.3968 - 347ms/epoch - 347ms/step\n",
      "Epoch 359/2000\n",
      "1/1 - 0s - loss: 0.4530 - val_loss: 0.3967 - 342ms/epoch - 342ms/step\n",
      "Epoch 360/2000\n",
      "1/1 - 0s - loss: 0.4519 - val_loss: 0.3963 - 338ms/epoch - 338ms/step\n",
      "Epoch 361/2000\n",
      "1/1 - 0s - loss: 0.4517 - val_loss: 0.3932 - 335ms/epoch - 335ms/step\n",
      "Epoch 362/2000\n",
      "1/1 - 0s - loss: 0.4518 - val_loss: 0.3954 - 355ms/epoch - 355ms/step\n",
      "Epoch 363/2000\n",
      "1/1 - 0s - loss: 0.4510 - val_loss: 0.3916 - 341ms/epoch - 341ms/step\n",
      "Epoch 364/2000\n",
      "1/1 - 0s - loss: 0.4506 - val_loss: 0.3883 - 342ms/epoch - 342ms/step\n",
      "Epoch 365/2000\n",
      "1/1 - 0s - loss: 0.4490 - val_loss: 0.3904 - 330ms/epoch - 330ms/step\n",
      "Epoch 366/2000\n",
      "1/1 - 0s - loss: 0.4484 - val_loss: 0.3953 - 334ms/epoch - 334ms/step\n",
      "Epoch 367/2000\n",
      "1/1 - 0s - loss: 0.4501 - val_loss: 0.3920 - 333ms/epoch - 333ms/step\n",
      "Epoch 368/2000\n",
      "1/1 - 0s - loss: 0.4485 - val_loss: 0.3908 - 344ms/epoch - 344ms/step\n",
      "Epoch 369/2000\n",
      "1/1 - 0s - loss: 0.4484 - val_loss: 0.3925 - 332ms/epoch - 332ms/step\n",
      "Epoch 370/2000\n",
      "1/1 - 0s - loss: 0.4465 - val_loss: 0.3904 - 339ms/epoch - 339ms/step\n",
      "Epoch 371/2000\n",
      "1/1 - 0s - loss: 0.4464 - val_loss: 0.3880 - 341ms/epoch - 341ms/step\n",
      "Epoch 372/2000\n",
      "1/1 - 0s - loss: 0.4464 - val_loss: 0.3904 - 334ms/epoch - 334ms/step\n",
      "Epoch 373/2000\n",
      "1/1 - 0s - loss: 0.4460 - val_loss: 0.3874 - 348ms/epoch - 348ms/step\n",
      "Epoch 374/2000\n",
      "1/1 - 0s - loss: 0.4460 - val_loss: 0.3916 - 332ms/epoch - 332ms/step\n",
      "Epoch 375/2000\n",
      "1/1 - 0s - loss: 0.4449 - val_loss: 0.3916 - 345ms/epoch - 345ms/step\n",
      "Epoch 376/2000\n",
      "1/1 - 0s - loss: 0.4450 - val_loss: 0.3879 - 339ms/epoch - 339ms/step\n",
      "Epoch 377/2000\n",
      "1/1 - 0s - loss: 0.4437 - val_loss: 0.3872 - 335ms/epoch - 335ms/step\n",
      "Epoch 378/2000\n",
      "1/1 - 0s - loss: 0.4429 - val_loss: 0.3857 - 331ms/epoch - 331ms/step\n",
      "Epoch 379/2000\n",
      "1/1 - 0s - loss: 0.4439 - val_loss: 0.3827 - 337ms/epoch - 337ms/step\n",
      "Epoch 380/2000\n",
      "1/1 - 0s - loss: 0.4418 - val_loss: 0.3850 - 334ms/epoch - 334ms/step\n",
      "Epoch 381/2000\n",
      "1/1 - 0s - loss: 0.4418 - val_loss: 0.3818 - 338ms/epoch - 338ms/step\n",
      "Epoch 382/2000\n",
      "1/1 - 0s - loss: 0.4413 - val_loss: 0.3809 - 343ms/epoch - 343ms/step\n",
      "Epoch 383/2000\n",
      "1/1 - 0s - loss: 0.4418 - val_loss: 0.3814 - 355ms/epoch - 355ms/step\n",
      "Epoch 384/2000\n",
      "1/1 - 0s - loss: 0.4412 - val_loss: 0.3845 - 330ms/epoch - 330ms/step\n",
      "Epoch 385/2000\n",
      "1/1 - 0s - loss: 0.4388 - val_loss: 0.3835 - 333ms/epoch - 333ms/step\n",
      "Epoch 386/2000\n",
      "1/1 - 0s - loss: 0.4393 - val_loss: 0.3786 - 338ms/epoch - 338ms/step\n",
      "Epoch 387/2000\n",
      "1/1 - 0s - loss: 0.4391 - val_loss: 0.3806 - 346ms/epoch - 346ms/step\n",
      "Epoch 388/2000\n",
      "1/1 - 0s - loss: 0.4381 - val_loss: 0.3800 - 345ms/epoch - 345ms/step\n",
      "Epoch 389/2000\n",
      "1/1 - 0s - loss: 0.4380 - val_loss: 0.3758 - 353ms/epoch - 353ms/step\n",
      "Epoch 390/2000\n",
      "1/1 - 0s - loss: 0.4375 - val_loss: 0.3786 - 352ms/epoch - 352ms/step\n",
      "Epoch 391/2000\n",
      "1/1 - 0s - loss: 0.4393 - val_loss: 0.3787 - 339ms/epoch - 339ms/step\n",
      "Epoch 392/2000\n",
      "1/1 - 0s - loss: 0.4369 - val_loss: 0.3766 - 339ms/epoch - 339ms/step\n",
      "Epoch 393/2000\n",
      "1/1 - 0s - loss: 0.4356 - val_loss: 0.3765 - 340ms/epoch - 340ms/step\n",
      "Epoch 394/2000\n",
      "1/1 - 0s - loss: 0.4365 - val_loss: 0.3779 - 337ms/epoch - 337ms/step\n",
      "Epoch 395/2000\n",
      "1/1 - 0s - loss: 0.4345 - val_loss: 0.3746 - 334ms/epoch - 334ms/step\n",
      "Epoch 396/2000\n",
      "1/1 - 0s - loss: 0.4349 - val_loss: 0.3752 - 342ms/epoch - 342ms/step\n",
      "Epoch 397/2000\n",
      "1/1 - 0s - loss: 0.4346 - val_loss: 0.3744 - 331ms/epoch - 331ms/step\n",
      "Epoch 398/2000\n",
      "1/1 - 0s - loss: 0.4359 - val_loss: 0.3745 - 332ms/epoch - 332ms/step\n",
      "Epoch 399/2000\n",
      "1/1 - 0s - loss: 0.4343 - val_loss: 0.3727 - 333ms/epoch - 333ms/step\n",
      "Epoch 400/2000\n",
      "1/1 - 0s - loss: 0.4325 - val_loss: 0.3708 - 332ms/epoch - 332ms/step\n",
      "Epoch 401/2000\n",
      "1/1 - 0s - loss: 0.4324 - val_loss: 0.3735 - 361ms/epoch - 361ms/step\n",
      "Epoch 402/2000\n",
      "1/1 - 0s - loss: 0.4322 - val_loss: 0.3701 - 351ms/epoch - 351ms/step\n",
      "Epoch 403/2000\n",
      "1/1 - 0s - loss: 0.4310 - val_loss: 0.3669 - 334ms/epoch - 334ms/step\n",
      "Epoch 404/2000\n",
      "1/1 - 0s - loss: 0.4314 - val_loss: 0.3687 - 339ms/epoch - 339ms/step\n",
      "Epoch 405/2000\n",
      "1/1 - 0s - loss: 0.4301 - val_loss: 0.3686 - 338ms/epoch - 338ms/step\n",
      "Epoch 406/2000\n",
      "1/1 - 0s - loss: 0.4301 - val_loss: 0.3705 - 347ms/epoch - 347ms/step\n",
      "Epoch 407/2000\n",
      "1/1 - 0s - loss: 0.4296 - val_loss: 0.3684 - 345ms/epoch - 345ms/step\n",
      "Epoch 408/2000\n",
      "1/1 - 0s - loss: 0.4297 - val_loss: 0.3715 - 330ms/epoch - 330ms/step\n",
      "Epoch 409/2000\n",
      "1/1 - 0s - loss: 0.4288 - val_loss: 0.3653 - 337ms/epoch - 337ms/step\n",
      "Epoch 410/2000\n",
      "1/1 - 0s - loss: 0.4277 - val_loss: 0.3691 - 337ms/epoch - 337ms/step\n",
      "Epoch 411/2000\n",
      "1/1 - 0s - loss: 0.4262 - val_loss: 0.3669 - 353ms/epoch - 353ms/step\n",
      "Epoch 412/2000\n",
      "1/1 - 0s - loss: 0.4274 - val_loss: 0.3606 - 346ms/epoch - 346ms/step\n",
      "Epoch 413/2000\n",
      "1/1 - 0s - loss: 0.4267 - val_loss: 0.3646 - 337ms/epoch - 337ms/step\n",
      "Epoch 414/2000\n",
      "1/1 - 0s - loss: 0.4265 - val_loss: 0.3617 - 343ms/epoch - 343ms/step\n",
      "Epoch 415/2000\n",
      "1/1 - 0s - loss: 0.4249 - val_loss: 0.3626 - 337ms/epoch - 337ms/step\n",
      "Epoch 416/2000\n",
      "1/1 - 0s - loss: 0.4247 - val_loss: 0.3639 - 344ms/epoch - 344ms/step\n",
      "Epoch 417/2000\n",
      "1/1 - 0s - loss: 0.4251 - val_loss: 0.3628 - 353ms/epoch - 353ms/step\n",
      "Epoch 418/2000\n",
      "1/1 - 0s - loss: 0.4252 - val_loss: 0.3621 - 334ms/epoch - 334ms/step\n",
      "Epoch 419/2000\n",
      "1/1 - 0s - loss: 0.4248 - val_loss: 0.3605 - 331ms/epoch - 331ms/step\n",
      "Epoch 420/2000\n",
      "1/1 - 0s - loss: 0.4238 - val_loss: 0.3592 - 339ms/epoch - 339ms/step\n",
      "Epoch 421/2000\n",
      "1/1 - 0s - loss: 0.4226 - val_loss: 0.3560 - 335ms/epoch - 335ms/step\n",
      "Epoch 422/2000\n",
      "1/1 - 0s - loss: 0.4221 - val_loss: 0.3596 - 346ms/epoch - 346ms/step\n",
      "Epoch 423/2000\n",
      "1/1 - 0s - loss: 0.4215 - val_loss: 0.3567 - 333ms/epoch - 333ms/step\n",
      "Epoch 424/2000\n",
      "1/1 - 0s - loss: 0.4212 - val_loss: 0.3623 - 331ms/epoch - 331ms/step\n",
      "Epoch 425/2000\n",
      "1/1 - 0s - loss: 0.4208 - val_loss: 0.3601 - 357ms/epoch - 357ms/step\n",
      "Epoch 426/2000\n",
      "1/1 - 0s - loss: 0.4201 - val_loss: 0.3547 - 356ms/epoch - 356ms/step\n",
      "Epoch 427/2000\n",
      "1/1 - 0s - loss: 0.4201 - val_loss: 0.3573 - 376ms/epoch - 376ms/step\n",
      "Epoch 428/2000\n",
      "1/1 - 0s - loss: 0.4201 - val_loss: 0.3543 - 336ms/epoch - 336ms/step\n",
      "Epoch 429/2000\n",
      "1/1 - 0s - loss: 0.4198 - val_loss: 0.3522 - 338ms/epoch - 338ms/step\n",
      "Epoch 430/2000\n",
      "1/1 - 0s - loss: 0.4184 - val_loss: 0.3573 - 335ms/epoch - 335ms/step\n",
      "Epoch 431/2000\n",
      "1/1 - 0s - loss: 0.4183 - val_loss: 0.3520 - 349ms/epoch - 349ms/step\n",
      "Epoch 432/2000\n",
      "1/1 - 0s - loss: 0.4179 - val_loss: 0.3546 - 342ms/epoch - 342ms/step\n",
      "Epoch 433/2000\n",
      "1/1 - 0s - loss: 0.4177 - val_loss: 0.3516 - 333ms/epoch - 333ms/step\n",
      "Epoch 434/2000\n",
      "1/1 - 0s - loss: 0.4168 - val_loss: 0.3523 - 341ms/epoch - 341ms/step\n",
      "Epoch 435/2000\n",
      "1/1 - 0s - loss: 0.4172 - val_loss: 0.3569 - 338ms/epoch - 338ms/step\n",
      "Epoch 436/2000\n",
      "1/1 - 0s - loss: 0.4162 - val_loss: 0.3524 - 338ms/epoch - 338ms/step\n",
      "Epoch 437/2000\n",
      "1/1 - 0s - loss: 0.4159 - val_loss: 0.3508 - 371ms/epoch - 371ms/step\n",
      "Epoch 438/2000\n",
      "1/1 - 0s - loss: 0.4141 - val_loss: 0.3513 - 358ms/epoch - 358ms/step\n",
      "Epoch 439/2000\n",
      "1/1 - 0s - loss: 0.4151 - val_loss: 0.3477 - 333ms/epoch - 333ms/step\n",
      "Epoch 440/2000\n",
      "1/1 - 0s - loss: 0.4136 - val_loss: 0.3498 - 340ms/epoch - 340ms/step\n",
      "Epoch 441/2000\n",
      "1/1 - 0s - loss: 0.4150 - val_loss: 0.3495 - 357ms/epoch - 357ms/step\n",
      "Epoch 442/2000\n",
      "1/1 - 0s - loss: 0.4144 - val_loss: 0.3525 - 350ms/epoch - 350ms/step\n",
      "Epoch 443/2000\n",
      "1/1 - 0s - loss: 0.4130 - val_loss: 0.3492 - 334ms/epoch - 334ms/step\n",
      "Epoch 444/2000\n",
      "1/1 - 0s - loss: 0.4114 - val_loss: 0.3491 - 333ms/epoch - 333ms/step\n",
      "Epoch 445/2000\n",
      "1/1 - 0s - loss: 0.4118 - val_loss: 0.3444 - 340ms/epoch - 340ms/step\n",
      "Epoch 446/2000\n",
      "1/1 - 0s - loss: 0.4109 - val_loss: 0.3436 - 343ms/epoch - 343ms/step\n",
      "Epoch 447/2000\n",
      "1/1 - 0s - loss: 0.4099 - val_loss: 0.3503 - 354ms/epoch - 354ms/step\n",
      "Epoch 448/2000\n",
      "1/1 - 0s - loss: 0.4093 - val_loss: 0.3494 - 340ms/epoch - 340ms/step\n",
      "Epoch 449/2000\n",
      "1/1 - 0s - loss: 0.4109 - val_loss: 0.3462 - 341ms/epoch - 341ms/step\n",
      "Epoch 450/2000\n",
      "1/1 - 0s - loss: 0.4103 - val_loss: 0.3438 - 335ms/epoch - 335ms/step\n",
      "Epoch 451/2000\n",
      "1/1 - 0s - loss: 0.4094 - val_loss: 0.3459 - 350ms/epoch - 350ms/step\n",
      "Epoch 452/2000\n",
      "1/1 - 0s - loss: 0.4085 - val_loss: 0.3415 - 340ms/epoch - 340ms/step\n",
      "Epoch 453/2000\n",
      "1/1 - 0s - loss: 0.4079 - val_loss: 0.3401 - 341ms/epoch - 341ms/step\n",
      "Epoch 454/2000\n",
      "1/1 - 0s - loss: 0.4087 - val_loss: 0.3395 - 336ms/epoch - 336ms/step\n",
      "Epoch 455/2000\n",
      "1/1 - 0s - loss: 0.4081 - val_loss: 0.3420 - 333ms/epoch - 333ms/step\n",
      "Epoch 456/2000\n",
      "1/1 - 0s - loss: 0.4071 - val_loss: 0.3392 - 336ms/epoch - 336ms/step\n",
      "Epoch 457/2000\n",
      "1/1 - 0s - loss: 0.4074 - val_loss: 0.3423 - 341ms/epoch - 341ms/step\n",
      "Epoch 458/2000\n",
      "1/1 - 0s - loss: 0.4070 - val_loss: 0.3390 - 345ms/epoch - 345ms/step\n",
      "Epoch 459/2000\n",
      "1/1 - 0s - loss: 0.4061 - val_loss: 0.3375 - 341ms/epoch - 341ms/step\n",
      "Epoch 460/2000\n",
      "1/1 - 0s - loss: 0.4057 - val_loss: 0.3367 - 341ms/epoch - 341ms/step\n",
      "Epoch 461/2000\n",
      "1/1 - 0s - loss: 0.4051 - val_loss: 0.3339 - 338ms/epoch - 338ms/step\n",
      "Epoch 462/2000\n",
      "1/1 - 0s - loss: 0.4029 - val_loss: 0.3413 - 335ms/epoch - 335ms/step\n",
      "Epoch 463/2000\n",
      "1/1 - 0s - loss: 0.4042 - val_loss: 0.3387 - 351ms/epoch - 351ms/step\n",
      "Epoch 464/2000\n",
      "1/1 - 0s - loss: 0.4037 - val_loss: 0.3376 - 335ms/epoch - 335ms/step\n",
      "Epoch 465/2000\n",
      "1/1 - 0s - loss: 0.4029 - val_loss: 0.3395 - 334ms/epoch - 334ms/step\n",
      "Epoch 466/2000\n",
      "1/1 - 0s - loss: 0.4037 - val_loss: 0.3383 - 339ms/epoch - 339ms/step\n",
      "Epoch 467/2000\n",
      "1/1 - 0s - loss: 0.4019 - val_loss: 0.3346 - 339ms/epoch - 339ms/step\n",
      "Epoch 468/2000\n",
      "1/1 - 0s - loss: 0.4025 - val_loss: 0.3336 - 349ms/epoch - 349ms/step\n",
      "Epoch 469/2000\n",
      "1/1 - 0s - loss: 0.4030 - val_loss: 0.3322 - 339ms/epoch - 339ms/step\n",
      "Epoch 470/2000\n",
      "1/1 - 0s - loss: 0.4006 - val_loss: 0.3337 - 337ms/epoch - 337ms/step\n",
      "Epoch 471/2000\n",
      "1/1 - 0s - loss: 0.3993 - val_loss: 0.3370 - 338ms/epoch - 338ms/step\n",
      "Epoch 472/2000\n",
      "1/1 - 0s - loss: 0.4001 - val_loss: 0.3336 - 337ms/epoch - 337ms/step\n",
      "Epoch 473/2000\n",
      "1/1 - 0s - loss: 0.3987 - val_loss: 0.3317 - 338ms/epoch - 338ms/step\n",
      "Epoch 474/2000\n",
      "1/1 - 0s - loss: 0.3984 - val_loss: 0.3297 - 342ms/epoch - 342ms/step\n",
      "Epoch 475/2000\n",
      "1/1 - 0s - loss: 0.3975 - val_loss: 0.3305 - 340ms/epoch - 340ms/step\n",
      "Epoch 476/2000\n",
      "1/1 - 0s - loss: 0.3971 - val_loss: 0.3289 - 338ms/epoch - 338ms/step\n",
      "Epoch 477/2000\n",
      "1/1 - 0s - loss: 0.3970 - val_loss: 0.3298 - 342ms/epoch - 342ms/step\n",
      "Epoch 478/2000\n",
      "1/1 - 0s - loss: 0.3967 - val_loss: 0.3292 - 335ms/epoch - 335ms/step\n",
      "Epoch 479/2000\n",
      "1/1 - 0s - loss: 0.3956 - val_loss: 0.3302 - 335ms/epoch - 335ms/step\n",
      "Epoch 480/2000\n",
      "1/1 - 0s - loss: 0.3954 - val_loss: 0.3327 - 341ms/epoch - 341ms/step\n",
      "Epoch 481/2000\n",
      "1/1 - 0s - loss: 0.3951 - val_loss: 0.3277 - 333ms/epoch - 333ms/step\n",
      "Epoch 482/2000\n",
      "1/1 - 0s - loss: 0.3947 - val_loss: 0.3281 - 356ms/epoch - 356ms/step\n",
      "Epoch 483/2000\n",
      "1/1 - 0s - loss: 0.3943 - val_loss: 0.3255 - 344ms/epoch - 344ms/step\n",
      "Epoch 484/2000\n",
      "1/1 - 0s - loss: 0.3941 - val_loss: 0.3261 - 344ms/epoch - 344ms/step\n",
      "Epoch 485/2000\n",
      "1/1 - 0s - loss: 0.3939 - val_loss: 0.3231 - 347ms/epoch - 347ms/step\n",
      "Epoch 486/2000\n",
      "1/1 - 0s - loss: 0.3923 - val_loss: 0.3219 - 335ms/epoch - 335ms/step\n",
      "Epoch 487/2000\n",
      "1/1 - 0s - loss: 0.3923 - val_loss: 0.3244 - 345ms/epoch - 345ms/step\n",
      "Epoch 488/2000\n",
      "1/1 - 0s - loss: 0.3921 - val_loss: 0.3247 - 335ms/epoch - 335ms/step\n",
      "Epoch 489/2000\n",
      "1/1 - 0s - loss: 0.3926 - val_loss: 0.3296 - 334ms/epoch - 334ms/step\n",
      "Epoch 490/2000\n",
      "1/1 - 0s - loss: 0.3923 - val_loss: 0.3224 - 339ms/epoch - 339ms/step\n",
      "Epoch 491/2000\n",
      "1/1 - 0s - loss: 0.3904 - val_loss: 0.3204 - 336ms/epoch - 336ms/step\n",
      "Epoch 492/2000\n",
      "1/1 - 0s - loss: 0.3910 - val_loss: 0.3210 - 334ms/epoch - 334ms/step\n",
      "Epoch 493/2000\n",
      "1/1 - 0s - loss: 0.3901 - val_loss: 0.3201 - 334ms/epoch - 334ms/step\n",
      "Epoch 494/2000\n",
      "1/1 - 0s - loss: 0.3902 - val_loss: 0.3190 - 333ms/epoch - 333ms/step\n",
      "Epoch 495/2000\n",
      "1/1 - 0s - loss: 0.3892 - val_loss: 0.3196 - 334ms/epoch - 334ms/step\n",
      "Epoch 496/2000\n",
      "1/1 - 0s - loss: 0.3889 - val_loss: 0.3177 - 333ms/epoch - 333ms/step\n",
      "Epoch 497/2000\n",
      "1/1 - 0s - loss: 0.3889 - val_loss: 0.3253 - 344ms/epoch - 344ms/step\n",
      "Epoch 498/2000\n",
      "1/1 - 0s - loss: 0.3870 - val_loss: 0.3192 - 338ms/epoch - 338ms/step\n",
      "Epoch 499/2000\n",
      "1/1 - 0s - loss: 0.3894 - val_loss: 0.3212 - 335ms/epoch - 335ms/step\n",
      "Epoch 500/2000\n",
      "1/1 - 0s - loss: 0.3859 - val_loss: 0.3215 - 335ms/epoch - 335ms/step\n",
      "Epoch 501/2000\n",
      "1/1 - 0s - loss: 0.3873 - val_loss: 0.3228 - 337ms/epoch - 337ms/step\n",
      "Epoch 502/2000\n",
      "1/1 - 0s - loss: 0.3876 - val_loss: 0.3208 - 336ms/epoch - 336ms/step\n",
      "Epoch 503/2000\n",
      "1/1 - 0s - loss: 0.3865 - val_loss: 0.3182 - 336ms/epoch - 336ms/step\n",
      "Epoch 504/2000\n",
      "1/1 - 0s - loss: 0.3859 - val_loss: 0.3151 - 337ms/epoch - 337ms/step\n",
      "Epoch 505/2000\n",
      "1/1 - 0s - loss: 0.3854 - val_loss: 0.3144 - 347ms/epoch - 347ms/step\n",
      "Epoch 506/2000\n",
      "1/1 - 0s - loss: 0.3842 - val_loss: 0.3153 - 333ms/epoch - 333ms/step\n",
      "Epoch 507/2000\n",
      "1/1 - 0s - loss: 0.3850 - val_loss: 0.3154 - 333ms/epoch - 333ms/step\n",
      "Epoch 508/2000\n",
      "1/1 - 0s - loss: 0.3835 - val_loss: 0.3173 - 337ms/epoch - 337ms/step\n",
      "Epoch 509/2000\n",
      "1/1 - 0s - loss: 0.3843 - val_loss: 0.3161 - 339ms/epoch - 339ms/step\n",
      "Epoch 510/2000\n",
      "1/1 - 0s - loss: 0.3825 - val_loss: 0.3175 - 336ms/epoch - 336ms/step\n",
      "Epoch 511/2000\n",
      "1/1 - 0s - loss: 0.3835 - val_loss: 0.3133 - 335ms/epoch - 335ms/step\n",
      "Epoch 512/2000\n",
      "1/1 - 0s - loss: 0.3815 - val_loss: 0.3134 - 344ms/epoch - 344ms/step\n",
      "Epoch 513/2000\n",
      "1/1 - 0s - loss: 0.3826 - val_loss: 0.3157 - 340ms/epoch - 340ms/step\n",
      "Epoch 514/2000\n",
      "1/1 - 0s - loss: 0.3802 - val_loss: 0.3159 - 333ms/epoch - 333ms/step\n",
      "Epoch 515/2000\n",
      "1/1 - 0s - loss: 0.3799 - val_loss: 0.3155 - 336ms/epoch - 336ms/step\n",
      "Epoch 516/2000\n",
      "1/1 - 0s - loss: 0.3814 - val_loss: 0.3139 - 336ms/epoch - 336ms/step\n",
      "Epoch 517/2000\n",
      "1/1 - 0s - loss: 0.3799 - val_loss: 0.3110 - 336ms/epoch - 336ms/step\n",
      "Epoch 518/2000\n",
      "1/1 - 0s - loss: 0.3797 - val_loss: 0.3111 - 331ms/epoch - 331ms/step\n",
      "Epoch 519/2000\n",
      "1/1 - 0s - loss: 0.3794 - val_loss: 0.3131 - 337ms/epoch - 337ms/step\n",
      "Epoch 520/2000\n",
      "1/1 - 0s - loss: 0.3794 - val_loss: 0.3124 - 333ms/epoch - 333ms/step\n",
      "Epoch 521/2000\n",
      "1/1 - 0s - loss: 0.3787 - val_loss: 0.3124 - 334ms/epoch - 334ms/step\n",
      "Epoch 522/2000\n",
      "1/1 - 0s - loss: 0.3780 - val_loss: 0.3106 - 344ms/epoch - 344ms/step\n",
      "Epoch 523/2000\n",
      "1/1 - 0s - loss: 0.3769 - val_loss: 0.3103 - 334ms/epoch - 334ms/step\n",
      "Epoch 524/2000\n",
      "1/1 - 0s - loss: 0.3773 - val_loss: 0.3070 - 333ms/epoch - 333ms/step\n",
      "Epoch 525/2000\n",
      "1/1 - 0s - loss: 0.3778 - val_loss: 0.3115 - 332ms/epoch - 332ms/step\n",
      "Epoch 526/2000\n",
      "1/1 - 0s - loss: 0.3765 - val_loss: 0.3088 - 338ms/epoch - 338ms/step\n",
      "Epoch 527/2000\n",
      "1/1 - 0s - loss: 0.3773 - val_loss: 0.3060 - 332ms/epoch - 332ms/step\n",
      "Epoch 528/2000\n",
      "1/1 - 0s - loss: 0.3751 - val_loss: 0.3068 - 339ms/epoch - 339ms/step\n",
      "Epoch 529/2000\n",
      "1/1 - 0s - loss: 0.3759 - val_loss: 0.3074 - 340ms/epoch - 340ms/step\n",
      "Epoch 530/2000\n",
      "1/1 - 0s - loss: 0.3749 - val_loss: 0.3022 - 351ms/epoch - 351ms/step\n",
      "Epoch 531/2000\n",
      "1/1 - 0s - loss: 0.3753 - val_loss: 0.3081 - 332ms/epoch - 332ms/step\n",
      "Epoch 532/2000\n",
      "1/1 - 0s - loss: 0.3756 - val_loss: 0.3075 - 335ms/epoch - 335ms/step\n",
      "Epoch 533/2000\n",
      "1/1 - 0s - loss: 0.3745 - val_loss: 0.3033 - 340ms/epoch - 340ms/step\n",
      "Epoch 534/2000\n",
      "1/1 - 0s - loss: 0.3734 - val_loss: 0.3047 - 336ms/epoch - 336ms/step\n",
      "Epoch 535/2000\n",
      "1/1 - 0s - loss: 0.3740 - val_loss: 0.3032 - 346ms/epoch - 346ms/step\n",
      "Epoch 536/2000\n",
      "1/1 - 0s - loss: 0.3727 - val_loss: 0.3063 - 336ms/epoch - 336ms/step\n",
      "Epoch 537/2000\n",
      "1/1 - 0s - loss: 0.3718 - val_loss: 0.3007 - 335ms/epoch - 335ms/step\n",
      "Epoch 538/2000\n",
      "1/1 - 0s - loss: 0.3716 - val_loss: 0.3060 - 333ms/epoch - 333ms/step\n",
      "Epoch 539/2000\n",
      "1/1 - 0s - loss: 0.3718 - val_loss: 0.3061 - 335ms/epoch - 335ms/step\n",
      "Epoch 540/2000\n",
      "1/1 - 0s - loss: 0.3704 - val_loss: 0.3019 - 336ms/epoch - 336ms/step\n",
      "Epoch 541/2000\n",
      "1/1 - 0s - loss: 0.3707 - val_loss: 0.3017 - 335ms/epoch - 335ms/step\n",
      "Epoch 542/2000\n",
      "1/1 - 0s - loss: 0.3703 - val_loss: 0.3029 - 336ms/epoch - 336ms/step\n",
      "Epoch 543/2000\n",
      "1/1 - 0s - loss: 0.3687 - val_loss: 0.2959 - 344ms/epoch - 344ms/step\n",
      "Epoch 544/2000\n",
      "1/1 - 0s - loss: 0.3684 - val_loss: 0.3006 - 340ms/epoch - 340ms/step\n",
      "Epoch 545/2000\n",
      "1/1 - 0s - loss: 0.3707 - val_loss: 0.3031 - 340ms/epoch - 340ms/step\n",
      "Epoch 546/2000\n",
      "1/1 - 0s - loss: 0.3674 - val_loss: 0.2998 - 337ms/epoch - 337ms/step\n",
      "Epoch 547/2000\n",
      "1/1 - 0s - loss: 0.3693 - val_loss: 0.2983 - 338ms/epoch - 338ms/step\n",
      "Epoch 548/2000\n",
      "1/1 - 0s - loss: 0.3668 - val_loss: 0.3016 - 334ms/epoch - 334ms/step\n",
      "Epoch 549/2000\n",
      "1/1 - 0s - loss: 0.3690 - val_loss: 0.2999 - 339ms/epoch - 339ms/step\n",
      "Epoch 550/2000\n",
      "1/1 - 0s - loss: 0.3674 - val_loss: 0.2951 - 339ms/epoch - 339ms/step\n",
      "Epoch 551/2000\n",
      "1/1 - 0s - loss: 0.3675 - val_loss: 0.3019 - 339ms/epoch - 339ms/step\n",
      "Epoch 552/2000\n",
      "1/1 - 0s - loss: 0.3650 - val_loss: 0.3033 - 337ms/epoch - 337ms/step\n",
      "Epoch 553/2000\n",
      "1/1 - 0s - loss: 0.3661 - val_loss: 0.2973 - 336ms/epoch - 336ms/step\n",
      "Epoch 554/2000\n",
      "1/1 - 0s - loss: 0.3655 - val_loss: 0.2942 - 336ms/epoch - 336ms/step\n",
      "Epoch 555/2000\n",
      "1/1 - 0s - loss: 0.3643 - val_loss: 0.2980 - 333ms/epoch - 333ms/step\n",
      "Epoch 556/2000\n",
      "1/1 - 0s - loss: 0.3640 - val_loss: 0.2989 - 334ms/epoch - 334ms/step\n",
      "Epoch 557/2000\n",
      "1/1 - 0s - loss: 0.3634 - val_loss: 0.2945 - 338ms/epoch - 338ms/step\n",
      "Epoch 558/2000\n",
      "1/1 - 0s - loss: 0.3662 - val_loss: 0.2980 - 331ms/epoch - 331ms/step\n",
      "Epoch 559/2000\n",
      "1/1 - 0s - loss: 0.3637 - val_loss: 0.2947 - 332ms/epoch - 332ms/step\n",
      "Epoch 560/2000\n",
      "1/1 - 0s - loss: 0.3632 - val_loss: 0.2970 - 349ms/epoch - 349ms/step\n",
      "Epoch 561/2000\n",
      "1/1 - 0s - loss: 0.3628 - val_loss: 0.2967 - 344ms/epoch - 344ms/step\n",
      "Epoch 562/2000\n",
      "1/1 - 0s - loss: 0.3618 - val_loss: 0.2962 - 333ms/epoch - 333ms/step\n",
      "Epoch 563/2000\n",
      "1/1 - 0s - loss: 0.3628 - val_loss: 0.2940 - 334ms/epoch - 334ms/step\n",
      "Epoch 564/2000\n",
      "1/1 - 0s - loss: 0.3627 - val_loss: 0.2931 - 338ms/epoch - 338ms/step\n",
      "Epoch 565/2000\n",
      "1/1 - 0s - loss: 0.3618 - val_loss: 0.3000 - 339ms/epoch - 339ms/step\n",
      "Epoch 566/2000\n",
      "1/1 - 0s - loss: 0.3623 - val_loss: 0.2976 - 333ms/epoch - 333ms/step\n",
      "Epoch 567/2000\n",
      "1/1 - 0s - loss: 0.3617 - val_loss: 0.2932 - 336ms/epoch - 336ms/step\n",
      "Epoch 568/2000\n",
      "1/1 - 0s - loss: 0.3612 - val_loss: 0.2943 - 343ms/epoch - 343ms/step\n",
      "Epoch 569/2000\n",
      "1/1 - 0s - loss: 0.3607 - val_loss: 0.2894 - 334ms/epoch - 334ms/step\n",
      "Epoch 570/2000\n",
      "1/1 - 0s - loss: 0.3611 - val_loss: 0.2883 - 337ms/epoch - 337ms/step\n",
      "Epoch 571/2000\n",
      "1/1 - 0s - loss: 0.3609 - val_loss: 0.2934 - 339ms/epoch - 339ms/step\n",
      "Epoch 572/2000\n",
      "1/1 - 0s - loss: 0.3605 - val_loss: 0.2945 - 345ms/epoch - 345ms/step\n",
      "Epoch 573/2000\n",
      "1/1 - 0s - loss: 0.3606 - val_loss: 0.2920 - 334ms/epoch - 334ms/step\n",
      "Epoch 574/2000\n",
      "1/1 - 0s - loss: 0.3584 - val_loss: 0.2939 - 335ms/epoch - 335ms/step\n",
      "Epoch 575/2000\n",
      "1/1 - 0s - loss: 0.3592 - val_loss: 0.2923 - 334ms/epoch - 334ms/step\n",
      "Epoch 576/2000\n",
      "1/1 - 0s - loss: 0.3585 - val_loss: 0.2904 - 331ms/epoch - 331ms/step\n",
      "Epoch 577/2000\n",
      "1/1 - 0s - loss: 0.3564 - val_loss: 0.2933 - 334ms/epoch - 334ms/step\n",
      "Epoch 578/2000\n",
      "1/1 - 0s - loss: 0.3588 - val_loss: 0.2926 - 336ms/epoch - 336ms/step\n",
      "Epoch 579/2000\n",
      "1/1 - 0s - loss: 0.3572 - val_loss: 0.2897 - 335ms/epoch - 335ms/step\n",
      "Epoch 580/2000\n",
      "1/1 - 0s - loss: 0.3566 - val_loss: 0.2921 - 334ms/epoch - 334ms/step\n",
      "Epoch 581/2000\n",
      "1/1 - 0s - loss: 0.3568 - val_loss: 0.2879 - 338ms/epoch - 338ms/step\n",
      "Epoch 582/2000\n",
      "1/1 - 0s - loss: 0.3561 - val_loss: 0.2875 - 333ms/epoch - 333ms/step\n",
      "Epoch 583/2000\n",
      "1/1 - 0s - loss: 0.3571 - val_loss: 0.2901 - 334ms/epoch - 334ms/step\n",
      "Epoch 584/2000\n",
      "1/1 - 0s - loss: 0.3551 - val_loss: 0.2885 - 335ms/epoch - 335ms/step\n",
      "Epoch 585/2000\n",
      "1/1 - 0s - loss: 0.3561 - val_loss: 0.2902 - 338ms/epoch - 338ms/step\n",
      "Epoch 586/2000\n",
      "1/1 - 0s - loss: 0.3536 - val_loss: 0.2898 - 334ms/epoch - 334ms/step\n",
      "Epoch 587/2000\n",
      "1/1 - 0s - loss: 0.3544 - val_loss: 0.2883 - 334ms/epoch - 334ms/step\n",
      "Epoch 588/2000\n",
      "1/1 - 0s - loss: 0.3544 - val_loss: 0.2878 - 344ms/epoch - 344ms/step\n",
      "Epoch 589/2000\n",
      "1/1 - 0s - loss: 0.3554 - val_loss: 0.2894 - 337ms/epoch - 337ms/step\n",
      "Epoch 590/2000\n",
      "1/1 - 0s - loss: 0.3552 - val_loss: 0.2879 - 334ms/epoch - 334ms/step\n",
      "Epoch 591/2000\n",
      "1/1 - 0s - loss: 0.3541 - val_loss: 0.2858 - 343ms/epoch - 343ms/step\n",
      "Epoch 592/2000\n",
      "1/1 - 0s - loss: 0.3539 - val_loss: 0.2854 - 340ms/epoch - 340ms/step\n",
      "Epoch 593/2000\n",
      "1/1 - 0s - loss: 0.3529 - val_loss: 0.2836 - 335ms/epoch - 335ms/step\n",
      "Epoch 594/2000\n",
      "1/1 - 0s - loss: 0.3530 - val_loss: 0.2839 - 341ms/epoch - 341ms/step\n",
      "Epoch 595/2000\n",
      "1/1 - 0s - loss: 0.3540 - val_loss: 0.2840 - 338ms/epoch - 338ms/step\n",
      "Epoch 596/2000\n",
      "1/1 - 0s - loss: 0.3521 - val_loss: 0.2845 - 333ms/epoch - 333ms/step\n",
      "Epoch 597/2000\n",
      "1/1 - 0s - loss: 0.3525 - val_loss: 0.2884 - 332ms/epoch - 332ms/step\n",
      "Epoch 598/2000\n",
      "1/1 - 0s - loss: 0.3513 - val_loss: 0.2835 - 334ms/epoch - 334ms/step\n",
      "Epoch 599/2000\n",
      "1/1 - 0s - loss: 0.3496 - val_loss: 0.2827 - 337ms/epoch - 337ms/step\n",
      "Epoch 600/2000\n",
      "1/1 - 0s - loss: 0.3500 - val_loss: 0.2847 - 335ms/epoch - 335ms/step\n",
      "Epoch 601/2000\n",
      "1/1 - 0s - loss: 0.3514 - val_loss: 0.2802 - 335ms/epoch - 335ms/step\n",
      "Epoch 602/2000\n",
      "1/1 - 0s - loss: 0.3493 - val_loss: 0.2825 - 344ms/epoch - 344ms/step\n",
      "Epoch 603/2000\n",
      "1/1 - 0s - loss: 0.3492 - val_loss: 0.2793 - 335ms/epoch - 335ms/step\n",
      "Epoch 604/2000\n",
      "1/1 - 0s - loss: 0.3494 - val_loss: 0.2826 - 334ms/epoch - 334ms/step\n",
      "Epoch 605/2000\n",
      "1/1 - 0s - loss: 0.3497 - val_loss: 0.2831 - 335ms/epoch - 335ms/step\n",
      "Epoch 606/2000\n",
      "1/1 - 0s - loss: 0.3498 - val_loss: 0.2828 - 343ms/epoch - 343ms/step\n",
      "Epoch 607/2000\n",
      "1/1 - 0s - loss: 0.3501 - val_loss: 0.2779 - 333ms/epoch - 333ms/step\n",
      "Epoch 608/2000\n",
      "1/1 - 0s - loss: 0.3498 - val_loss: 0.2769 - 337ms/epoch - 337ms/step\n",
      "Epoch 609/2000\n",
      "1/1 - 0s - loss: 0.3480 - val_loss: 0.2828 - 338ms/epoch - 338ms/step\n",
      "Epoch 610/2000\n",
      "1/1 - 0s - loss: 0.3473 - val_loss: 0.2805 - 339ms/epoch - 339ms/step\n",
      "Epoch 611/2000\n",
      "1/1 - 0s - loss: 0.3471 - val_loss: 0.2790 - 334ms/epoch - 334ms/step\n",
      "Epoch 612/2000\n",
      "1/1 - 0s - loss: 0.3477 - val_loss: 0.2772 - 334ms/epoch - 334ms/step\n",
      "Epoch 613/2000\n",
      "1/1 - 0s - loss: 0.3447 - val_loss: 0.2774 - 336ms/epoch - 336ms/step\n",
      "Epoch 614/2000\n",
      "1/1 - 0s - loss: 0.3467 - val_loss: 0.2801 - 334ms/epoch - 334ms/step\n",
      "Epoch 615/2000\n",
      "1/1 - 0s - loss: 0.3471 - val_loss: 0.2780 - 336ms/epoch - 336ms/step\n",
      "Epoch 616/2000\n",
      "1/1 - 0s - loss: 0.3454 - val_loss: 0.2799 - 350ms/epoch - 350ms/step\n",
      "Epoch 617/2000\n",
      "1/1 - 0s - loss: 0.3461 - val_loss: 0.2801 - 335ms/epoch - 335ms/step\n",
      "Epoch 618/2000\n",
      "1/1 - 0s - loss: 0.3452 - val_loss: 0.2745 - 333ms/epoch - 333ms/step\n",
      "Epoch 619/2000\n",
      "1/1 - 0s - loss: 0.3453 - val_loss: 0.2774 - 335ms/epoch - 335ms/step\n",
      "Epoch 620/2000\n",
      "1/1 - 0s - loss: 0.3460 - val_loss: 0.2754 - 338ms/epoch - 338ms/step\n",
      "Epoch 621/2000\n",
      "1/1 - 0s - loss: 0.3453 - val_loss: 0.2783 - 336ms/epoch - 336ms/step\n",
      "Epoch 622/2000\n",
      "1/1 - 0s - loss: 0.3441 - val_loss: 0.2768 - 337ms/epoch - 337ms/step\n",
      "Epoch 623/2000\n",
      "1/1 - 0s - loss: 0.3441 - val_loss: 0.2792 - 340ms/epoch - 340ms/step\n",
      "Epoch 624/2000\n",
      "1/1 - 0s - loss: 0.3453 - val_loss: 0.2800 - 335ms/epoch - 335ms/step\n",
      "Epoch 625/2000\n",
      "1/1 - 0s - loss: 0.3432 - val_loss: 0.2768 - 332ms/epoch - 332ms/step\n",
      "Epoch 626/2000\n",
      "1/1 - 0s - loss: 0.3430 - val_loss: 0.2758 - 347ms/epoch - 347ms/step\n",
      "Epoch 627/2000\n",
      "1/1 - 0s - loss: 0.3428 - val_loss: 0.2777 - 334ms/epoch - 334ms/step\n",
      "Epoch 628/2000\n",
      "1/1 - 0s - loss: 0.3431 - val_loss: 0.2766 - 335ms/epoch - 335ms/step\n",
      "Epoch 629/2000\n",
      "1/1 - 0s - loss: 0.3429 - val_loss: 0.2734 - 336ms/epoch - 336ms/step\n",
      "Epoch 630/2000\n",
      "1/1 - 0s - loss: 0.3412 - val_loss: 0.2732 - 339ms/epoch - 339ms/step\n",
      "Epoch 631/2000\n",
      "1/1 - 0s - loss: 0.3410 - val_loss: 0.2708 - 343ms/epoch - 343ms/step\n",
      "Epoch 632/2000\n",
      "1/1 - 0s - loss: 0.3420 - val_loss: 0.2768 - 334ms/epoch - 334ms/step\n",
      "Epoch 633/2000\n",
      "1/1 - 0s - loss: 0.3412 - val_loss: 0.2750 - 340ms/epoch - 340ms/step\n",
      "Epoch 634/2000\n",
      "1/1 - 0s - loss: 0.3406 - val_loss: 0.2734 - 337ms/epoch - 337ms/step\n",
      "Epoch 635/2000\n",
      "1/1 - 0s - loss: 0.3415 - val_loss: 0.2728 - 332ms/epoch - 332ms/step\n",
      "Epoch 636/2000\n",
      "1/1 - 0s - loss: 0.3407 - val_loss: 0.2707 - 342ms/epoch - 342ms/step\n",
      "Epoch 637/2000\n",
      "1/1 - 0s - loss: 0.3404 - val_loss: 0.2722 - 338ms/epoch - 338ms/step\n",
      "Epoch 638/2000\n",
      "1/1 - 0s - loss: 0.3400 - val_loss: 0.2733 - 344ms/epoch - 344ms/step\n",
      "Epoch 639/2000\n",
      "1/1 - 0s - loss: 0.3393 - val_loss: 0.2714 - 334ms/epoch - 334ms/step\n",
      "Epoch 640/2000\n",
      "1/1 - 0s - loss: 0.3389 - val_loss: 0.2667 - 337ms/epoch - 337ms/step\n",
      "Epoch 641/2000\n",
      "1/1 - 0s - loss: 0.3391 - val_loss: 0.2709 - 337ms/epoch - 337ms/step\n",
      "Epoch 642/2000\n",
      "1/1 - 0s - loss: 0.3394 - val_loss: 0.2718 - 336ms/epoch - 336ms/step\n",
      "Epoch 643/2000\n",
      "1/1 - 0s - loss: 0.3380 - val_loss: 0.2690 - 335ms/epoch - 335ms/step\n",
      "Epoch 644/2000\n",
      "1/1 - 0s - loss: 0.3380 - val_loss: 0.2723 - 338ms/epoch - 338ms/step\n",
      "Epoch 645/2000\n",
      "1/1 - 0s - loss: 0.3386 - val_loss: 0.2672 - 347ms/epoch - 347ms/step\n",
      "Epoch 646/2000\n",
      "1/1 - 0s - loss: 0.3373 - val_loss: 0.2653 - 334ms/epoch - 334ms/step\n",
      "Epoch 647/2000\n",
      "1/1 - 0s - loss: 0.3379 - val_loss: 0.2738 - 337ms/epoch - 337ms/step\n",
      "Epoch 648/2000\n",
      "1/1 - 0s - loss: 0.3374 - val_loss: 0.2685 - 335ms/epoch - 335ms/step\n",
      "Epoch 649/2000\n",
      "1/1 - 0s - loss: 0.3362 - val_loss: 0.2663 - 333ms/epoch - 333ms/step\n",
      "Epoch 650/2000\n",
      "1/1 - 0s - loss: 0.3377 - val_loss: 0.2680 - 333ms/epoch - 333ms/step\n",
      "Epoch 651/2000\n",
      "1/1 - 0s - loss: 0.3351 - val_loss: 0.2662 - 351ms/epoch - 351ms/step\n",
      "Epoch 652/2000\n",
      "1/1 - 0s - loss: 0.3370 - val_loss: 0.2700 - 340ms/epoch - 340ms/step\n",
      "Epoch 653/2000\n",
      "1/1 - 0s - loss: 0.3358 - val_loss: 0.2722 - 335ms/epoch - 335ms/step\n",
      "Epoch 654/2000\n",
      "1/1 - 0s - loss: 0.3374 - val_loss: 0.2673 - 337ms/epoch - 337ms/step\n",
      "Epoch 655/2000\n",
      "1/1 - 0s - loss: 0.3355 - val_loss: 0.2681 - 338ms/epoch - 338ms/step\n",
      "Epoch 656/2000\n",
      "1/1 - 0s - loss: 0.3348 - val_loss: 0.2651 - 333ms/epoch - 333ms/step\n",
      "Epoch 657/2000\n",
      "1/1 - 0s - loss: 0.3364 - val_loss: 0.2679 - 334ms/epoch - 334ms/step\n",
      "Epoch 658/2000\n",
      "1/1 - 0s - loss: 0.3344 - val_loss: 0.2686 - 341ms/epoch - 341ms/step\n",
      "Epoch 659/2000\n",
      "1/1 - 0s - loss: 0.3331 - val_loss: 0.2675 - 350ms/epoch - 350ms/step\n",
      "Epoch 660/2000\n",
      "1/1 - 0s - loss: 0.3340 - val_loss: 0.2638 - 333ms/epoch - 333ms/step\n",
      "Epoch 661/2000\n",
      "1/1 - 0s - loss: 0.3345 - val_loss: 0.2670 - 334ms/epoch - 334ms/step\n",
      "Epoch 662/2000\n",
      "1/1 - 0s - loss: 0.3321 - val_loss: 0.2618 - 335ms/epoch - 335ms/step\n",
      "Epoch 663/2000\n",
      "1/1 - 0s - loss: 0.3325 - val_loss: 0.2690 - 335ms/epoch - 335ms/step\n",
      "Epoch 664/2000\n",
      "1/1 - 0s - loss: 0.3331 - val_loss: 0.2685 - 334ms/epoch - 334ms/step\n",
      "Epoch 665/2000\n",
      "1/1 - 0s - loss: 0.3334 - val_loss: 0.2654 - 339ms/epoch - 339ms/step\n",
      "Epoch 666/2000\n",
      "1/1 - 0s - loss: 0.3316 - val_loss: 0.2669 - 334ms/epoch - 334ms/step\n",
      "Epoch 667/2000\n",
      "1/1 - 0s - loss: 0.3318 - val_loss: 0.2647 - 339ms/epoch - 339ms/step\n",
      "Epoch 668/2000\n",
      "1/1 - 0s - loss: 0.3313 - val_loss: 0.2670 - 341ms/epoch - 341ms/step\n",
      "Epoch 669/2000\n",
      "1/1 - 0s - loss: 0.3337 - val_loss: 0.2650 - 335ms/epoch - 335ms/step\n",
      "Epoch 670/2000\n",
      "1/1 - 0s - loss: 0.3312 - val_loss: 0.2651 - 339ms/epoch - 339ms/step\n",
      "Epoch 671/2000\n",
      "1/1 - 0s - loss: 0.3323 - val_loss: 0.2656 - 335ms/epoch - 335ms/step\n",
      "Epoch 672/2000\n",
      "1/1 - 0s - loss: 0.3310 - val_loss: 0.2621 - 336ms/epoch - 336ms/step\n",
      "Epoch 673/2000\n",
      "1/1 - 0s - loss: 0.3307 - val_loss: 0.2622 - 334ms/epoch - 334ms/step\n",
      "Epoch 674/2000\n",
      "1/1 - 0s - loss: 0.3304 - val_loss: 0.2631 - 334ms/epoch - 334ms/step\n",
      "Epoch 675/2000\n",
      "1/1 - 0s - loss: 0.3296 - val_loss: 0.2623 - 339ms/epoch - 339ms/step\n",
      "Epoch 676/2000\n",
      "1/1 - 0s - loss: 0.3296 - val_loss: 0.2620 - 355ms/epoch - 355ms/step\n",
      "Epoch 677/2000\n",
      "1/1 - 0s - loss: 0.3307 - val_loss: 0.2583 - 335ms/epoch - 335ms/step\n",
      "Epoch 678/2000\n",
      "1/1 - 0s - loss: 0.3285 - val_loss: 0.2613 - 337ms/epoch - 337ms/step\n",
      "Epoch 679/2000\n",
      "1/1 - 0s - loss: 0.3294 - val_loss: 0.2626 - 337ms/epoch - 337ms/step\n",
      "Epoch 680/2000\n",
      "1/1 - 0s - loss: 0.3286 - val_loss: 0.2652 - 335ms/epoch - 335ms/step\n",
      "Epoch 681/2000\n",
      "1/1 - 0s - loss: 0.3299 - val_loss: 0.2613 - 346ms/epoch - 346ms/step\n",
      "Epoch 682/2000\n",
      "1/1 - 0s - loss: 0.3286 - val_loss: 0.2588 - 352ms/epoch - 352ms/step\n",
      "Epoch 683/2000\n",
      "1/1 - 0s - loss: 0.3288 - val_loss: 0.2600 - 333ms/epoch - 333ms/step\n",
      "Epoch 684/2000\n",
      "1/1 - 0s - loss: 0.3278 - val_loss: 0.2595 - 333ms/epoch - 333ms/step\n",
      "Epoch 685/2000\n",
      "1/1 - 0s - loss: 0.3271 - val_loss: 0.2603 - 335ms/epoch - 335ms/step\n",
      "Epoch 686/2000\n",
      "1/1 - 0s - loss: 0.3283 - val_loss: 0.2616 - 345ms/epoch - 345ms/step\n",
      "Epoch 687/2000\n",
      "1/1 - 0s - loss: 0.3296 - val_loss: 0.2580 - 345ms/epoch - 345ms/step\n",
      "Epoch 688/2000\n",
      "1/1 - 0s - loss: 0.3273 - val_loss: 0.2607 - 338ms/epoch - 338ms/step\n",
      "Epoch 689/2000\n",
      "1/1 - 0s - loss: 0.3265 - val_loss: 0.2550 - 336ms/epoch - 336ms/step\n",
      "Epoch 690/2000\n",
      "1/1 - 0s - loss: 0.3267 - val_loss: 0.2571 - 335ms/epoch - 335ms/step\n",
      "Epoch 691/2000\n",
      "1/1 - 0s - loss: 0.3259 - val_loss: 0.2601 - 337ms/epoch - 337ms/step\n",
      "Epoch 692/2000\n",
      "1/1 - 0s - loss: 0.3261 - val_loss: 0.2592 - 335ms/epoch - 335ms/step\n",
      "Epoch 693/2000\n",
      "1/1 - 0s - loss: 0.3264 - val_loss: 0.2560 - 338ms/epoch - 338ms/step\n",
      "Epoch 694/2000\n",
      "1/1 - 0s - loss: 0.3242 - val_loss: 0.2587 - 335ms/epoch - 335ms/step\n",
      "Epoch 695/2000\n",
      "1/1 - 0s - loss: 0.3243 - val_loss: 0.2560 - 335ms/epoch - 335ms/step\n",
      "Epoch 696/2000\n",
      "1/1 - 0s - loss: 0.3254 - val_loss: 0.2567 - 347ms/epoch - 347ms/step\n",
      "Epoch 697/2000\n",
      "1/1 - 0s - loss: 0.3264 - val_loss: 0.2559 - 335ms/epoch - 335ms/step\n",
      "Epoch 698/2000\n",
      "1/1 - 0s - loss: 0.3248 - val_loss: 0.2593 - 335ms/epoch - 335ms/step\n",
      "Epoch 699/2000\n",
      "1/1 - 0s - loss: 0.3238 - val_loss: 0.2572 - 334ms/epoch - 334ms/step\n",
      "Epoch 700/2000\n",
      "1/1 - 0s - loss: 0.3243 - val_loss: 0.2592 - 337ms/epoch - 337ms/step\n",
      "Epoch 701/2000\n",
      "1/1 - 0s - loss: 0.3243 - val_loss: 0.2531 - 341ms/epoch - 341ms/step\n",
      "Epoch 702/2000\n",
      "1/1 - 0s - loss: 0.3247 - val_loss: 0.2545 - 335ms/epoch - 335ms/step\n",
      "Epoch 703/2000\n",
      "1/1 - 0s - loss: 0.3231 - val_loss: 0.2513 - 337ms/epoch - 337ms/step\n",
      "Epoch 704/2000\n",
      "1/1 - 0s - loss: 0.3235 - val_loss: 0.2568 - 336ms/epoch - 336ms/step\n",
      "Epoch 705/2000\n",
      "1/1 - 0s - loss: 0.3237 - val_loss: 0.2549 - 332ms/epoch - 332ms/step\n",
      "Epoch 706/2000\n",
      "1/1 - 0s - loss: 0.3225 - val_loss: 0.2522 - 332ms/epoch - 332ms/step\n",
      "Epoch 707/2000\n",
      "1/1 - 0s - loss: 0.3219 - val_loss: 0.2565 - 340ms/epoch - 340ms/step\n",
      "Epoch 708/2000\n",
      "1/1 - 0s - loss: 0.3227 - val_loss: 0.2596 - 338ms/epoch - 338ms/step\n",
      "Epoch 709/2000\n",
      "1/1 - 0s - loss: 0.3223 - val_loss: 0.2562 - 338ms/epoch - 338ms/step\n",
      "Epoch 710/2000\n",
      "1/1 - 0s - loss: 0.3218 - val_loss: 0.2509 - 342ms/epoch - 342ms/step\n",
      "Epoch 711/2000\n",
      "1/1 - 0s - loss: 0.3226 - val_loss: 0.2522 - 342ms/epoch - 342ms/step\n",
      "Epoch 712/2000\n",
      "1/1 - 0s - loss: 0.3229 - val_loss: 0.2529 - 335ms/epoch - 335ms/step\n",
      "Epoch 713/2000\n",
      "1/1 - 0s - loss: 0.3221 - val_loss: 0.2530 - 334ms/epoch - 334ms/step\n",
      "Epoch 714/2000\n",
      "1/1 - 0s - loss: 0.3218 - val_loss: 0.2554 - 338ms/epoch - 338ms/step\n",
      "Epoch 715/2000\n",
      "1/1 - 0s - loss: 0.3227 - val_loss: 0.2569 - 335ms/epoch - 335ms/step\n",
      "Epoch 716/2000\n",
      "1/1 - 0s - loss: 0.3225 - val_loss: 0.2528 - 337ms/epoch - 337ms/step\n",
      "Epoch 717/2000\n",
      "1/1 - 0s - loss: 0.3206 - val_loss: 0.2529 - 339ms/epoch - 339ms/step\n",
      "Epoch 718/2000\n",
      "1/1 - 0s - loss: 0.3192 - val_loss: 0.2507 - 335ms/epoch - 335ms/step\n",
      "Epoch 719/2000\n",
      "1/1 - 0s - loss: 0.3203 - val_loss: 0.2532 - 337ms/epoch - 337ms/step\n",
      "Epoch 720/2000\n",
      "1/1 - 0s - loss: 0.3205 - val_loss: 0.2488 - 334ms/epoch - 334ms/step\n",
      "Epoch 721/2000\n",
      "1/1 - 0s - loss: 0.3191 - val_loss: 0.2542 - 341ms/epoch - 341ms/step\n",
      "Epoch 722/2000\n",
      "1/1 - 0s - loss: 0.3197 - val_loss: 0.2545 - 333ms/epoch - 333ms/step\n",
      "Epoch 723/2000\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "outdir = './results/runs/FoundationModel/'\n",
    "\n",
    "trainer = Trainer(\n",
    "    out_dir = outdir,\n",
    "    max_features_percentile=max_features_percentile,\n",
    "    test_size=test_size,\n",
    "    mode=mode,\n",
    "    model=SelfSupervisedTransformer, \n",
    "    dataloader=SelfSupervisedDataGenerator,\n",
    "    loss=loss,\n",
    "    metrics=[]\n",
    ")\n",
    "\n",
    "trainer.setup_data(\n",
    "    data, \n",
    "    discrete_features = [],\n",
    "    continuous_features = features,\n",
    ")\n",
    "\n",
    "trainer.setup_model(\n",
    "    embedding_size=embedding_size, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers,\n",
    "    learning_rate=learning_rate,\n",
    "    batch_size_max = True,\n",
    "    batch_size=10000, # This will take a batch with the size of the training / testing shape,\n",
    "    save_best_only=False\n",
    ")\n",
    "\n",
    "trainer.fit(repetitions=repetitions, epochs=epochs, verbose=verbose, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a83c8c-3843-48f1-a19b-70100b1e7a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xai.models import clean_run\n",
    "\n",
    "clean_run(\n",
    "    path='./results/runs/FoundationModel/',\n",
    "    keep=[1, 10, 100, 500, 1000, 1500, 2000]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e9e96-22b6-4bdf-815b-8e8a6111aa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfh = pd.read_csv(\"./results/runs/FoundationModel//fold-0_id-0/history.csv\", index_col=0)\n",
    "fig, ax=plt.subplots(figsize=(5,4))\n",
    "dfh.plot(ax=ax)\n",
    "ax.set_ylabel('SSL loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8e052d-33f6-49d7-b000-e3e7f31ae383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
